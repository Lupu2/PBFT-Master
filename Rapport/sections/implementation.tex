\chapter{Implementation}

%Implementation 
%describe code, but not to detailed
%motivation for using this design
%good parts/bad parts
\label{chapter:Imp}
In this chapter we introduce our \ac{pbft} implementation. We will introduce the implementation for the request handler, normal protocol workflow, view-changes and finally checkpointing. We will also discuss how the Cleipnir framework has been used to create the working \ac{pbft} implementation as well as discuss some benefits and limitations within the current implementation design.

%Implementation talks about the actual algorithm implementation that is run in protocol execution, includes process of checkpoints and view-changes. Should have figures to simplify explanation. Go over briefly the different phases, show some pseudocode. keeps to take note of, maybe include a model to show the persistency layers present. The importance of using the Cleipnir scheduler and CTask, where you have used them etc.

%outline aka what needs to be talked about, note not in any particular order
%detailed explanation of how the general workflow for the PBFT algorithm is performed, with code snippets.
%the simplicity of some of the necessary tools needed in PBFT is handled. Object oriented programming for Messages, Certificates. Static function for workflow, including Listeners, and handlers
%detailed explanation for how the checkpointing are handled
%detailed explanation for how the view-changes are handled
%Describe how you though of persistency during implementation(not much since its not general focus, and ofcourse doesn't work fully)
%small description on how clients are working, how they use the same reactive operators to count number of replies received.
%Add comment on how well they work, don't work do to design, this is an evaluation afterall.

\section{Motivation}
With the goal of the thesis in mind, the \ac{pbft} protocol workflows were designed to be as close to the protocol description as possible. In order to accomplish this we believed the best approach would be to design protocol related workflow as synchronous as possible. In addition, in order to make it easier to understand the protocol workflow, we believe the best approach is to keep the protocol related code centred around single function or class when possible. This is deemed especially important when constructing the normal protocol workflow. This is because the normal protocol description is orderly constructed, going through different phases until consensus as been reached over the \ac{pbft} network. This could in theory also apply to the view-change description as it is divided into several detailed steps. However, there are several factors which leads to the view-change functionality being split into three separate, but nested, functions. The first reason being that view-changes requires the ability to restart the processes in the case where the process is stationary for too long. To handle this functionality we currently use a mix of timeout operations and goto statements in order to reroute the program flow back to the beginning of the view-change process~\cite{WEB:goto}. The second reason, which also applies to checkpoints, is that the view-change process can be initialized early by receiving view-change messages from other replicas. This may seem similar to protocol operations since its initialized by client requests, however the difference lie in the amount of messages required to initialize the process. Currently, the server needs to support the functionality of starting reactive listeners for view-changes if it ever receives a view-change, however the view-change process itself doesn't start until either the timeout occurs or the replica as received $2f$ messages. Because of this functionality, keeping the code completely synchronous and centered around a single is not possible. As for checkpoints, because the checkpoint processes can be initialized whenever, and majority of the timespent for checkpoints is simply waiting for a replica to receive $2f+1$ unique checkpoints with identical sequence numbers, the source code cannot be centered around a single function.

As we previously mention in \autoref{sec:persvsephe} and \autoref{section:PersistentProgramming} using normal asynchronous operations inside Cleipnir is not a good idea. Therefore, the only form of asynchronous operations performed inside any of the protocol workflows are restricted to other \code{CTask} operations. The \code{await} operator still works well with creating statemachines for asynchronous \code{CTask} operations and waiting for \code{Source} objects to finish all of its operators. Thereby giving the protocol an abstraction as an synchronous process, when in reality it is not. Other than that, traditional \code{Task} based asynchronous operations are well used in the network layer for our implementation. 

Another important topic discussed in \autoref{sec:persvsephe} was the need to use the Cleipnir execution engine to schedule operations when operations outside of Cleipnir required to affect the system within Cleipnir. To simply this design, practically almost all of the scheduled operations using the Cleipnir execution engine are performed within the server. This design is chosen to make it easier to keep track of the origin of the message emit. The server has several emit functions ready to schedule the given message type to its desired \code{Source} object. In order for the protocols workflow to take advantage of this design, they are either required to have a reference to the server object to call the function, or the workflow gets a callback referring to the emit function in the server. The second functionality is quite useful when the operations are initialized by the server. Checkpoints and view-change listeners tend to be initialized by the server, making it simple to assign the correct callback function.

There are currently exists two different implementations for checkpointing and view-changes. Both versions are still documented in the source code, where the second implementations have the letter 2 at the end of its name. The workflow for the both of them remain practically the same for both implementations. The main difference lies with how the implementations handle creating and handling certificates. Essentially the certificate is not deemed valid until it has received $2f+1$ unique and valid message for said message type. This processes is handled differently between the implementation. The second implementation performs the message validation, adding message to proof list and proof list validation over a \code{Source} object by using reactive operators. Meanwhile the first implementation performs these same operations for certificates inside the checkpoint certificate itself inside an append function. Once the certificates are deemed valid, another emit to \code{Source} object has to be made, so that the view-change and checkpoint operations are given the signal to continue with the next operations. The first implementation is required to have the callback function to the server emit message inside the certificate, while the second implementation simply has this function callback right after the reactive listener. The design was changed in order to accommodate the need for more reactive operations in the application. From our experience the second implementation generally performed better than the first implementation and was also for the most part more consistent. The first implementation sometimes encountered issues with the callback function, especially the view-change implementation.

As persistency is a core part of the Cleipnir it was decided to design the application to handle some form of persistency. This meant the protocol related operations had to be run in Cleipnir, which in turn meant the protocol had to be either synchronous or asynchronous using \code{CTask}'s. In addition objects run in the protocols are required to be persistable, meaning the objects needed to get a serializer function and deserializer function connected to Cleipnir. This also includes using Cleipnir inbuilt data structures when the persisted objects needed to also persist data structures. This is especially present in certificates since they need to persist their list of proofs, meaning the proof list uses the inbuilt Cleipnir \code{CList} to substitute normal list. Unfortunately there is an unfortunate oversight in our part when we designed the application. The application currently uses \ac{json}~\cite{WEB:NewJSON} to serialize and deserialize messages when sent over the \ac{pbft} network. \ac{json} formatting does not support inbuilt Cleipnir data structures, which is not surprising yet was not directly discovered until very late into the \ac{pbft} implementation. Currently this issue is solved by converting between traditional data structures and inbuilt Cleipnir data structures whenever a message with said data structure needs to be serialized by \ac{json}. The conversation itself is rather simple, a copy of the data storage in the traditional data structure format is created, then the content is copied from the source data storage to the newly created data storage. The process is then reversed on the receiver end after the message as been properly deserialized. Although in retrospect, it might have been more beneficial if the serialization and deserialization for the networking were the same used by Cleipnir as to avoid this issue in the future. Finally, we have done our best to avoid creating circular dependencies. Circular dependencies would essentially cause the serialization process to fail, as both references depend on each other. We do believe there currently are no circular dependencies in our implementation, because the Cleipnir serialization process does not crash during Cleipnir's synchronization process. The server and protocol workflow relationship can potentially be too close to being one. The server emits messages to the protocol workflow while the protocol workflow has a reference to the server. The only reason this does not create a circular dependency is because the server interacts with the protocol workflow through the Cleipnir execution engine and does not have a direct reference to the protocol workflow. Currently the \ac{pbft} implementation does not currently support functional persistency. The reason for as to why the persistency  does not work completely is uncertain. There are two main issues as of now. The first issue is that protocol logger for some reason does not persist all the entries in the logger. As of now, no distinguishable pattern as been found with the data which is lost. Either way, losing certificates in the logger does create big problems. The other reason is that some of the \code{Source} objects linked to the server gets duplicated, meaning that in the system there exist two \code{Source} objects with the exact same reference. This becomes a problem, because each time the server emits a message to the \code{Source} object, it will emit the message to both the original and duplicate \code{Source}. This in turn can essentially cause two identical iterations of the protocol to occur. This even includes even storing the resulting protocol certificates to the logger with the same sequence. As it is not ever intended to store four certificates to a single sequence number, it is very clearly not working properly. 
%As our main goal for this thesis was to evaluate the tools given and not focus on making the implementation persistable, we decided on prioritising other factors of our implementation. We do believe however, that if it were possible to solve the previously mention issues, that the current implementation has a decent (word for solid background for getting it to work). 

\section{Workflow Details}
\label{sec:protocolwork}
\subsection{Protocol Workflow Implementation}

\subsubsection{Starting protocol instance}
A normal sequence for the \ac{pbft} implementation begins once the request handler receives a request message from the server. The source code for the request handler can be seen in \autoref{code:StartProtocol}. The request handler listens for new requests messages emitted to the \code{Source} object \emph{requestMessage} as seen on line three. As mentioned in \autoref{sec:persvsephe}, the server is tasked with emitting messages received in the network layer to the appropriate \code{Source} object in order for the protocol to access the message. The request handler is responsible for making sure that the request received is valid. In addition the request handler does not start any new iterations of the \ac{pbft} protocol if the system sequence number exceeds the current sequence number interval. This is handled by the if condition spanning lines four to ten. Finally, a new protocol instance is not called if the system is performing a view-change. This is determined by the boolean value \code{active} which is tied to the protocol execution object. Once all checks are passed the request handler updates and collects the current sequence number. Then it calls the asynchronous \code{CTask} function \emph{PerformProtocol} which initializes and starts the \ac{pbft} protocol for the given request. It is important that the request handler does not wait for the \code{PerformProtocol} function to finish as it is important to not block the \emph{requestMessage} as we desire an application which can process multiple requests from clients at the same time.

%\paragraph{Testsec1}
%\paragraph{Testsec2}
\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:StartProtocol, caption=Code section from the request handler, captionpos = b, basicstyle=\scriptsize]
while (true)
{
    var req = await requestMessage.Next();
    if (Crypto.VerifySignature(
            req.Signature, 
            req.CreateCopyTemplate().SerializeToBuffer(), 
            serv.ClientPubKeyRegister[req.ClientID]
        ) 
        && serv.CurSeqNr < serv.CurSeqRange.End.Value
    )
    {
        if (execute.Active)
        {
            int seq = ++serv.CurSeqNr;
            Console.WriteLine("Curseq: " + seq + " for request: " + req);
            _ = PerformProtocol(execute, serv, scheduler, shutdownPhaseSource, req, seq);
        }
    }
}
	\end{lstlisting}
\end{figure}

\subsubsection{Pre-Prepare phase}
The pre-prepare phase is the only part of the normal operation workflow which has different structure depending on whether or not the replica is the primary or not. The primary's source code for the pre-prepare phase can be seen in \autoref{code:Pre-PreparePrimary}. If the replica is the primary, then it uses the sequence number in which the protocol instance was assigned and creates a pre-prepare message for this sequence number. The pre-prepare message also contains information in regards to the primary’s server id, current view and digest of the request. The pre-prepare message dictates the other replicas sequence number for the processing of the given request. The primary then initializes the protocol certificate used for storing the proof of the prepare phase. Since the first \code{phase message} that is first active in the prepare phase is the pre-prepare message, the protocol certificate that is used for the prepare phase always has the pre-prepare message as its first entry in its proof list. The protocol instance then uses the server reference to multicast the pre-prepare message to the other replicas in the network. 

%The pre-prepare phase is the only part of the normal operation workflow which has different structure depending on whether or not the replica is the primary or not. If the replica is the primary, then it takes the sequence number that the function was assigned and creates a pre-prepare message using this sequence number together with its server id, current view and digest of the request. This pre-prepare message will dictate the other replicas sequence number for this given request. The primary will than initialize the protocol certificate used for storing the proof of the prepare phase. This protocol certificate for the prepare phase will always have the pre-prepare message as its first entry in its proof list. The protocol will then use its server reference to multicast this pre-prepare message to the other replicas in the network. The primary's source code for the pre-prepare phase can be seen in \autoref{code:Pre-PreparePrimary}.

The source code of the other non-primary replicas can be seen in \autoref{code:Pre-PrepareNonPrimary}. The other non-primary replica starts its protocol instance by subscribing to the \code{Source<PhaseMessage>} \code{MesBridge} and listens for incoming phase messages. The whole subscribing, listening and handling of incoming items on the \code{MesBridge} are performed on lines three to twelve. Considering the replicas only wants a pre-prepare message in this reactive listener, it uses a \code{WHERE} clause to ignore any other phase message other than ones which uses the pre-prepare messages enum type. In addition, another \code{WHERE} clause is assigned to avoid any pre-prepare messages designated for other requests by comparing request digests. Therefore an incoming phase message can only pass the \code{WHERE} clause if it involves the same request which the protocol instance is processing. The final \code{WHERE} clause validates the phase message where the validation criterias are the same as the ones mentioned in \autoref{sec:detailedProtocol} for pre-prepare messages. Once the replica receives a pre-prepare phase message which passes all the \code{WHERE} clauses it creates its own protocol certificate which uses the sequence number given by the primary, which in turn means all protocol certificates for the prepare phase now match for each replica. The non-primary replica finally ends the pre-prepare phase and starts the prepare phase by creating a prepare message and multicasting this message using the same method the primary used for multicasting its pre-prepare phase.

The \code{MERGE} operator is used to ensure that the protocol execution is terminated if a view-change occurs. If the timeout occurs, a unique phase message is emitted to the \code{Source<PhaseMessage>} \emph{ShutdownBridgePhase}. The \code{MERGE} operator somewhat binds these reactive streams together. This essentially means the the \emph{MesBridge}  is unsubscribed for new phase messages if a phase message is detected in the \emph{ShutdownBridgePhase} and returns this phase message as the resulting phase message. The opposite situation also applies for the \emph{ShutdownBridgePhase}. As this phase message is intentionally faulty it is not allowed to be used in the prepare phase of the protocol. Therefore a timeout exception is instead called, which closes this instance of the protocol execution.

The design chosen for the source code to the pre-prepare phase is simple and follows a synchronous workflow as we desired, which in turn makes it easier for developers to write. Unfortunately there are two severe issues with our current implementation of the pre-prepare phase. These issues are caused by a combination of having to split the code based on primary vs non-primary and the importance of initializing instances of the reactive listeners early. Both issues are theoretically very similar as they both are caused by improper initialization of the reactive listeners used in the \ac{pbft} implementation. The first issue occurs when the primary sends out its pre-prepare phase message before the non-primary replicas have initialized the pre-prepare reactive listener. Which results in the phase message not being received by the non-replica, which means it fails the pre-prepare phase. As the pre-prepare phase fails, the timeout is going to occur, which puts the replica into view-change mode as it believes the primary replica is faulty. The second issue which can occur is that a non-primary can receive a prepare message before it has received the initial pre-prepare message from the primary. When this situation occurs the prepare message gets filtered out by the pre-prepare reactive listener and is therefore not available once this non-primary reaches the prepare phase. In the worst case scenario the replica loses all of the other replicas prepare messages, meaning the protocol instance is stuck in the prepare phase once it finally receives its pre-prepare message.

These issues are caused mostly due to the application struggling on handling phase messages that are being received out of intended order. There are several workarounds to handling messages that arrive out of order, however most of the workarounds available would require adding a lot more complexity to the implementation. As our goal for this thesis is to create an \ac{pbft} implementation that is very simple and accurate to the protocol description, it was decided not to redesign the protocol workflow to handle issues regarding pre-prepare messages out of order. As the pre-prepare message is meant to be responsible for getting the other non-primary to start processing the request and assigning the sequence number, we feel it would not be true to the original algorithm to change this design. One workaround to this issue would for instance be to initialize the prepare phase reactive listeners at the start of the workflow. Once the pre-prepare message was received, the prepare messages not related to the given request with different sequence numbers would be filtered out. Currently, in order to somewhat mitigate this issue, the primary is forced to wait for at least a second before starting to multicast its pre-prepare message. This wait period is performed to allow the other replicas to catch up. Which makes it less likely that a replica is far enough behind to lose out on prepare messages before completing handling their pre-prepare message. With this workaround, the issues discussed are for the most part stable. As an estimate, an average of 15 operations can be progressed without incident before encountering these issues.LOOK HERE(Potentially mention the workaround discussed in future work)


\iffalse
prior version
\subsubsection{Pre-Prepare phase}
The pre-prepare phase is the only part of the normal operation workflow which has different structure depending on whether or not the replica is the primary or not. If the replica is the primary, than it will take the sequence number initialized by its server and create a pre-prepare message using this sequence number together with its server id, current view and digest of the request. This pre-prepare message will dictate the other replicas sequence number for this given request. The primary will than initialize the protocol certificate used for storing the proof of the prepare phase. This protocol certificate for the prepare phase will always have the pre-prepare message as its first entry in its proof list. The protocol will then use its server reference to multicast this pre-prepare message to the other replicas in the network. The primary's source code for the pre-prepare phase can be seen in \autoref{code:Pre-PreparePrimary}.

The source code of the other non-primary replicas can be seen in \autoref{code:Pre-PrepareNonPrimary}. The other non-primary replicas will subscribe to the \code{Source<PhaseMessage>} \emph{MesBridge} and attempt to listen for incoming phase messages. Since the replica only wants pre-prepare message in this reactive listener, it will use a \code{WHERE} clause to ignore any other phase message other than ones which uses the pre-prepare messages type. In addition, another \code{WHERE} clause is assigned to avoid any pre-prepare messages designated for other requests by comparing the request digests. The final \code{WHERE} clause validates the phase message where the validation  rules are the same as the once mention in \autoref{sec:detailedProtocol} for pre-prepare messages. Once the replica receives a pre-prepare phase message which passes all the \code{WHERE} clauses it will create its own protocol certificate which uses the sequence number given by the primary, which in turn means all protocol certificates for the prepare phase should match for each replica. The non-primary replica will finally end the pre-prepare phase and start the prepare phase by creating a prepare message and multicast this message using the same method the primary used for multicasting its pre-prepare phase. 

The \code{MERGE} operator is used to ensure that the protocol execution is terminated if a view-change occurs. If the timeout occurs, a unique phase message will be emitted to the \code{Source<PhaseMessage>} \emph{ShutdownBridgePhase}. The \code{MERGE} operator will somewhat bind these reactive streams together. This essentially means the the \emph{MesBridge} will be unsubscribe for new phase messages if a phase message is detected in the \emph{ShutdownBridgePhase} and will return this phase message as the resulting phase message. The opposite situation will also apply for the \emph{ShutdownBridgePhase}. As this phase message is intentionally faulty it will not be allowed to be used in the prepare phase of the protocol. Therefore a timeout exception is called instead, which will properly close this instance of the protocol execution.

The design for the source code for the pre-prepare phase is simple and follows a synchronous workflow as we desired, which in turn makes it easier for developers to write. Unfortunately there are two severe issues with our current implementation of the pre-prepare phase. These issues are caused by a combination of having to split the code based on primary vs non-primary and the importance of initializing instances of the reactive listeners early. Both issues are theoretically very similar as they both are caused by improper initialization of the reactive listeners used in the \ac{pbft} implementation. The first issue occurs when the primary sends out its pre-prepare phase message before the non-primary replicas as initialized the pre-prepare reactive listener. Which results in the phase message not being received by the non-replica, which means it fails the pre-prepare phase and the timeout will put the replica into view-change mode. The second issue occurs when a non-primary receives a prepare message before it has received the pre-prepare message from the primary. When this situation occurs the prepare message gets filtered out and will not be available once this non-primary reaces the prepare phase. In worst case scenario the replica loses all of the other replicas prepare message, meaning it gets stuck in the prepare phase once it finally receives its pre-prepare message.

These issues mostly comes down to the application struggling on handling phase messages being received out of order. There are several workarounds to handling messages out of order, however most of the workarounds available would require adding a lot more complexity to the implementation. As our goal for this thesis being to create the simplest and most true implementation of \ac{pbft} when compared to the protocol description, it was decided not to redesign the protocol workflow to handle issues with pre-prepare messages out of order. As the pre-prepare message is meant to be responsible for getting the other non-primary to start processing the request and assigning the sequence number, we feel it would not be true to the original algorithm to change this design. One workaround to this issue would for instance be to simply initialize a prepare reactive listeners at the start and once the pre-prepare message was received, the prepare messages not related to the given request with different sequence numbers would be filtered out. Currently, in order to somewhat mitigate this issue, the primary is forced to wait for atleast a second before starting to multicast its pre-prepare message. This to allow the other replicas to catch up, meaning its less likely that a replica is far enough behind to lose out on prepare messages before getting handling their pre-prepare message. With this workaround, the issue is for the most part stable, with an average of 15 operations being processed before encountering this issue.\fi

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:Pre-PreparePrimary, caption= Source code for pre-prepare phase for primary replica, captionpos = b, basicstyle=\scriptsize]
ProtocolCertificate qcertpre;
byte[] digest = Crypto.CreateDigest(clireq);
int curSeq; 
if (Serv.IsPrimary()) //Primary
{
    curSeq = leaderseq;
    Console.WriteLine("CurSeq:" + curSeq);
    Serv.InitializeLog(curSeq);
    PhaseMessage preprepare = new PhaseMessage(
        Serv.ServID, 
    	curSeq, 
        Serv.CurView, 
        digest, 
        PMessageType.PrePrepare
    );
    Serv.SignMessage(preprepare, MessageType.PhaseMessage);
    qcertpre = new ProtocolCertificate(
        preprepare.SeqNr, 
        preprepare.ViewNr, 
        digest, 
        CertType.Prepared, 
        preprepare
    );
    await Sleep.Until(1000);
    Serv.Multicast(preprepare.SerializeToBuffer(), MessageType.PhaseMessage);
}
	\end{lstlisting}
\end{figure}

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:Pre-PrepareNonPrimary, caption= Pre-prepare phase for non-primary replica, captionpos = b, basicstyle=\scriptsize]
else	//Not Primary
{ 
    var preprepared = await MesBridge
    	              .Where(pm => pm.PhaseType == PMessageType.PrePrepare)
                      .Where(pm => pm.Digest != null && pm.Digest.SequenceEqual(digest))
                      .Where(pm => pm.Validate(
                        Serv.ServPubKeyRegister[pm.ServID],
                        Serv.CurView, 
                        Serv.CurSeqRange)
                       )
                       .Merge(ShutdownBridgePhase)
                       .Next();
                
    if (preprepared.ServID == -1 && preprepared.PhaseType == PMessageType.End) 
        throw new TimeoutException("Timeout Occurred! System is no longer active!");
    qcertpre = new ProtocolCertificate(
        preprepared.SeqNr, 
        Serv.CurView, 
        digest, 
        CertType.Prepared, 
        preprepared
    );
    curSeq = qcertpre.SeqNr; 
    Serv.InitializeLog(curSeq);
    PhaseMessage prepare = new PhaseMessage(
        Serv.ServID, 
        curSeq, 
        Serv.CurView, 
        digest, 
        PMessageType.Prepare
    );
    Serv.SignMessage(prepare, MessageType.PhaseMessage);
    qcertpre.ProofList.Add(prepare);
    Serv.Multicast(prepare.SerializeToBuffer(), MessageType.PhaseMessage);
}
	\end{lstlisting}
\end{figure}		

\subsubsection{Prepare phase}
In comparison to the Pre-prepare phase and the start of the prepare phase, the rest of the workflow in the implementation is relatively stable and straightforward. The prepare and commit phase source code can be seen in \autoref{code:PrepareAndCommit}. The first step of the prepare phase is to initialize the reactive listeners for prepare and commit phase messages. Due to the listeners having several reactive operators connected to their stream, the code must span several code lines in order to make it readable. The prepare listener is initialized on lines two to 18, and the commit listener is initialized on lines 25 to 42 in \autoref{code:PrepareAndCommit}. There are two reasons why the reactive listeners for prepare and commit messages are initialized early. The first reason is to reduce the time it takes for the workflow to move from the pre-prepare listener to the next reactive listeners. This time needs to be small to avoid losing potential incoming phase messages to the reactive streams. 
The other reason is to avoid ordering issues between prepare and commit messages. Since the sequence number for the workflow has already been determined during the pre-prepare phase, the prepare and commit phase can initialize their reactive streams early and be active at the same time. Because of this the prepare and commit phase does not suffer issues in regards to phase messages being out of order. If the pre-prepare message did not dictate the sequence number for non-primary replicas, this would have also been the ideal design for handling phase messages during the pre-prepare phase.

The reactive listeners used for the prepare phase and the commit phase are almost practically identical. The only major difference between the two is that they only accept phase messages in the stream of their respective protocol phase. Meaning the reactive listener for the prepare phase filters away phase messages that do not have protocol phase type prepare. This operation is performed by the first \code{WHERE} clause. In addition, the certificates for both protocol phases are also initialized early. This is because the certificates are this time actively updated through the operations in the reactive listeners stream.

During the prepare phase the workflow waits until the prepare certificate has added $2f+1$ unique prepare phase messages to its proof list. In order for a phase message to be added to the prepare certificate it must pass all of the \code{WHERE} clauses assigned for the reactive listener. In actuality the workflow only waits for $2f$ prepare phase messages due to the pre-prepare message already being assigned in the pre-prepare phase. Once a valid phase message passes all of the first \code{WHERE} operators, it is added to the designated protocol certificate using the \code{SCAN} operator. The \code{SCAN} operator actually transforms the certificate’s proof list to include the incoming phase message.  The final \code{WHERE} clause determines whether or not the certificate has achieved the sufficient number of valid phase messages in its proof list.
The \code{ValidateCertificate} function essentially calculates the number of phase messages inside the proof list when it excludes duplicates. It also makes sure that the phase messages in the list are indeed valid. The asynchronous \code{await} operator on line 45 is used to wait for the \emph{CAwaitable} in the prepare phase reactive listener to finish all of the linked operators for the listener before moving on with the protocol. Once the prepare certificate has succeeded its validation process, the workflow can move past the \code{await} operator. The prepare phase is finished after the prepare certificate is added to protocol log in the server on line 47.

\subsubsection{Commit Phase}
As for the commit phase, like the other protocol phases, the first step is to have each replica create a commit phase message and use the server to multicast the commit phase over the \ac{pbft} network. Afterwards the commit phase performs practically the same operations as the prepare reactive listener. The commit reactive listener waits for the proof list for the commit certificate to have at least $2f+1$ commit phase messages. The reactive listener for the commit phase has an additional \code{WHERE} clause that makes sure that the prepare phase is finished. This is done to avoid finishing the commit certificate before the prepare phase is complete.  After the commit certificate is successfully validated, the protocol workflow is almost complete. To start the remaining operations the protocol workflow first adds the commit certificate to the protocol logger as was done prior with the prepare certificate. The server now has two valid certificates for the given sequence number to request, meaning the replica has the necessary proof that the protocol was successful for the given request. The second remaining process is to create a reply message, digitally sign the reply and then send the reply to the client that originally sent the processed request. Additionally the operation within the request is performed by the application. In our \ac{pbft} implementation the 'operation' is to simply write the operation to the console window and add the operation to a persistent list. The persistent list representing the application state is more discussed in \autoref{section:ImpCheckpointing}.

\subsubsection{Protocol Workflow Evaluation}
Insert whether you believe the code for each section is defined as good code, explain why. How did usage async, reactive operation help/hinder the protocol workflow
Around 135 lines of code excluding extra lines for initializing objects. Where 30\% is from the reactive operators.

\iffalse
\subsubsection{Prepare phase and Commit phase}
In comparison to the Pre-prepare phase and starting the prepare phase, the rest of the workflow in the implementation is relatively stable and straightforward. The prepare and commit phase source code can be seen in \autoref{code:PrepareAndCommit}. The first step is to initialize the reactive listeners for prepare and commit phase messages. This is done early for two reasons! The first is to mitigate the time between waiting for pre-prepare messages and prepare messages as two avoid potentially losing prepare messages. The other reason is so that the protocol can listen for both prepare messages and commit messages, which means there aren't any ordering issues between messages during the prepare and commit phase. If the pre-prepare message did not dictate the sequence number for non-primary replicas, this would have also been the ideal design for handling pre-prepare message. 

The reactive listener used for the prepare phase are pretty much the same for both phases. The only major difference between the two is that they only accept phase messages for their respective protocol phase. Additionally a commit certificate is initialized early to be used together with the commit reactive listener. Since the prepare messages and prepare certificates are already been initialized in the pre-prepare phase, there is only one more thing to do in the prepare phase. The prepare phase will wait until the prepare certificate as received $2f+1$ unique prepare phase messages which passes all of the \code{WHERE} clauses assigned. In actuality it is to wait for $2f$ prepares and one pre-prepare message. To add the valid phase messages to the designated certificates, we use the \code{SCAN} operator to transform the original proof list for the certificate to include the messages received in the reactive listener. The final \code{WHERE} clause determines whether or not the certificate has received the desired number of unique valid phase messages. Essentially calculating the number of phase messages inside the proof list excluding duplicates, and making sure the phase messages in the list are valid. The asynchronous \code{await} operator is used to wait for the \emph{CAwaitable} to finish this all of the operators linked to the prepare reactive listener before moving on with the protocol. Once the prepare certificate has succeeded its validation process, the prepare certificate is added to protocol log in the server and the commit phase officially starts. 

Like the other phases, the first step will be for each replica to create their commit phase messages and use the server to multicast their commit phase over the \ac{pbft} network. Afterwards the protocol will wait for the proof list for the commit certificate to reach $2f+1$. This rule applies to each of the replica as there are no difference in operations between primaries and non-primaries in the commit phase. The reactive listener for the commit phase will additionally check that prepare phase as already finished validating as to avoid finishing the commit certificate before the protocol certificate. However, this extra check does not effect the protocol workflow in either way, since the protocol certificate is awaited earlier in the process. After the commit certificate is successfully validated, the protocol workflow is essentially completed. The remaining operations performed in the protocol execution is to add the commit certificate to the logger similar to the prepare certificate. The server will now have two valid certificates for the given sequence number to request, meaning the replica now has proof that the protocol was successful for the given request. Finally a reply message will be created, signed and sent to the client that sent the processed request. Additionally the operation within the request will be performed by the application. In our \ac{pbft} implementation the 'operation' will simply be to write the operation to the console window and add the operation to a persistent list. The persistent list representing the application state will be more discussed in \autoref{section:ImpCheckpointing}.
\fi

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:PrepareAndCommit, caption= Prepare and Commit phase, captionpos = b, basicstyle=\scriptsize]
	
var prepared = MesBridge
               .Where(pm => pm.PhaseType == PMessageType.Prepare)
               .Where(pm => pm.SeqNr == qcertpre.SeqNr)
               .Where(pm => pm.Validate(
                    Serv.ServPubKeyRegister[pm.ServID], 
                    Serv.CurView, 
                    Serv.CurSeqRange, 
                    qcertpre)
                )
                .Where(pm => pm.Digest.SequenceEqual(qcertpre.CurReqDigest))
                .Scan(qcertpre.ProofList, (prooflist, message) =>
                {
                    prooflist.Add(message);
                    return prooflist;
                })
                .Where(_ => qcertpre.ValidateCertificate(FailureNr))
                .Next();
ProtocolCertificate qcertcom = new ProtocolCertificate(
    qcertpre.SeqNr, 
    Serv.CurView, 
    digest, 
    CertType.Committed
);   
var committed = MesBridge
                .Where(pm => pm.PhaseType == PMessageType.Commit)
                .Where(pm => pm.SeqNr == qcertcom.SeqNr)
                .Where(pm => pm.Validate(
                    Serv.ServPubKeyRegister[pm.ServID], 
                    Serv.CurView, 
                    Serv.CurSeqRange, 
                    qcertcom)
                )
                .Where(pm => pm.Digest.SequenceEqual(qcertcom.CurReqDigest))
                .Scan(qcertcom.ProofList, (prooflist, message) =>
                {
                    prooflist.Add(message);
                    return prooflist;
                })
                .Where(_ => qcertcom.ValidateCertificate(FailureNr))
                .Where(_ => qcertpre.ValidateCertificate(FailureNr))
                .Next();
                
Console.WriteLine("Waiting for prepares");
if (Active) await prepared;
else throw new ConstraintException("System is no longer active!");
Serv.AddProtocolCertificate(qcertpre.SeqNr, qcertpre); //add first certificate to Log

//Commit phase
PhaseMessage commitmes = new PhaseMessage(
    Serv.ServID, 
    curSeq, 
    Serv.CurView, 
   	digest, 
    PMessageType.Commit
);
Serv.SignMessage(commitmes, MessageType.PhaseMessage);
Serv.Multicast(commitmes.SerializeToBuffer(), MessageType.PhaseMessage);
Serv.EmitPhaseMessageLocally(commitmes);
Console.WriteLine("Waiting for commits");
if (Active) await committed;
else throw new ConstraintException("System is no longer active!");
Serv.AddProtocolCertificate(qcertcom.SeqNr, qcertcom); //add second certificate to Log
	\end{lstlisting}
\end{figure}

\subsection{View-change Implementation}
As previously mention in \autoref{sec:view-change} the goal of a view-change is to successfully replace a faulty primary replica with another non-faulty replica. In order for a primary change to be successful, the replicas in the \ac{pbft} network needs to agree upon the state the program continues on after the primary change has occurred. Furthermore the view-change must ensure that the new replica selected for primary responsibility is not faulty. 

The operations to ensure these criteria were briefly mentioned in \autoref{sec:view-change}. Although in total there are a quite lot of operations needed for a successful view-change to take place. However, it is possible to divide up the operations into two segments based on which goal the operations attempt to fulfill. Excluding the processes of shutting down the protocol execution, the first part of the view-change process is for the replicas in the network to agree on that a view-change is necessary. This goal is achieved by having the replicas multicast and listen for view-change messages. Since the next primary is determined by the formula $p = v ~mod~ R$, \ac{pbft} doesn't require any election process. The view-change messages instead contains information of the replicas current checkpoint information as well as current state of the logged certificates. This is so that the new primary can have all the relevant information to create the new state for the \ac{pbft} system. The goal of the second segment is to initialize the \ac{pbft} system state after the view-change is finished. This goal is fulfilled by looking at the current stable checkpoint and the current protocol certificates stored in memory. In order to make sure that requests were not fully processed before the view-change occured. The \ac{pbft} needs to redo each of the requests stored in the logger up to the highest sequence number seen in the \ac{pbft} system. Thankfully, due to stable checkpoints, the process does not need to take into account every single request ever processed. The new primary is responsible for starting this process by creating and multicast a new-view message. This message acts as an introduction letter, telling the other replicas in the network that it is the new primary and additionally provide a view-change certificate proving this fact. The new-view message also contain a list of pre-preprepares which are created from the information stored within the protocol certificates in the view-messages received. This new-view message is validated by the other replicas. If the replicas deem the information in new-view message as valid, the replicas will use these pre-prepares message to create prepare and commit messages and redo the \ac{pbft} algorithm for these pre-prepares. The system is finally finished with the view-change once all the pre-prepares have stored their respective two protocol certificates. 

As for implementing this functionality, our implementation can be divided into four segments. The first consist of the timeout functionality that when triggers puts the application into non-active mode. The second part consist of updating the view data, creating view-change messages, multicast these view-changes over the \ac{pbft} network and finally store the collection of view-changes until quorum has been reached. The third consist of creating and validating functionality for new-view message. Finally the last segment consist of the redo protocol functionality. 

In non-active mode all protocol related messages and requests are denied by the main protocol execution. This mode is active until all four segments of the view-change functionality as been completed successfully. The view-change functionality differs from the other functionality due to the handling of timeout. It has already been mention that the view-change functionality start once a replica exceeds its timeout before receiving a pre-preprepare message. However, there are two additional timeout present in the view-change functionality. These timeout exists in order for the system to be absolute sure that the new primary chosen by the $p = v ~mod~ R$ formula does not result in a faulty replica. If the formula does result in a faulty formula, then either view-change process or redo protocol process will most likely fail. Setting a timeout for these two functionalities, the protocol can recover from a potential frozen state and restart the view-change process by now selecting the next replica on the list. Essentially, the view number is incremented every time the view-change protocol changes, meaning a new primary is selected until a non-faulty primary is chosen. Unfortunately the current implementation only handles timeout at the start of the normal protocol workflow, which also gets stopped once the replica receives a pre-prepare message. This means the protocol gets effectively stuck in the case where the protocol fails at handling prepare and commit messages. There were to main reason for why this issue was not resolved in our implementation. The first reason was that timeout functionality relies on the \code{WhenAny} asynchronous function~\cite{WEB:whenany}. This function creates a \code{Task} that is set to finish once either of the attached \code{Task}'s completes. In our implementation this effectively is set to either the timeout is exceeded or the process that is waited for completes. This was unfortunately not very well integrated with reactive listeners, as it is forced to finish all the operators before it is deemed completed. It required the \code{Merge} operator in order to enforce reactive listeners to stop and dispose of the active reactive stream when a timeout occured. The \code{Merge} operator required that both the reactive streams that were to merge had the same format. This means that the stop signal to the \code{Merge} operator needed to also be a phase message for the pre-prepare reactive stream. The current workflow for handling the timeout functionality can be seen in \autoref{code:timeout}. The time is first initialized with a cancellationtoken which is brought into the main protocol workflow so that the protocol can cancel the timeout when it receives a pre-prepare message. The timeout used in the current implementation is set to ten seconds. The timeout gets a reference to an active \code{Source} object which is the same \code{Source} which is listened to at the function ListenForShutdown. When the timeout exceeds, the timeout function will emit a message to the shutdown \code{Source} which in turn makes the \code{CTask} in ListenForShutdown to return before the AppOperation, which lets the program flow to continue. The AppOperation is still active as an asynchronous function, meaning we want to forcefully shut it down so as to avoid creating conflicts with the future emits to the protocol \code{Source} object. To solve this we emit an obviously faulty phase message with an unique phase message type called \emph{End}. Thanks to the \code{Merge} operator, the pre-prepare reactive listener will finish and returns the faulty pre-prepare message. As seen in \autoref{code:Pre-PrepareNonPrimary}, the protocol calls a timeout exception if the pre-prepare reactive listeners returns the faulty phase message, meaning the protocol effectively shuts down as intended.   

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:timeout, caption=Handling timeout for the normal protocol workflow, captionpos = b, basicstyle=\scriptsize]
CancellationTokenSource cancel = new CancellationTokenSource();
_ = TimeoutOps.AbortableProtocolTimeoutOperation( //starts timeout
   serv.Subjects.ShutdownSubject,
   10000,
   cancel.Token,
   scheduler
);
execute.Serv.ChangeClientStatus(req.ClientID);
bool res = await WhenAny<bool>.Of(
                AppOperation(req, execute, seq, cancel),
                ListenForShutdown(serv.Subjects.ShutdownSubject)
);
Console.WriteLine("Result: " + res);
if (res)
{
   Console.WriteLine($"APP OPERATION {seq} FINISHED");
   ...
}
else
{
   if (execute.Active)
   {
      Console.WriteLine("View-Change starting");
      execute.Active = false;
      serv.ProtocolActive = false;
      await scheduler.Schedule(() =>
         shutdownPhaseSource.Emit(new PhaseMessage(-1, -1, -1, null, PMessageType.End)
      ));
      await execute.HandlePrimaryChange2(); 
      Console.WriteLine("View-Change completed");
      serv.UpdateSeqNr();
      ...
    \end{lstlisting}
\end{figure} 
 
The view-change exchange segment of the code starts by first setting the replica into the next view by incrementing its view number. The next operation sets the replicas view-change certificate. This step is dependent on whether or not the replica has received previously received view-change messages from another replica. If the replica has not received any view-messages from other replicas than it initializes the view-change certificate and initializes the view-change reactive listener. Info about the rest of the view-change process in the main workflow...

The view-change listener deviates a bit from the other reactive listeners. The main difference is that it also requires the ability to call upon a shutdown emit in the case where the system already has gotten $2f$ view-change messages. The reason for this functionality is mostly due to making the system more efficient. The replica does not need to wait for a timeout to occur if it already has received $2f$ view-change messages since the \ac{pbft} network only requires that replica's view-change in order for the new view to be initialized. Therefore the process is speed up by calling for a shutdown emit if already has $2f$. Ofcourse this functionality is only useful if the replica is still in active mode. This is the reason as to why the option to not trigger the shutdown emit is an option. Other than that the reactive listener performs relatively the same operators for the reactive stream. Firstly, we want to only accept view-change message that belongs to the same next view nr as the replica. Secondly the view-change messages received are validated to make sure that it is a valid view-change message. Assuming the validation process is successful, the view-change message is added to the view-change certificate proof list. The final reactive operator validates that the view-change certificate to see if it has received the sufficient number of valid view-change messages in its proof list. After the view-change reactive listener is finished and a valid view-change certificate is ready, the callback function \emph{finCallback} calls the servers to emit a signal to the view-change workflow that the view-change certificate is finished and can move on with the next step of the view-change process. 

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:viewListener, caption=Handling timeout for the normal protocol workflow, captionpos = b, basicstyle=\scriptsize]
if (Shutdown && shutdownCallback != null)
{
   Console.WriteLine("With shutdown");
   await ViewBridge
      .Where(vc => vc.NextViewNr == NewViewNr)
      .Where(vc => vc.Validate(keys[vc.ServID], ServerViewInfo.ViewNr))
      .Scan(vcc.ProofList, (prooflist, message) =>
      {
        prooflist.Add(message);
        return prooflist;
      })
      .Where(_ => vcc.ShutdownReached(FailureNr))
      .Next();
   Console.WriteLine("Calling shutdown");
   shutdownCallback();
}
await ViewBridge
   .Where(vc => vc.NextViewNr == NewViewNr)
   .Where(vc => vc.Validate(keys[vc.ServID], ServerViewInfo.ViewNr))
   .Scan(vcc.ProofList, (prooflist, message) =>
   {
     prooflist.Add(message);
     return prooflist;
   })
   .Where(_ => vcc.ValidateCertificate(FailureNr))
   .Next();
Console.WriteLine("Finished Listen view changes");
finCallback();
    \end{lstlisting}
\end{figure} 

\subsection{Checkpoint Implementation}
\label{section:ImpCheckpointing}
The checkpointing process occurs only after a certain number of requests have been processed by the \ac{pbft} implementation. The number of requests is determined by the \emph{checkpoint interval}. For our implementation the \emph{checkpoint interval} is set to five, meaning after processing five requests a new checkpoint is created for the system. For our implementation the checkpoint workflow is divided into three parts. The first part revolves around creating a checkpoint certificate and starting an instance of the reactive checkpoint workflow. The second part is performed by the instance of the reactive checkpoint workflow. In this part the reactive instance listens for incoming checkpoint messages, which are then validated and added to the certificate’s proof list. This reactive process ends once a certificate has received sufficient checkpoints to be deemed valid. The final part consists of emitting the stable checkpoint to the system to replace the old stable checkpoint and start the garbage collection process. We will now discuss each of these parts in more detail.

\subsubsection{Initialize Checkpoint Certificate}
The checkpoint certificate is initialized using the last sequence number used by the protocol workflow. The checkpoint certificate also needs to create and store a digest of the current state of the application. In our implementation, we create the system digest based on the persistent list that represents the application state. This list contains the operation messages from each of the requests that have been fully processed by the \ac{pbft} protocol. Therefore assuming no errors occur, then the checkpoint for sequence number five has the digest of the list containing the operation from requests one to five. After a checkpoint certificate and the replica checkpoint message is created, an instance of the checkpoint reactive workflow is initialized. We usually refer to this process as initializing an instance of a \emph{Checkpoint Listener}. 

The checkpoint workflow similar to the view-change workflow can be started in two ways. The first being that the system has processed enough requests in the \ac{pbft} workflow to reach the checkpoint interval. The other way to start the checkpoint workflow is to receive a checkpoint message with a sequence number currently not in the checkpoint logger and has a higher sequence number than the last stable checkpoint from one of the other replicas. Both methods initialize the checkpoint certificate and checkpoint listener. However, a replica does not create and send its own checkpoint message until the replica reaches its own checkpoint interval. The checkpoint message created in the replica is also emitted to the checkpoint listener to allow it to be handled the same way as the other checkpoint messages received from the \ac{pbft} network. Unlike the protocol certificates, the checkpoint certificates are added to the checkpoint logger once it's been created, no validation is required. However, in order for a checkpoint to be deemed stable it needs to pass the certificate validation processes in the checkpoint listener, which follow the exact same guidelines as the protocol certificate. A replica can only have one stable checkpoint, meaning the previous stable checkpoint is overwritten if a new stable checkpoint with higher sequence is available. Afterall the goal of the checkpoint process is to garbage collect the protocol data from the logger up to the sequence number that is deemed stable. The garbage collection also includes removing any active checkpoints in the checkpoint logger that have lower or equal sequence number to the stable checkpoint certificate.

\subsubsection{Checkpoint Listener Workflow}
The source code for an instance of a checkpoint listener can be seen in \autoref{code:CreateCheckpoint}. The checkpoint listener works similarly to how reactive \code{Source} objects were used in the protocol workflow. The server once it receives a checkpoint message from the network emits the checkpoint message to the \code{Source<Checkpoint>} shared by the server and the checkpoint listener. The checkpoint listener listens for any item emitted by the server to the \code{Source<Checkpoint>} object. The reactive operations performed on the \code{Source<Checkpoint>} object can be seen on lines eight to 17. The checkpoint message received on the stream is first validated before the checkpoint certificate proof list is transformed to have the checkpoint message in its proof list. Unlike the protocol workflow and view-change workflow, each iteration of the checkpoint workflow is not required to finish its execution. In addition, the checkpoint functionality is performed separately to the protocol workflow, meaning it is possible to still process new requests while the checkpoint is created, assuming the protocol workflow has not exceeded the sequence number interval.
This means if the protocol processes enough requests, a new checkpoint checkpoint is created for a checkpoint with a higher sequence number than the previous one. This means it can be possible to have multiple checkpoint listeners active at the same time. However, it then becomes a race for the checkpoint certificates to see which one becomes the next stable checkpoint certificate. Although it is important to remember that the system does not process any new requests after it has exceeded the current sequence number interval. The reactive listener is finished when all of the reactive operators have been successfully finished, which requires the checkpoint certificate to be deemed stable. A checkpoint certificate is deemed stable once it has $2f+1$ unique and valid checkpoint messages in its proof list. The checkpoint messages must obviously match the checkpoint certificate sequence number and digest.

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:CreateCheckpoint, caption=Source code for the Checkpoint Listener, captionpos = b, basicstyle=\scriptsize]
public async CTask Listen(
CheckpointCertificate cpc, 
Dictionary<int, RSAParameters> keys, 
Action<CheckpointCertificate> finCallback
)
{
    Console.WriteLine("Checkpoint Listener: " + StableSeqNr);
    await CheckpointBridge
    .Where(check => check.StableSeqNr == StableSeqNr)
    .Where(check => check.Validate(keys[check.ServID]))
    .Scan(cpc.ProofList, (prooflist, message) =>
    {
        prooflist.Add(message);
        return prooflist;
    })
    .Where(_ => cpc.ValidateCertificate(FailureNr))
    .Next();
    finCallback(cpc);
}
    \end{lstlisting}
\end{figure}

\subsubsection{Initiate Garbage Collection}
The second part of the checkpoint functionality is rather simplistic. At the beginning when the replica initialized it's server functionality, an additional asynchronous function was initialized for listening on a reactive listener. This reactive listener is set to await for a new stable checkpoint certificate. Once the \code{Source} object receives a stable checkpoint certificate, the current stable checkpoint is overwritten by the one it received. Afterwards the garbage collection operations are performed. The source code for listening for stable checkpoint certificate can be seen in \autoref{code:ListenForCheckpoint}
The \code{Source<CheckpointCertificate>} object which is connected to this function is persisted on the server. The server has a predefined function specifically to use the Cleipnir scheduler to schedule an emit to this \code{Source} object. Each of the checkpoint workflow instances have a callback referanse to this function. Thereby allowing the checkpoint workflow to immediately use the callback address to schedule and emit their stable checkpoint to the system in order to replace the old stable checkpoint. After the garbage collection is completed, the sequence number interval is extended, allowing the protocol workflow to process more requests than before.

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:ListenForCheckpoint, caption=Reactive handler for new stable checkpoints, captionpos = b, basicstyle=\scriptsize]
public async CTask ListenForStableCheckpoint()
{
    Console.WriteLine("Listen for stable checkpoints");
    while (true)
    {
    	var stablecheck = await Subjects.CheckpointFinSubject.Next();
        Console.WriteLine("Update Checkpoint State");
        Console.WriteLine(stablecheck);
        StableCheckpointsCertificate = stablecheck;
        GarbageCollectLog(StableCheckpointsCertificate.LastSeqNr);
        GarbageCollectReplyLog(StableCheckpointsCertificate.LastSeqNr);
        GarbageCollectCheckpointLog(StableCheckpointsCertificate.LastSeqNr);
        UpdateRange(stablecheck.LastSeqNr);
     }
}
    \end{lstlisting}
\end{figure}

\subsubsection{Checkpoint Workflow Evaluation}

\iffalse 
The checkpointing process follows the \emph{checkpoint interval}. This means it only gets used once the system has processed a certain number of requests equal to the checkpoint interval. In our implementation the current checkpoint interval is set to five, meaning after processing five requests a new checkpoint is created. In our implementation we divided the workflow of the checkpointing into two parts.
The first part is the creation part, which is essentially initializing the checkpoint certificate to the last sequence number using the current application state as digest. In our implementation, we create the system digest from a persistent list which represent the current state of the system. The list contains the operation messages from each of the requests that has been fully processed by the \ac{pbft} protocol. So assuming no errors occurs, than the checkpoint for sequence number five will be the digest of a list containing the operation from requests one to five. After creating the checkpoint certificate and the checkpoint message, a checkpoint reactive listener is initialized. This reactive listener works similar to how reactive listeners worked in the protocol workflow. The server once it receives a checkpoint message from network will emit the checkpoint message to the \code{Source<Checkpoint>} registered in the server. The checkpoint listener will listen for any item emitted by the server and the given checkpoint message will be first validated before transforming the proof list of the checkpoint  certificate to be a proof list which has the checkpoint message. Unlike the protocol workflow, checkpoints can theoretically not be completed during execution and runs separate to the normal protocol workflow. This means if the protocol processes enough requests, a checkpoint checkpoint will be created with higher sequence number the previous one. This means it can be possible to have multiple checkpoint listeners active at the time. However, it becomes a race for the checkpoint certificates to see which one becomes next stable one. After the checkpoint listener has been created, than the replica will also emit its own local checkpoint to the checkpoint listener, meaning it will have to pass all of the same checks as the networked checkpoint messages has to. Since we're never sure which of the replica in the network is the fastest when it comes to setting up the checkpoint certificate, it means the server is also prepared to initialize the checkpoint processes if it receives a checkpoint with higher sequence number than the current stable checkpoint. Unlike the protocol certificates, the checkpoint certificates are added to the checkpoint logger once its been created, no validation is required. However, in order for a checkpoint to be deemed stable it will need to pass the certificate validation processes which follows the same guidelines as the protocol certificate. A replica can only have one stable checkpoint. The goal of the checkpoint process is to attempt to try replace this stable checkpoint, so we can garbage collect the protocol data from the logger. The garbage collection also includes active checkpoints in the checkpoint logger with lower or equal sequence number to the stable checkpoint certificate.
The second part of the checkpoint functionality is rather simplistic. Originally when the server side of a replica was initialized it also initialized another reactive listener, which is set to await for new stable checkpoint certificate. Once it receives a stable checkpoint certificate in the reactive \code{Source} object, it will overwrite the stable checkpoint registered on the system and then perform the garbage collection process. The \code{Source} is linked to the server and it schedules the new checkpoint certificate similar to schedules any message emit to the persistent layer. This is important because each checkpoint listener will have a reference to the callback function which schedules the emit to the \code{Source} object. This means that once the checkpoint certificate passes all of the reactive operators and the checkpoint is deemed valid, the callback function will be called with the resulting checkpoint certificate, which in turn will overwrite the stable checkpoint. Checkpoint process is then deemed successful and the garbage collection processes is started. The source code for the an instance of a checkpoint listener can be seen in \autoref{code:CreateCheckpoint}. The source code for listening for stable checkpoint certificate can be seen in \autoref{code:ListenForCheckpoint}
\fi

\section{Client}
The client implementation created for the \ac{pbft} implementation is a primitive console application that is interactable by the user. The client uses interactivity to create unique operations that are to be handled by the \ac{pbft} algorithm. In our current \ac{pbft} implementation we treat operations as simple strings objects, meaning mostly any assigned string value can be used as an operation value. Although an exception to this rule is that the operation cannot contain a pipeline symbol in the operation. This is because the pipeline symbol is used as an end delimiter for serialized messages in order to resolve a \ac{tcp} issue with messages being linked together. An operation is created though prompting the user for a value which in this case represents the value of the operation. 
Just like the replicas in the system, the client takes the network addresses stored in a \ac{json} file and then establishes a socket connection to each of the network addresses. Unfortunately, this means the client can not be initialized before the replicas.

In principle the workflow for the client implementation is very simple. The client starts by first initializing its connection to each of the replicas in the system based on the addresses found in the chosen \ac{json} file. Then the user is prompted for the value to be used in the operation. Once the operation is deemed valid, the client creates a new request message using the message provided by the user. The request is signed by the client's private key and then multicast to the replicas in the \ac{pbft} network. After a request is sent, the client waits for replicas to reply to the request the same way the normal workflow does for a phase shift. A reply certificate is created and the client uses a \code{Source<Reply>} to wait reactively to add new reply messages to the reply certificate until the certificate has received at least $f+1$ valid replies from different replicas. The $f+1$ criteria is known as a weak certificate, which is a certificate that can provide that at least \emph{f} non-faulty replica stored the request in its log~\cites[p.~9]{PAPER:PBFTRecovery}[p.~2]{PAPER:DPBFT}. Because the client is not part of the \ac{pbft} system it only requires \emph{f} number replies in order to guarantee the success of the request~\cites[p.~3]{PAPER:OGPBFT}[p.~9]{PAPER:PBFTRecovery}.

If the reply certificate receives $f+1$ replies from different replicas the certificate is stored in the client's log and the client application moves back to prompting the user for the next operation to be sent to the server. However, if the reply certificate does not become valid within a specific time duration, a timeout will occur and the request is once again multicasted to the \ac{pbft} network. This process is repeated until the $f+1$ criteria is met. Unfortunately, if the \ac{pbft} application gets stuck on one of the client operations, the server does not accept the resent request if another request is currently processed. Which could potentially lead to an endless loop. A way to get out of this loop would be for a view-change to occur, since the status information the replica has for the clients are reset after a view-change. This means the new request is treated as a new one and the entire processing starts anew.

The client shares a lot of the network related code with the \ac{pbft} replicas. The main difference lies in the client always has the responsibility for initiating the socket connection. This also means the client has to attempt to reconnect to a lost replica in the situation where a replica has previously been shut down. The reconnection attempt is done whenever the client is about to multicast a request to the \ac{pbft} network. In the case where the reconnection fails, the client simply moves onto the other replicas. The client does however, try once again to reconnect to the lost replica when the next replica is to be sent to the \ac{pbft} network. 

We decided to not include persistency for the client implementation. Despite this, the network portion of the client still uses the Cleipnir execution engine when it sends incoming replies from the network layer to its reactive listener. The reason for this is because scheduling the emit using the Cleipnir execution engine enforces synchrony. Enforcing synchrony helps the client avoid a potential race condition which can occur in this section of the code. We are currently not sure what is causing this issue. We are running the reactive listener completely outside of Cleipnir's influence, which means additional threads should not be created, despite still encountering race conditions in this section.