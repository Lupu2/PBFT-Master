\chapter{Implementation}

%Implementation 
%describe code, but not to detailed
%motivation for using this design
%good parts/bad parts
\label{chapter:Imp}
%Implementation talks about the actual algorithm implementation that is run in protocol execution, includes process of checkpoints and view-changes. Should have figures to simplify explanation. Go over briefly the different phases, show some pseudocode. keeps to take note of, maybe include a model to show the persistency layers present. The importance of using the Cleipnir scheduler and CTask, where you have used them etc.

%outline aka what needs to be talked about, note not in any particular order
%detailed explanation of how the general workflow for the PBFT algorithm is performed, with code snippets.
%the simplicity of some of the necessary tools needed in PBFT is handled. Object oriented programming for Messages, Certificates. Static function for workflow, including Listeners, and handlers
%detailed explanation for how the checkpointing are handled
%detailed explanation for how the view-changes are handled
%Describe how you though of persistency during implementation(not much since its not general focus, and ofcourse doesn't work fully)
%small description on how clients are working, how they use the same reactive operators to count number of replies received.
%Add comment on how well they work, don't work do to design, this is an evaluation afterall.
In this chapter we introduce our \ac{pbft} implementation. We will introduce the implementation for the request handler, normal protocol workflow, view-changes, and finally checkpointing. We will also discuss how the Cleipnir framework is used to create the working \ac{pbft} implementation, as well as discuss some benefits and limitations within the current implementation design.

\section{Design Choices}
%Motivation is supposed to be short and summarize these points:
%why did we implement our application the way we did, what was the main focus
%-Usage/testing the usage of the tools
%	-Simplicity --> Protocol description as close to possible
%-Some thought went into persistency due to Cleipnir, CTask for protocol workflow, persisted objects gets assigned Sync/Desync funcs
%-async used for networking

With the goal of the thesis in mind, the \ac{pbft} protocol workflows were designed to be as close defined to the protocol description as possible. To accomplish this, we believed the best approach would be to design protocol-related workflow as orderly as possible, meaning we generally want to use synchronous programming workflow whenever it is possible. Several factors persuaded us to focus on keeping the majority of the protocol operations synchronous. The first reason was that it is generally easier for developers to keep track of the program’s progress, making it easier to debug. Modern async/await workflow can generally achieve a similar program workflow to that of synchronous workflow. However, unless you are worried about blocking the main thread, there are no added benefits in terms of complexity or efficiency in using asynchronous programming over synchronous programming. 
The second reason is in regards to using Cleipnir for our protocol workflows. We previously discussed in \autoref{sec:persvsephe} and \autoref{section:PersistentProgramming} that using normal asynchronous operations inside a function that uses Cleipnir is not a good idea. Because we wanted to take advantage of reactive programming to handle protocol-related messages in our protocol workflow, we needed to use Cleipnir reactive framework. In addition, since persistency is a core part of the Cleipnir framework, we decided it was also best to keep persistency in mind while designing our application. Therefore we decided to keep our protocol implementations inside \code{CTask} functions. Because of this, the only form of asynchronous operations that are performed inside any of the protocol workflows are restricted to other \code{CTask} asynchronous operations. The \code{await} operator still works the same as it does for traditional asynchronous operations. Allowing us to take advantage of the \code{await} to set waiting points for \code{CTask} operations as well as ongoing reactive streams; Thereby giving the protocol workflows an abstraction to that of synchronous workflows, even though, in reality, it is an asynchronous process. 
To enable Cleipnir to persist in our protocol workflow, we also had to make sure that objects we wanted to persist in the protocol were persistable. To accomplish this, we initialized both a serializer and deserializer functions for Cleipnir that follows the format specified in \autoref{section:PersistentProgramming} to use for each of our defined protocol objects.  
Finally, to take advantage of the Cleipnir persistency functionality, we have done our best to avoid creating circular dependencies. Circular dependencies would essentially cause the serialization process to fail, as it would lead to two references that depend on one another. We believe there are no circular dependencies in our current implementation because the Cleipnir serialization process does not crash during Cleipnir’s synchronization process. However, if there does exist a circular dependency in our application, the server’s relationship with the protocol workflows would be our primary suspect. This is because the server emits messages to the protocol workflow while the protocol workflow has an object reference to the server. We believe they do not have a circular dependency because the server interacts with the protocol workflow through the Cleipnir execution engine and does not directly reference the protocol workflow. However, if our assumption is wrong, then this design would have to be changed in the future.

To make it easier to understand the protocol workflow, we believed the best approach was to keep only the protocol processes described in the descriptions centred inside a single function or class whenever it was possible. We chose this design primarily because we wanted to make the code as readable as possible. It was deemed especially important when designing the standard \ac{pbft} workflow. This design was not entirely possible to replicate for checkpoint and view-change workflows. We also attempted to keep operations unrelated to the protocol outside the protocol workflow. Although in some cases, we cannot avoid this issue. In these cases, a simple function call with a good function name must suffice to avoid increasing the complexity of the overall workflow.
An example of this is using the server to send protocol messages to \ac{pbft}. It is a fundamental part of the \ac{pbft} consensus algorithm to interchange protocol messages. However, the operations performed in the sending operation itself do not affect the protocol workflow. Therefore, a simple call to the servers \code{Multicast} using the newly created protocol message as a parameter should be decisive enough for readers of the workflow. 

Another important topic discussed in \autoref{sec:persvsephe} was the need to use the Cleipnir execution engine to schedule operations when operations outside of Cleipnir are required to affect persistent systems. To simplify this design in our application, we made practically all of the scheduled operations that use the Cleipnir execution engine be performed within the server. This design is chosen to make it easier to keep track of where the items are emitted to the protocol workflows. The server has several emit functions ready for scheduling the given message type to its desired \code{Source} object. In addition, all functionality in regards to handling and sending incoming protocol messages from the \ac{pbft} network to their respective protocol workflow is centred in its own class called \code{MessageHandler}. In order for the protocol workflows to emit their protocol messages and take advantage of this design, they are required to have a reference to the server object; so they can easily call the correct emit functions. Alternatively, the workflows need to have access to and make a call to a given callback reference that calls the desired emit function in the server. The second alternative here is quite useful when the operations for the protocol workflow are initialized in the server, making it easy to add a callback reference as an initializer parameter. The reactive operations for the checkpoints and view-change can potentially be initialized in the server, making it simple to assign the correct callback function. An example of this is seen in \autoref{code:viewListener} and \autoref{code:CreateCheckpoint} and are discussed in more detail later in \autoref{sec:protocolwork}

Due to our goal of testing the Cleipnir reactive framework, we have deliberately chosen to use the reactive framework every time our protocol workflows needed to wait for and handle protocol messages. This means the functionality for listening in for desired protocol messages and the functionality used to make valid certificates are handled using Cleipnir reactive framework. In addition, in certain areas, we have used Cleipnir reactive framework to implement event-handlers that are set to activate certain processes once a signal or item is sent to the desired process \code{Source} object.

Our \ac{pbft} implementation takes advantage of traditional asynchronous programming for the network layer. We chose this design primarily due to asynchronous programming being generally preferred for multi-client server design~\cite{VIDEO:AsyncConBack, DOC:AsyncAwait}. Considering a replica needed to handle multiple client requests and protocol messages from the other replicas, this seemed like the best choice. The network layer does not take advantage of Cleipnir reactive programming and persistency functionality. Therefore we do not have to worry about \code{CTask} either, meaning the network functionality all uses traditional \code{Task}.


\iffalse
In this chapter we introduce our \ac{pbft} implementation. We will introduce the implementation for the request handler, normal protocol workflow, view-changes, and finally checkpointing. We will also discuss how the Cleipnir framework is used to create the working \ac{pbft} implementation, as well as discuss some benefits and limitations within the current implementation design.

\section{Design Choices}
%Motivation is supposed to be short and summarize these points:
%why did we implement our application the way we did, what was the main focus
%-Usage/testing the usage of the tools
%	-Simplicity --> Protocol description as close to possible
%-Some thought went into persistency due to Cleipnir, CTask for protocol workflow, persisted objects gets assigned Sync/Desync funcs
%-async used for networking
With the goal of the thesis in mind, the \ac{pbft} protocol workflows were designed to be as close defined to the protocol description as possible. To accomplish this, we believed the best approach would be to design protocol-related workflow as synchronous as possible. Several factors persuaded us to focus on keeping the protocol operations mostly synchronous. The first reason was that it is generally easier for developers to keep track of the program progress, making it easier to debug. Modern async/await can generally achieve a similar program workflow. Still, unless you are worried about blocking the main thread, there are no added benefits in terms of complexity or efficiency in using asynchronous programming. The second reason is in regards to Cleipnir. We previously discussed in \autoref{sec:persvsephe} and \autoref{section:PersistentProgramming} using normal asynchronous operations inside a function that uses Cleipnir is not a good idea. Because we wanted to take advantage of reactive programming to handle protocol-related messages in our protocol workflow, we needed to use Cleipnir reactive framework. Since persistency is a core part of the Cleipnir framework, we decided it was best to keep persistency in mind while designing our application. Therefore we decided to keep our protocol implementations inside \code{CTask} functions. Because of this, the only form of asynchronous operations that are performed inside any of the protocol workflows are restricted to other \code{CTask} asynchronous operations. The \code{await} operator still works the same as it does for traditional asynchronous operations. This allows us to take advantage of the \code{await} to set waiting points for \code{CTask} operations as well as ongoing reactive streams; Thereby giving the protocol the abstraction of a synchronous process, even though, in reality, it is an asynchronous process. 
To enable Cleipnir to persist in our protocol workflow, we also had to make sure that objects we wanted to persist in the protocol were persistable. To accomplish this we initialized both a serializer and deserializer functions for Cleipnir that follows the format specified in \autoref{section:PersistentProgramming} to use for each of our defined protocol objects.  
Finally, we have done our best to avoid creating circular dependencies. Circular dependencies would essentially cause the serialization process to fail, as it would lead to two references that depend on one another. We believe there are no circular dependencies in our implementation because the Cleipnir serialization process does not crash during Cleipnir's synchronization process. However, in the case where it does exist a circular dependency in our application, the server and protocol workflow would be our primary suspect. This is because the server emits messages to the protocol workflow while the protocol workflow has an object reference to the server. We believe they do not have a circular dependency because the server interacts with the protocol workflow through the Cleipnir execution engine and does not directly reference the protocol workflow.
%The \ac{pbft} protocol description is orderly constructed. By this, we mean the tasks described in the protocol description can easily be made into a list and can be further split operations in the description based on which protocol phase it is performed. (come back when you have any idea where to go from here)

To make it easier to understand the protocol workflow, we believed the best approach was to keep only the protocol processes described in the descriptions centred inside a single function or class whenever it was possible. We chose this design primarily because we wanted to make the code as readable as possible. It was deemed especially important when designing the standard \ac{pbft} workflow. This design was not entirely possible to replicate for checkpoint and view-change workflows. We also attempted to keep operations unrelated to the protocol outside the protocol workflow. Although in some cases, we cannot avoid this issue. In these cases, a simple function call with a good function name must suffice to avoid increasing the complexity of the overall workflow.
An example of this is using the server to send protocol messages to \ac{pbft}. It is a fundamental part of the \ac{pbft} consensus algorithm to interchange protocol messages. However, the operations performed in the sending operation itself do not affect the protocol workflow. Therefore, a simple call to the servers \code{Multicast} using the newly created protocol message as a parameter should be decisive enough for readers of the workflow. 

Another important topic discussed in \autoref{sec:persvsephe} was the need to use the Cleipnir execution engine to schedule operations when operations outside of Cleipnir are required to affect persistent systems. To simplify this design, practically all of the scheduled operations using the Cleipnir execution engine are performed within the server. This design is chosen to make it easier to keep track of where the items are emitted to the \code{Source} objects. The server has several emit functions ready for scheduling the given message type to its desired \code{Source} object. In addition, all functionality in regards to sending incoming protocol messages from the \ac{pbft} to their respective protocol workflow is centred in its own class called \code{MessageHandler}. In order for the protocol workflows to emit their protocol messages and take advantage of this design, they are required to have a reference to the server object, so they can easily call the correct emit functions, or the workflow needs to make a call to a given callback reference that calls the desired emit function in the server. The second alternative here is quite useful when the operations for the protocol workflow are initialized in the server, making it easy to add a callback reference as an initializer parameter. The reactive operations for the checkpoints and view-change can potentially be initialized in the server, making it simple to assign the correct callback function. An example of this is seen in \autoref{code:viewListener} and \autoref{code:CreateCheckpoint} and are discussed in more detail later in \autoref{sec:protocolwork}

Our \ac{pbft} implementation takes advantage of traditional asynchronous programming for the network layer. We chose this design primarily due to asynchronous programming being generally preferred for multi-client server design~\cite{VIDEO:AsyncConBack, DOC:AsyncAwait}. Considering a replica needed to handle multiple client requests and protocol messages from the other replicas, this seemed like the best choice. The network layer does not take advantage of Cleipnir reactive programming and persistency functionality. Therefore we do not have to worry about \code{CTask} either, meaning the network functionality all uses traditional \code{Task}.
\fi

\iffalse
\section{Motivation}
With the goal of the thesis in mind, the \ac{pbft} protocol workflows were designed to be as close to the protocol description as possible. In order to accomplish this we believed the best approach would be to design protocol related workflow as synchronous as possible. In addition, in order to make it easier to understand the protocol workflow, we believe the best approach is to keep the protocol related code centred around single function or class when possible. This is deemed especially important when constructing the normal protocol workflow. This is because the normal protocol description is orderly constructed, going through different phases until consensus as been reached over the \ac{pbft} network. This could in theory also apply to the view-change description as it is divided into several detailed steps. However, there are several factors which leads to the view-change functionality being split into three separate, but nested, functions. The first reason being that view-changes requires the ability to restart the processes in the case where the process is stationary for too long. To handle this functionality we currently use a mix of timeout operations and goto statements in order to reroute the program flow back to the beginning of the view-change process~\cite{WEB:goto}. The second reason, which also applies to checkpoints, is that the view-change process can be initialized early by receiving view-change messages from other replicas. This may seem similar to protocol operations since its initialized by client requests, however the difference lie in the amount of messages required to initialize the process. Currently, the server needs to support the functionality of starting reactive listeners for view-changes if it ever receives a view-change, however the view-change process itself doesn't start until either the timeout occurs or the replica as received $2f$ messages. Because of this functionality, keeping the code completely synchronous and centered around a single is not possible. As for checkpoints, because the checkpoint processes can be initialized whenever, and majority of the timespent for checkpoints is simply waiting for a replica to receive $2f+1$ unique checkpoints with identical sequence numbers, the source code cannot be centered around a single function.

As we previously mention in \autoref{sec:persvsephe} and \autoref{section:PersistentProgramming} using normal asynchronous operations inside Cleipnir is not a good idea. Therefore, the only form of asynchronous operations performed inside any of the protocol workflows are restricted to other \code{CTask} operations. The \code{await} operator still works well with creating statemachines for asynchronous \code{CTask} operations and waiting for \code{Source} objects to finish all of its operators. Thereby giving the protocol an abstraction as an synchronous process, when in reality it is not. Other than that, traditional \code{Task} based asynchronous operations are well used in the network layer for our implementation. 

Another important topic discussed in \autoref{sec:persvsephe} was the need to use the Cleipnir execution engine to schedule operations when operations outside of Cleipnir required to affect the system within Cleipnir. To simply this design, practically almost all of the scheduled operations using the Cleipnir execution engine are performed within the server. This design is chosen to make it easier to keep track of the origin of the message emit. The server has several emit functions ready to schedule the given message type to its desired \code{Source} object. In order for the protocols workflow to take advantage of this design, they are either required to have a reference to the server object to call the function, or the workflow gets a callback referring to the emit function in the server. The second functionality is quite useful when the operations are initialized by the server. Checkpoints and view-change listeners tend to be initialized by the server, making it simple to assign the correct callback function.

There are currently exists two different implementations for checkpointing and view-changes. Both versions are still documented in the source code, where the second implementations have the letter 2 at the end of its name. The workflow for the both of them remain practically the same for both implementations. The main difference lies with how the implementations handle creating and handling certificates. Essentially the certificate is not deemed valid until it has received $2f+1$ unique and valid message for said message type. This processes is handled differently between the implementation. The second implementation performs the message validation, adding message to proof list and proof list validation over a \code{Source} object by using reactive operators. Meanwhile the first implementation performs these same operations for certificates inside the checkpoint certificate itself inside an append function. Once the certificates are deemed valid, another emit to \code{Source} object has to be made, so that the view-change and checkpoint operations are given the signal to continue with the next operations. The first implementation is required to have the callback function to the server emit message inside the certificate, while the second implementation simply has this function callback right after the reactive listener. The design was changed in order to accommodate the need for more reactive operations in the application. From our experience the second implementation generally performed better than the first implementation and was also for the most part more consistent. The first implementation sometimes encountered issues with the callback function, especially the view-change implementation.

As persistency is a core part of the Cleipnir it was decided to design the application to handle some form of persistency. This meant the protocol related operations had to be run in Cleipnir, which in turn meant the protocol had to be either synchronous or asynchronous using \code{CTask}'s. In addition objects run in the protocols are required to be persistable, meaning the objects needed to get a serializer function and deserializer function connected to Cleipnir. This also includes using Cleipnir inbuilt data structures when the persisted objects needed to also persist data structures. This is especially present in certificates since they need to persist their list of proofs, meaning the proof list uses the inbuilt Cleipnir \code{CList} to substitute normal list. Unfortunately there is an unfortunate oversight in our part when we designed the application. The application currently uses \ac{json}~\cite{WEB:NewJSON} to serialize and deserialize messages when sent over the \ac{pbft} network. \ac{json} formatting does not support inbuilt Cleipnir data structures, which is not surprising yet was not directly discovered until very late into the \ac{pbft} implementation. Currently this issue is solved by converting between traditional data structures and inbuilt Cleipnir data structures whenever a message with said data structure needs to be serialized by \ac{json}. The conversation itself is rather simple, a copy of the data storage in the traditional data structure format is created, then the content is copied from the source data storage to the newly created data storage. The process is then reversed on the receiver end after the message as been properly deserialized. Although in retrospect, it might have been more beneficial if the serialization and deserialization for the networking were the same used by Cleipnir as to avoid this issue in the future. Finally, we have done our best to avoid creating circular dependencies. Circular dependencies would essentially cause the serialization process to fail, as both references depend on each other. We do believe there currently are no circular dependencies in our implementation, because the Cleipnir serialization process does not crash during Cleipnir's synchronization process. The server and protocol workflow relationship can potentially be too close to being one. The server emits messages to the protocol workflow while the protocol workflow has a reference to the server. The only reason this does not create a circular dependency is because the server interacts with the protocol workflow through the Cleipnir execution engine and does not have a direct reference to the protocol workflow. Currently the \ac{pbft} implementation does not currently support functional persistency. The reason for as to why the persistency  does not work completely is uncertain. There are two main issues as of now. The first issue is that protocol logger for some reason does not persist all the entries in the logger. As of now, no distinguishable pattern as been found with the data which is lost. Either way, losing certificates in the logger does create big problems. The other reason is that some of the \code{Source} objects linked to the server gets duplicated, meaning that in the system there exist two \code{Source} objects with the exact same reference. This becomes a problem, because each time the server emits a message to the \code{Source} object, it will emit the message to both the original and duplicate \code{Source}. This in turn can essentially cause two identical iterations of the protocol to occur. This even includes even storing the resulting protocol certificates to the logger with the same sequence. As it is not ever intended to store four certificates to a single sequence number, it is very clearly not working properly. 
%As our main goal for this thesis was to evaluate the tools given and not focus on making the implementation persistable, we decided on prioritising other factors of our implementation. We do believe however, that if it were possible to solve the previously mention issues, that the current implementation has a decent (word for solid background for getting it to work). 
\fi

\section{Workflow Details}
\label{sec:protocolwork}
\subsection{Protocol Workflow Implementation}

\subsubsection{Starting protocol instance}
A normal sequence for the \ac{pbft} implementation begins once the request handler receives a request message from the server. The source code for the request handler can be seen in \autoref{code:StartProtocol}. The request handler listens for new requests messages emitted to the \code{Source} object \emph{requestMessage} as seen on line 3. As mentioned in \autoref{sec:persvsephe}, the server is tasked with emitting messages received in the network layer to the appropriate \code{Source} object for the protocol to access the message. The request handler is responsible for making sure that the request received is valid. In addition, the request handler only starts a new iteration of the \ac{pbft} protocol when the next sequence number is within the current sequence number interval. This condition is handled by the if condition spanning lines 4-10. Finally, a new protocol instance is not initialized when the system is performing a view-change. This is determined by the boolean value \code{active} which is tied to the protocol execution object. Once all checks are passed, the request handler updates and collects the current sequence number. Then it calls the asynchronous \code{CTask} function \emph{PerformProtocol} which initializes and starts the \ac{pbft} protocol for the given request. It is important that the request handler is not forced to wait for the \code{PerformProtocol} function to finish because the application must have access to the \emph{requestMessage} \code{Source<Request>} object. This is because we desire an application that can process multiple requests from clients at the same time. If the application does not have access to the \emph{requestMessage} for a long period, then it is likely that a request message emitted by the server gets lost.

%\paragraph{Testsec1}
%\paragraph{Testsec2}
\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:StartProtocol, caption=Code section from the request handler, captionpos = b, basicstyle=\scriptsize]
while (true)
{
    var req = await requestMessage.Next();
    if (Crypto.VerifySignature(
            req.Signature, 
            req.CreateCopyTemplate().SerializeToBuffer(), 
            serv.ClientPubKeyRegister[req.ClientID]
        ) 
        && serv.CurSeqNr < serv.CurSeqRange.End.Value
    )
    {
        if (execute.Active)
        {
            int seq = ++serv.CurSeqNr;
            Console.WriteLine("Curseq: " + seq + " for request: " + req);
            _ = PerformProtocol(execute, serv, scheduler, shutdownPhaseSource, req, seq);
        }
    }
}
	\end{lstlisting}
\end{figure}

\subsubsection{Pre-Prepare phase}
\label{sec:prepare}
\iffalse
The pre-prepare phase is the only part of the normal operation workflow that has a different structure depending on whether or not the replica is the primary replica. The source code for the primary replica`s pre-prepare phase can be seen in \autoref{code:Pre-PreparePrimary}. If the replica is the primary, it uses the sequence number that the protocol instance was initialized with and creates a pre-prepare message for this sequence number. The pre-prepare message also contains information regarding the primary’s server id, current view, and the request digest. The pre-prepare message dictates the other replicas sequence number for the processing of the given request. The primary then initializes the protocol certificate used for storing the proof of the prepare phase. Since the first received phase message in the prepare phase is always supposed to be the pre-prepare message, the protocol certificate used for the prepare phase always has the pre-prepare message as its first entry in its proof list. The protocol instance then uses the server reference to multicast the pre-prepare message to the other replicas in the network. 

The source code for the pre-prepare phase for the non-primary replicas is shown in \autoref{code:Pre-PrepareNonPrimary}. The non-primary replica starts its protocol instance by subscribing to the \code{Source<PhaseMessage>} \emph{MesBridge} and listens for incoming phase messages. The subscribe, listening, and handling process of the incoming items to the \emph{MesBridge} is performed on lines 3-12. Considering the replicas only want a pre-prepare message in this reactive listener, it uses a \code{WHERE} clause to ignore any other phase message other than ones that use the pre-prepare messages enum type. In addition, another \code{WHERE} clause is assigned to avoid any pre-prepare messages designated for other requests by comparing request digests. Therefore an incoming phase message can only pass the \code{WHERE} clause if it involves the same request which the protocol instance is processing. The final \code{WHERE} clause validates the phase message where the validation criteria are the same as the ones mentioned in \autoref{sec:detailedProtocol} for pre-prepare messages. Once the replica receives a pre-prepare phase message which passes all the \code{WHERE} clauses, it creates a protocol certificate that uses the same sequence number as the primary`s pre-prepare phase message. Each protocol certificates for the prepare phase now have a matching sequence number for each replica. The non-primary replica finally ends the pre-prepare phase and starts the prepare phase by creating a prepare message and multicasting this message using the same method the primary used for multicasting its pre-prepare phase.

The \code{MERGE} operator is used to ensure that the protocol execution is terminated if a view-change occurs. If the timeout occurs, a unique phase message is emitted to the \code{Source<PhaseMessage>} \emph{ShutdownBridgePhase}. The \code{MERGE} operator binds \emph{ShutdownBridgePhase} reactive stream together with the \emph{MesBridge} stream. This means it is now possible for the \emph{MesBridge} to be unsubscribed without having to pass the previous operators in the reactive chain. This scenario only applies whenever a phase message is detected in the \emph{ShutdownBridgePhase}. As the \code{Merge} operator is the last reactive operator in the chain, the stream returns the phase message received from the \emph{ShutdownBridgePhase} as the resulting phase message. Because this phase message is intentionally faulty, it is not allowed to be used in the prepare phase of the protocol. Therefore a timeout exception is instead called, which closes the instance of the protocol execution.

The design chosen for the source code to the pre-prepare phase is simple and follows a synchronous workflow as we desired, making it easier for developers to write. Unfortunately, there are two severe issues with our current implementation of the pre-prepare phase. These issues are caused by a combination of splitting the code based on primary versus non-primary and the importance of initializing instances of the reactive listeners early. Both problems are theoretically very similar as they both are caused by improper initialization of the reactive listeners used in the \ac{pbft} implementation. The first issue occurs when the primary sends out its pre-prepare phase message before the non-primary replicas have initialized the pre-prepare reactive listener. This results in the pre-prepare phase message not being received by the non-replica, which means it fails the pre-prepare phase. As the pre-prepare phase fails, the timeout will occur, which puts the replica into view-change mode as it believes the primary replica is faulty. The second issue is that a non-primary can receive a prepare message before it has received the initial pre-prepare message from the primary. When this situation occurs, the prepare message gets filtered out by the pre-prepare reactive listener and is therefore not available once this non-primary reaches the prepare phase. In the worst-case scenario, the replica loses all of the prepare phase messages from the other replicas, meaning the protocol instance is stuck in the prepare phase once it finally receives its pre-prepare message.

These issues just discussed are caused primarily because the application struggles with handling phase messages that are received out of intended order. There are several workarounds to handle messages that arrive out of order. However, most of the workarounds available would require adding a lot more complexity to the implementation. As our goal for this thesis is to create an \ac{pbft} implementation that is very simple and accurate to the protocol description, we decided not to redesign the protocol workflow to handle issues regarding pre-prepare messages out of order. As it is meant for the pre-prepare message to get the other non-primary to start processing the request by providing the correct sequence number, we feel it would not be faithful to the original algorithm to change this design. One workaround to this issue would be to initialize the prepare phase reactive listeners at the start of the workflow. Once the pre-prepare message is received, the reactive listener for the prepare messages that did not have the same sequence number that matches the received pre-prepare message would be filtered out. Currently, to somewhat mitigate this issue, the primary is forced to wait for at least a second before starting to multicast its pre-prepare message. Performing this waiting period allows the other replicas to catch up. Which makes it less likely that a replica is far enough behind to lose out on prepare messages before completing handling their pre-prepare message. With this workaround, the issues discussed are, for the most part, stable. As an estimate, an average of 15 operations can be progressed without incident before a user encounters these issues.
\fi

The pre-prepare phase is the only part of the normal operation workflow that has a different structure depending on whether or not the replica is the primary replica. The source code for the primary replica’s pre-prepare phase can be seen in \autoref{code:Pre-PreparePrimary}. If the replica is the primary, it uses the sequence number that the protocol instance was initialized with and creates a pre-prepare message for this sequence number. The pre-prepare message also contains information regarding the primary’s server id, current view, and the request digest. The pre-prepare message dictates the other replicas sequence number for the processing of the given request. The primary then initializes the protocol certificate used for storing the proof of the prepare phase. Since the first received phase message in the prepare phase is always supposed to be the pre-prepare message, the protocol certificate used for the prepare phase always has the pre-prepare message as its first entry in its proof list. The protocol instance then uses the server reference to multicast the pre-prepare message to the other replicas in the network. 

The source code for the pre-prepare phase for the non-primary replicas is shown in \autoref{code:Pre-PrepareNonPrimary}. The non-primary replica starts its protocol instance by subscribing to the \code{Source<PhaseMessage>} \emph{MesBridge} and listens for incoming phase messages. The subscribe, listening, and handling process of the incoming items to the \emph{MesBridge} is performed on lines 3-12. Considering the replicas only want a pre-prepare message in this reactive listener, it uses a \code{WHERE} clause to ignore any other phase message other than ones that use the pre-prepare messages enum type. In addition, another \code{WHERE} clause is assigned to avoid any pre-prepare messages designated for other requests by comparing request digests. Therefore an incoming phase message can only pass the \code{WHERE} clause if it involves the same request which the protocol instance is processing. The final \code{WHERE} clause validates the phase message where the validation criteria are the same as the ones mentioned in \autoref{sec:detailedProtocol} for pre-prepare messages. Once the replica receives a pre-prepare phase message which passes all the \code{WHERE} clauses, it creates a protocol certificate that uses the same sequence number as the primary’s pre-prepare phase message. The protocol certificate for the prepare phase now has a matching sequence number for each replica. The non-primary replica finally ends the pre-prepare phase. The normal workflow implementation starts the prepare phase by creating a prepare message and multicasting this message using the same method the primary used for multicasting its pre-prepare phase.

The \code{MERGE} operator is used to ensure that the protocol execution is terminated if a view-change occurs. If the timeout occurs, a unique phase message is emitted to the \code{Source<PhaseMessage>} \emph{ShutdownBridgePhase}. The \code{MERGE} operator binds \emph{ShutdownBridgePhase} reactive stream together with the \emph{MesBridge} stream. This means it is now possible for the \emph{MesBridge} to be unsubscribed without having to pass the previous operators in the reactive chain. This scenario only applies whenever a phase message is detected in the \emph{ShutdownBridgePhase}. As the \code{Merge} operator is the last reactive operator in the chain, the stream returns the phase message received from the \emph{ShutdownBridgePhase} as the resulting phase message. This phase message is intentionally faulty and is not allowed to be used in the prepare phase of the protocol. Therefore once this faulty phase message is received, a timeout exception is instead called, which closes the instance of the protocol execution.

The design chosen for the source code to the pre-prepare phase is simple and follows a synchronous workflow as we desired, making it easier for developers to write. Unfortunately, there are two severe issues with our current implementation of the pre-prepare phase. These issues are caused by a combination of splitting the code based on primary versus non-primary and the importance of initializing instances of the reactive listeners early. Both problems are theoretically very similar as they both are caused by improper initialization of the reactive listeners used in the \ac{pbft} implementation. The first issue occurs when the primary sends out its pre-prepare phase message before the non-primary replicas have initialized the pre-prepare reactive listener. This results in the pre-prepare phase message not being received by the non-replica, which means it fails the pre-prepare phase. As the pre-prepare phase fails, the timeout will eventually occur, which puts the replica into view-change mode as it believes that the primary replica is faulty. The second issue is that a non-primary can receive a prepare message before it has received the initial pre-prepare message from the primary. When this situation occurs, the prepare message gets filtered out by the pre-prepare reactive listener and is therefore not available once this non-primary reaches the prepare phase. In the worst-case scenario, the replica loses all of the prepare phase messages from the other replicas, meaning the protocol instance is stuck in the prepare phase once it finally receives its pre-prepare message.

These issues just discussed are caused primarily because the application struggles with handling phase messages that are received out of intended order. There exist several workarounds to handle messages that arrive out of order. However, most of the workarounds available would require adding a lot more complexity to the implementation. As our goal for this thesis is to create an \ac{pbft} implementation that is very simple and accurate to the protocol description, we decided not to redesign the protocol workflow to handle issues regarding pre-prepare messages out of order. As it is meant for the pre-prepare message to get the other non-primary to start processing the request by providing the correct sequence number, we feel it would not be faithful to the original algorithm to change this design. Once the pre-prepare message is received, the reactive listener for the prepare messages that did not have the same sequence number that matches the received pre-prepare message would be filtered out. Currently, to somewhat mitigate this issue, the primary is forced to wait for at least a second before starting to multicast its pre-prepare message. Performing this waiting period allows the other replicas to catch up. Which makes it less likely that a replica is far enough behind to lose out on prepare messages before completing handling their pre-prepare message. With this workaround, the issues discussed here are, for the most part, stable. As an estimate, an average of 15 operations can be progressed without incident before a user encounters these issues.


\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:Pre-PreparePrimary, caption= Source code for pre-prepare phase for primary replica, captionpos = b, basicstyle=\scriptsize]
ProtocolCertificate qcertpre;
byte[] digest = Crypto.CreateDigest(clireq);
int curSeq; 
if (Serv.IsPrimary()) //Primary
{
    curSeq = leaderseq;
    Console.WriteLine("CurSeq:" + curSeq);
    Serv.InitializeLog(curSeq);
    PhaseMessage preprepare = new PhaseMessage(
        Serv.ServID, 
    	curSeq, 
        Serv.CurView, 
        digest, 
        PMessageType.PrePrepare
    );
    Serv.SignMessage(preprepare, MessageType.PhaseMessage);
    qcertpre = new ProtocolCertificate(
        preprepare.SeqNr, 
        preprepare.ViewNr, 
        digest, 
        CertType.Prepared, 
        preprepare
    );
    await Sleep.Until(1000);
    Serv.Multicast(preprepare.SerializeToBuffer(), MessageType.PhaseMessage);
}
	\end{lstlisting}
\end{figure}

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:Pre-PrepareNonPrimary, caption= Pre-prepare phase for non-primary replica, captionpos = b, basicstyle=\scriptsize]
else	//Not Primary
{ 
    var preprepared = await MesBridge
    	              .Where(pm => pm.PhaseType == PMessageType.PrePrepare)
                      .Where(pm => pm.Digest != null && pm.Digest.SequenceEqual(digest))
                      .Where(pm => pm.Validate(
                        Serv.ServPubKeyRegister[pm.ServID],
                        Serv.CurView, 
                        Serv.CurSeqRange)
                       )
                       .Merge(ShutdownBridgePhase)
                       .Next();
                
    if (preprepared.ServID == -1 && preprepared.PhaseType == PMessageType.End) 
        throw new TimeoutException("Timeout Occurred! System is no longer active!");
    qcertpre = new ProtocolCertificate(
        preprepared.SeqNr, 
        Serv.CurView, 
        digest, 
        CertType.Prepared, 
        preprepared
    );
    curSeq = qcertpre.SeqNr; 
    Serv.InitializeLog(curSeq);
    PhaseMessage prepare = new PhaseMessage(
        Serv.ServID, 
        curSeq, 
        Serv.CurView, 
        digest, 
        PMessageType.Prepare
    );
    Serv.SignMessage(prepare, MessageType.PhaseMessage);
    qcertpre.ProofList.Add(prepare);
    Serv.Multicast(prepare.SerializeToBuffer(), MessageType.PhaseMessage);
}
	\end{lstlisting}
\end{figure}		

\iffalse
\subsubsection{Prepare phase}
In comparison to the Pre-prepare phase and the start of the prepare phase, the rest of the workflow in the implementation is relatively stable and straightforward. The prepare and commit phase source code can be seen in \autoref{code:PrepareAndCommit}. The first step of the prepare phase is to initialize the reactive listeners for prepare and commit phase messages. Due to the listeners having several reactive operators connected to their stream, the code must span several code lines to make it more readable. The prepare listener is initialized on lines two to 18, and the commit listener is initialized on lines 25 to 42 in \autoref{code:PrepareAndCommit}. There are two reasons why the reactive listeners for prepare and commit messages are initialized early. The first reason is to reduce the time it takes for the workflow to move from the pre-prepare listener to the following reactive listeners. This time needs to be small to avoid losing potential incoming phase messages to the reactive streams. 
The other reason is to avoid ordering issues between prepare and commit messages. Since the sequence number for the workflow has already been determined during the pre-prepare phase, the prepare and commit phase can initialize their reactive streams early and be active simultaneously. Because of this, the prepare and commit phase does not suffer issues in regards to phase messages being out of order. If the pre-prepare message did not dictate the sequence number for non-primary replicas, this would have also been the ideal design for handling phase messages during the pre-prepare phase.

The reactive listeners used for the prepare phase and the commit phase are almost practically identical. The only significant difference between the two is that they only accept phase messages in the stream with their respective protocol phase. Meaning the reactive listener for the prepare phase filters away phase messages that do not have protocol phase-type prepare. This operation is performed by the first \code{WHERE} clause. In addition, the certificates for both protocol phases are also initialized early. This is because the certificates are now actively updated through the operations in the reactive listeners’ stream instead of returning the emitted phase message.

During the prepare phase, the workflow waits until the prepare certificate has added $2f+1$ unique prepare phase messages to its proof list. In order for a phase message to be added to the prepare certificate, it must pass all of the \code{WHERE} clauses assigned for the reactive listener. In actuality, the workflow only waits for $2f$ prepare phase messages due to the pre-prepare message already been added to the protocol certificate in the pre-prepare phase. Once a valid phase message passes all of the first \code{WHERE} operators, it is added to the designated protocol certificate using the \code{SCAN} operator. The \code{SCAN} operator transforms the certificate’s proof list to include the incoming phase message.  The final \code{WHERE} clause determines whether or not the certificate has reached a sufficient number of valid phase messages in its proof list.
The \code{ValidateCertificate} function essentially calculates the number of phase messages inside the proof list when it excludes duplicates. It also makes sure that the phase messages in the list are indeed valid. The asynchronous \code{await} operator on line 45 is used to wait for the \emph{CAwaitable} in the prepare phase reactive listener to finish all of the linked operators for the listener before moving on with the protocol. Once the validation process has succeeded for the protocol certificate, the workflow can move past the \code{await} operator. The prepare phase finishes after the prepare protocol certificate is added to the protocol log in the server on line 47.

\subsubsection{Commit Phase}
As for the commit phase, like the other protocol phases, the first step is to have each replica create a commit phase message and use the server to multicast the commit phase over the \ac{pbft} network. Afterwards, the commit phase performs practically the same operations as the prepare reactive listener. The commit reactive listener waits for the proof list for the commit certificate to have at least $2f+1$ commit phase messages. The reactive listener for the commit phase has an additional \code{WHERE} clause that makes sure that the prepare phase has already finished which is visible on line 41 in \autoref{code:PrepareAndCommit}. This extra \code{WHERE} clause is used to avoid the commit certificate from being finished before the prepare phase is complete.  After the commit certificate is successfully validated, the protocol workflow is almost finished processing the given request. The protocol workflow first adds the commit certificate to the protocol logger as done prior to the prepare certificate before starting the remaining operations in the protocol workflow. The server now has two valid certificates for the given sequence number assigned to the client request, meaning the replica has the necessary proof that the replicas in the \ac{pbft} network agree to have the application perform the operation in the request. The application finally performs the operation within the request. The last remaining process is to create a reply message, digitally sign this reply message and send the reply message to the client who initially sent the processed request. The reply message includes information to the client in regards to whether the operation given in the original request was completed successfully or not. In our \ac{pbft} implementation, the only  operation the application can do is to write the 'operation' received from the request to the console window and add the operation to a persistent list. The persistent list representing the application state is discussed more in \autoref{section:ImpCheckpointing}.


\subsubsection{Protocol Workflow Evaluation}
%Insert whether you believe the code for each section is defined as good code, explain why. How did usage async, reactive operation help/hinder the protocol workflow
%Around 135 lines of code excluding extra lines for initializing objects. Where 30\% is from the reactive operators.
We managed to achieve our goal of creating an implementation that performs the standard processes of the \ac{pbft} algorithm into a single function. We believe our resulting implementation is relatively faithful to the \ac{pbft} protocol when based only on the protocol description. The operations listed in the protocol workflow follow a synchronous workflow, excluding the operations performed by the reactive operators, making it easier to read the code. The reactive operators can still work well together with the synchronous workflow by completing the required checks and operations on the incoming phase messages independently from the rest of the protocol workflow. By taking advantage of the \code{await} command, we can easily mark the areas in the protocol workflow where we know the protocol workflow cannot function without the result from the reactive operators. We believe the most significant benefit for our design in the \ac{pbft} protocol workflow within a single function is that it became a lot easier to keep track of the protocol operations. For example, due to how the implementation handles the workflow that creates the protocol certificates, it is considerably straightforward to differentiate between the \ac{pbft} protocol phases by looking at the source code. Basically, by looking for the \code{await} points in the protocol workflow, we can approximately determine where one of the three protocol phases finishes. It is an approximation since there are still a couple of operations required to be performed, such as adding the certificate to the protocol log for the prepare and commit phases.  We would argue that it is a significant challenge to simplify the code to prepare and commit phases further without causing a severe issue for protocol workflow. Most of the complexity we currently have in our implementation comes from the fact that the primary and non-primary replicas have different operations in the pre-prepare phase. In addition, our current stop functionality is not exactly straightforward, which further hurts the simplicity of our implementation. We also believe that despite the functionality of the reactive operators being convenient for handling protocol messages, they may be difficult for inexperienced programmers to read. The programmers should at the very least have some fundamental knowledge in regards to chaining operators using query languages such as \ac{sql}, preferably knowing the fundamentals of \ac{linq}~\cite{WEB:sql} statements, to fully grasp most of the reactive operations available in the Cleipnir framework. The normal workflow implementation is around 135 lines of code when we exclude any additional spaces used to make object initialization easier for others to read. In addition, approximately 30\% of the lines are used for the reactive operators. All in all, based on these results, we would argue that the implementation is relatively short to be able to handle all three protocol phases inside a single function. However, an apparent problem with keeping the functionality in this format was the difficulty of handling protocol messages out of order, which forces the developer to deploy workarounds to avoid this problem. We choose to initialize the \code{Source} objects as soon as possible to reduce the number of phase messages dropped. Unfortunately, we cannot deal with the pre-prepare phase messages due to only being used by non-primary replicas, which is a big downside to using the desired format with reactive operators.

%asynchronous operations
Due to us performing the protocol workflow inside \code{CTASK} functions, we are not able to use traditional asynchronous operations inside the protocol workflow. This typically means tasks regarding reading data from files, server requests, networking, or any other job that is preferred to be performed asynchronously should do so outside the protocol workflow. A developer would therefore need to keep this in mind when designing the protocol-related workflows. Still, regardless of whether or not the protocol workflow is performed inside a \code{Task} or a \code{CTask}, the workflow is run asynchronously, allowing us to effortless run instances of the protocol workflow separately. Although, like when using threads, we need to ensure that the separate asynchronous functions do not alter the same properties if the execution order matters. Otherwise, the result of the application state would become unpredictable. Although our \ac{pbft} implementation does add the two resulting protocol certificates to a shared log, the sequence number assigned to the protocol workflow is unique for each iteration of the \ac{pbft} workflow, allowing us to avoid corrupting the protocol state. This is because all of the iterations will have a unique key to store their protocol information. Despite being run in a \code{CTask}, the \code{await} operator is still valuable for the protocol workflow as it is used to wait for the certificates to finish. Without the ability to use the \code{await} operator, we would not have been able to create the desired protocol workflow.
All in all, the asynchronous workflow may hinder the developer from performing certain operations directly inside the workflow. It is still beneficial when looking at the benefits of running the protocol workflow asynchronously. The most significant advantage of running the workflow asynchronous is how simple it is to start multiple iterations of the protocol workflow. In addition, running the protocol workflow asynchronous should scale better for numerous clients in comparison to creating separate threads.

%reactive:
Cleipnir reactive framework was handy for handling protocol workflow received from the server. Although we did have quite the big issue with the usage of \code{Source} objects to create the protocol certificate before using the Cleipnir execution engine to schedule the emits. Once we moved on to using universal formatting for emitting items to the workflows in Cleipnir, it has worked as intended. It was initially challenging to use a couple of Cleipnir reactive operators such as \code{Merge} and \code{Scan}, but relatively simple to learn the general approach of chaining reactive operations. Using \code{Source} objects for sending the protocol messages to their appropriate code section is simple once Cleipnir execution engine was used to schedule the emits in order. From our experience keeping the emit functionality centred to a designated object or class that has reference to the execution engine is the recommended structure. Although, we must consider one aspect when we use reactive programming to handle protocol messages. Each protocol message, regardless of the owner, must all be sent to the protocol workflow in the same way. In our case, all phase messages had to be emitted to the \code{Source} object for the phase message to be validated correctly. This includes its own phase message, meaning a functionality must be available for the protocol workflow to emit phase messages created during protocol workflow. The main advantage of using Cleipnir reactive framework to handle protocol messages for the protocol workflow is that we can structure all of the code related to the phase message inside a single block of code. In addition, due to the nature of the reactive operations chain, it is easy to control the order of the operations that need to be on the protocol message. Finally, combined with the \code{await} operator, we can also easily dictate the waiting for the condition to be met scenario of the consensus algorithm. In our case, the creation of valid protocol certificates by submitting validated proofs until reaching the quota.
To summarize the main benefits, the Cleipnir reactive framework provided our normal \ac{pbft} workflow implementation was a simple way to validate and collect phase messages to create valid protocol certificates.  In addition, to making the collaboration between the network layer and the protocol layer easier to develop. The downsides being that they struggle with handling phase messages that are received out of order. Therefore, as a consequence must be initialized as soon as possible to counteract this issue. 
\fi
%\subsubsection{Protocol Workflow Evaluation}



\iffalse
\subsubsection{Prepare phase and Commit phase}
In comparison to the Pre-prepare phase and starting the prepare phase, the rest of the workflow in the implementation is relatively stable and straightforward. The prepare and commit phase source code can be seen in \autoref{code:PrepareAndCommit}. The first step is to initialize the reactive listeners for prepare and commit phase messages. This is done early for two reasons! The first is to mitigate the time between waiting for pre-prepare messages and prepare messages as two avoid potentially losing prepare messages. The other reason is so that the protocol can listen for both prepare messages and commit messages, which means there aren't any ordering issues between messages during the prepare and commit phase. If the pre-prepare message did not dictate the sequence number for non-primary replicas, this would have also been the ideal design for handling pre-prepare message. 

The reactive listener used for the prepare phase are pretty much the same for both phases. The only major difference between the two is that they only accept phase messages for their respective protocol phase. Additionally a commit certificate is initialized early to be used together with the commit reactive listener. Since the prepare messages and prepare certificates are already been initialized in the pre-prepare phase, there is only one more thing to do in the prepare phase. The prepare phase will wait until the prepare certificate as received $2f+1$ unique prepare phase messages which passes all of the \code{WHERE} clauses assigned. In actuality it is to wait for $2f$ prepares and one pre-prepare message. To add the valid phase messages to the designated certificates, we use the \code{SCAN} operator to transform the original proof list for the certificate to include the messages received in the reactive listener. The final \code{WHERE} clause determines whether or not the certificate has received the desired number of unique valid phase messages. Essentially calculating the number of phase messages inside the proof list excluding duplicates, and making sure the phase messages in the list are valid. The asynchronous \code{await} operator is used to wait for the \emph{CAwaitable} to finish this all of the operators linked to the prepare reactive listener before moving on with the protocol. Once the prepare certificate has succeeded its validation process, the prepare certificate is added to protocol log in the server and the commit phase officially starts. 

Like the other phases, the first step will be for each replica to create their commit phase messages and use the server to multicast their commit phase over the \ac{pbft} network. Afterwards the protocol will wait for the proof list for the commit certificate to reach $2f+1$. This rule applies to each of the replica as there are no difference in operations between primaries and non-primaries in the commit phase. The reactive listener for the commit phase will additionally check that prepare phase as already finished validating as to avoid finishing the commit certificate before the protocol certificate. However, this extra check does not effect the protocol workflow in either way, since the protocol certificate is awaited earlier in the process. After the commit certificate is successfully validated, the protocol workflow is essentially completed. The remaining operations performed in the protocol execution is to add the commit certificate to the logger similar to the prepare certificate. The server will now have two valid certificates for the given sequence number to request, meaning the replica now has proof that the protocol was successful for the given request. Finally a reply message will be created, signed and sent to the client that sent the processed request. Additionally the operation within the request will be performed by the application. In our \ac{pbft} implementation the 'operation' will simply be to write the operation to the console window and add the operation to a persistent list. The persistent list representing the application state will be more discussed in \autoref{section:ImpCheckpointing}.
\fi

\subsubsection{Prepare phase}
In comparison to the Pre-prepare phase and the start of the prepare phase, the rest of the workflow in the implementation is relatively stable and straightforward. The prepare and commit phase source code can be seen in \autoref{code:PrepareAndCommit}. The first step of the prepare phase is to initialize the reactive listeners for prepare and commit phase messages. Due to the listeners having several reactive operators connected to their stream, the code must span several code lines to make it more readable. The prepare listener is initialized on lines 2 -18, and the commit listener is initialized on lines 25-42 in \autoref{code:PrepareAndCommit}. There are two reasons why the reactive listeners for prepare and commit messages are initialized early. The first reason is to reduce the time it takes for the workflow to move from the pre-prepare listener to the following reactive listeners. This time needs to be small to avoid losing potential incoming phase messages to the reactive streams. 
The other reason is to avoid ordering issues between prepare and commit messages. Since the sequence number for the workflow has already been determined during the pre-prepare phase, the prepare and commit phase can initialize their reactive streams early and be active simultaneously. Because of this, the prepare and commit phase does not suffer issues in regards to phase messages being out of order. If the pre-prepare message did not dictate the sequence number for non-primary replicas, this would have also been the ideal design for handling phase messages during the pre-prepare phase.

The reactive listeners used for the prepare phase and the commit phase are almost practically identical. The only significant difference between the two reactive listeners is that they only accept phase messages in the stream with their respective protocol phase. For example, the reactive listener for the prepare phase filters away phase messages that do not have protocol phase-type prepare. This operation is performed by the first \code{WHERE} clause. In addition, the certificates for both protocol phases are also initialized early. This is because the certificates are now actively updated through the operations in the reactive listeners’ stream instead of returning the emitted phase message.

During the prepare phase, the workflow waits until the prepare certificate has added $2f+1$ unique prepare phase messages to its proof list. For a phase message to be added to the prepare certificate, it must pass all of the \code{WHERE} clauses assigned for the reactive listener. In actuality, the workflow only waits for $2f$ prepare phase messages due to the pre-prepare message already been added to the protocol certificate during the pre-prepare phase. Once a valid phase message passes all of the first \code{WHERE} operators, it is added to the designated protocol certificate using the \code{SCAN} operator. The \code{SCAN} operator transforms the certificate’s proof list to include the incoming phase message.  The final \code{WHERE} clause determines whether or not the certificate has reached a sufficient number of valid phase messages in its proof list.
The \code{ValidateCertificate} function essentially calculates the number of phase messages inside the proof list when it excludes duplicates. It also makes sure that the phase messages in the list are indeed valid. The asynchronous \code{await} operator on line 45 is used to wait for the \emph{CAwaitable} in the prepare phase reactive listener to finish all of the linked operators for the listener before moving on with the protocol. Once the validation process has succeeded for the protocol certificate, the workflow can move past the \code{await} operator. The prepare phase finishes after the prepare protocol certificate is added to the protocol log in the server on line 47.

\subsubsection{Commit Phase}
As for the commit phase, like the other protocol phases, the first step is to have each replica create a commit phase message and use the server to multicast the commit phase over the \ac{pbft} network. Afterwards, the commit phase performs practically the same operations as the prepare reactive listener. The commit reactive listener waits for the proof list for the commit certificate to have at least $2f+1$ commit phase messages. The reactive listener for the commit phase has an additional \code{WHERE} clause that makes sure that the prepare phase has already finished before exiting the commit reactive listener, which is visible on line 41 in \autoref{code:PrepareAndCommit}. This extra \code{WHERE} clause is used to avoid the commit certificate from being finished before the prepare phase is complete.  After the commit certificate is successfully validated, the protocol workflow is almost finished processing the given request. The protocol workflow first adds the commit certificate to the protocol logger as done prior to the prepare certificate before starting the remaining operations in the protocol workflow. The server now has two valid certificates for the given sequence number assigned to the client request, meaning the replica has the necessary proof that the replicas in the \ac{pbft} network agree to have the application perform the operation for the given request. The application finally performs the operation within the request. The last remaining process is to create a reply message, digitally sign this reply message and send the reply message to the client who initially sent the processed request. The reply message includes information to the client in regards to whether the operation given in the original request was completed successfully or not. In our \ac{pbft} implementation, the only operation the application can do is to write the ‘operation’ received from the request to the console window and add the operation to a persistent list. The persistent list representing the application state is discussed more in \autoref{section:ImpCheckpointing}.

\subsubsection{Protocol Workflow Evaluation}
%Insert whether you believe the code for each section is defined as good code, explain why. How did usage async, reactive operation help/hinder the protocol workflow
%Around 135 lines of code excluding extra lines for initializing objects.Where 30\% is from the reactive operators.
We succeeded in our objective of creating an implementation that performs the standard processes of the \ac{pbft} algorithm into a single function. We believe our resulting implementation is relatively faithful to the \ac{pbft} protocol when based only on the protocol description. The majority of the operations performed in the normal protocol workflow are synchronous, making it easier to read the code. The reactive \code{Source} objects and their chain of operators are the only operations that are performed asynchronously in the normal protocol workflow. The reactive operators still work well together with the synchronous workflow by completing the required checks and operations on the incoming phase messages independently from the rest of the protocol workflow. By taking advantage of the \code{await} command, we easily mark the areas in the protocol workflow where we know the protocol workflow cannot function without the result from the reactive operators. We believe the most significant benefit for our design in the \ac{pbft} protocol workflow within a single function is that it became a lot easier to keep track of the protocol operations. For example, due to how the implementation handles the workflow that creates the protocol certificates, it is considerably straightforward to differentiate between the \ac{pbft} protocol phases by looking at the source code. Basically, by looking for the \code{await} points in the protocol workflow, we can approximately determine where one of the three protocol phases finishes. It is an approximation since there are still a couple of operations required to be performed, such as adding the certificate to the protocol log for the prepare and commit phases.  We would argue that it is a significant challenge to simplify the code to prepare and commit phases further without causing a severe issue for protocol workflow. Most of the complexity we currently have in our implementation comes from the fact that the primary and non-primary replicas have different operations in the pre-prepare phase. In addition, our current stop functionality is not exactly straightforward, which further hurts the simplicity of our implementation. We also believe that despite the functionality of the reactive operators being convenient for handling protocol messages, they may be difficult for inexperienced programmers to read. The programmers should at the very least have some fundamental knowledge in regards to chaining operators using query languages such as \ac{sql}, preferably knowing the fundamentals of \ac{linq}~\cite{WEB:sql} statements, to fully grasp most of the reactive operations available in the Cleipnir framework. The normal workflow implementation is around 135 lines of code when we exclude any additional spaces used to make object initialization easier for others to read. In addition, approximately 30\% of the lines are used for the reactive operators. All in all, based on these results, we would argue that the implementation is relatively short to be able to handle all three protocol phases inside a single function. However, an apparent problem with keeping the functionality in this format was the difficulty of handling protocol messages out of order, which forces the developer to deploy workarounds to avoid this problem. We choose to initialize the \code{Source} objects as soon as possible to reduce the number of phase messages dropped. Unfortunately, we cannot deal with the pre-prepare phase messages due to only being used by non-primary replicas, which is a big downside to using the desired format with reactive operators.

%asynchronous operations
Due to us performing the protocol workflow inside \code{CTASK} functions, we are not able to use traditional asynchronous operations inside the protocol workflow. This typically means tasks regarding reading data from files, server requests, networking, or any other job that is preferred to be performed asynchronously should do so outside the protocol workflow. A developer would therefore need to keep this in mind when designing the protocol-related workflows. Still, regardless of whether or not the protocol workflow is performed inside a \code{Task} or a \code{CTask}, the workflow is run asynchronously, allowing us to effortless run instances of the protocol workflow separately. Although, like when using threads, we need to ensure that the separate asynchronous functions do not alter the same properties if the execution order matters. Otherwise, the result of the application state would become unpredictable. Although our \ac{pbft} implementation does add the two resulting protocol certificates to a shared log, the sequence number assigned to the protocol workflow is unique for each iteration of the \ac{pbft} workflow, allowing us to avoid corrupting the protocol state. This is because all of the iterations will have a unique key to store their protocol information. Despite being run in a \code{CTask}, the \code{await} operator is still valuable for the protocol workflow as it is used to wait for the certificates to finish. Without the ability to use the \code{await} operator, we would not have been able to create the desired protocol workflow.
All in all, the asynchronous workflow may hinder the developer from performing certain operations directly inside the workflow. It is still beneficial when looking at the benefits of running the protocol workflow asynchronously. The most significant advantage of running the workflow asynchronous is how simple it is to start multiple iterations of the protocol workflow. In addition, running the protocol workflow asynchronous should scale better for various clients in comparison to creating separate threads.

%reactive:
Cleipnir reactive framework was handy for handling protocol workflow received from the server. Although we did have quite the big issue with the usage of \code{Source} objects to create the protocol certificate before using the Cleipnir execution engine to schedule the emits. Once we moved on to using universal formatting for emitting items to the workflows in Cleipnir, it has worked as intended. It was initially challenging to use a couple of Cleipnir reactive operators such as \code{Merge} and \code{Scan}, but relatively simple to learn the general approach of chaining reactive operations. Using \code{Source} objects for sending the protocol messages to their appropriate code section is simple once the Cleipnir execution engine was used to schedule the emits in order. From our experience keeping the emit functionality centred to a designated object or class that has reference to the execution engine is the recommended structure. Although, we must consider one aspect when we use reactive programming to handle protocol messages. Each protocol message, regardless of the owner, must all be sent to the protocol workflow in the same way. In our case, all phase messages had to be emitted to the \code{Source} object for the phase message to be validated correctly. This includes its own phase message, meaning a functionality must be available for the protocol workflow to emit phase messages created during protocol workflow. The main advantage of using Cleipnir reactive framework to handle protocol messages for the protocol workflow is that we can structure all of the code related to the phase message inside a single block of code. In addition, due to the nature of chaining operators together in a reactive chain, it is easy to control the order of the operations that need to be on the protocol message. Finally, combined with the \code{await} operator, we can also easily dictate wherein the workflow we want to wait for the sufficient number of protocol messages to be received until the condition required by the consensus algorithm is met. In our case, this would substitute for the process of creating valid protocol certificates by submitting validated proofs until reaching the quota.
To summarize the main benefits, the Cleipnir reactive framework provided our normal \ac{pbft} workflow implementation was a simple way to validate and collect phase messages to create valid protocol certificates.  In addition, to making the collaboration between the network layer and the protocol layer easier to develop. The downsides being that they struggle with handling phase messages that are received out of order. Therefore, as a consequence must be initialized as soon as possible to counteract this issue. 

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:PrepareAndCommit, caption= Prepare and Commit phase, captionpos = b, basicstyle=\scriptsize]
var prepared = MesBridge
               .Where(pm => pm.PhaseType == PMessageType.Prepare)
               .Where(pm => pm.SeqNr == qcertpre.SeqNr)
               .Where(pm => pm.Validate(
                    Serv.ServPubKeyRegister[pm.ServID], 
                    Serv.CurView, 
                    Serv.CurSeqRange, 
                    qcertpre)
                )
                .Where(pm => pm.Digest.SequenceEqual(qcertpre.CurReqDigest))
                .Scan(qcertpre.ProofList, (prooflist, message) =>
                {
                    prooflist.Add(message);
                    return prooflist;
                })
                .Where(_ => qcertpre.ValidateCertificate(FailureNr))
                .Next();
ProtocolCertificate qcertcom = new ProtocolCertificate(
    qcertpre.SeqNr, 
    Serv.CurView, 
    digest, 
    CertType.Committed
);   
var committed = MesBridge
                .Where(pm => pm.PhaseType == PMessageType.Commit)
                .Where(pm => pm.SeqNr == qcertcom.SeqNr)
                .Where(pm => pm.Validate(
                    Serv.ServPubKeyRegister[pm.ServID], 
                    Serv.CurView, 
                    Serv.CurSeqRange, 
                    qcertcom)
                )
                .Where(pm => pm.Digest.SequenceEqual(qcertcom.CurReqDigest))
                .Scan(qcertcom.ProofList, (prooflist, message) =>
                {
                    prooflist.Add(message);
                    return prooflist;
                })
                .Where(_ => qcertcom.ValidateCertificate(FailureNr))
                .Where(_ => qcertpre.ValidateCertificate(FailureNr))
                .Next();
                
Console.WriteLine("Waiting for prepares");
if (Active) await prepared;
else throw new ConstraintException("System is no longer active!");
Serv.AddProtocolCertificate(qcertpre.SeqNr, qcertpre); //add first certificate to Log

//Commit phase
PhaseMessage commitmes = new PhaseMessage(
    Serv.ServID, 
    curSeq, 
    Serv.CurView, 
   	digest, 
    PMessageType.Commit
);
Serv.SignMessage(commitmes, MessageType.PhaseMessage);
Serv.Multicast(commitmes.SerializeToBuffer(), MessageType.PhaseMessage);
Serv.EmitPhaseMessageLocally(commitmes);
Console.WriteLine("Waiting for commits");
if (Active) await committed;
else throw new ConstraintException("System is no longer active!");
Serv.AddProtocolCertificate(qcertcom.SeqNr, qcertcom); //add second certificate to Log
	\end{lstlisting}
\end{figure}

\iffalse
\subsection{Checkpoint Implementation}
\label{section:ImpCheckpointing}
The checkpointing process only occurs after a certain number of requests have been processed by the \ac{pbft} implementation. The \emph{checkpoint interval} determines the number of requests.
For our implementation, the \emph{checkpoint interval} is set to five, meaning after processing five requests, a new checkpoint is created for the system. Our implementation of the checkpoint workflow is divided into three sections. The first section revolves around creating a checkpoint certificate and starting an instance of the reactive checkpoint workflow. The reactive checkpoint workflow performs the second part of the checkpoint workflow. In this part, a reactive \code{Source} object listens for incoming checkpoint messages, which are then validated and added to the certificate’s proof list. This reactive process ends once a certificate has received sufficient checkpoint messages that are deemed valid. The final part consists of emitting the finished stable checkpoint to the server to replace the last stable checkpoint in memory and start the garbage collection process. We will now discuss each of these parts in more detail.

\subsubsection{Initialize Checkpoint Certificate}
The checkpoint certificate is initialized using the last sequence number used by the protocol workflow. The checkpoint certificate also needs to create and store a digest of the current state of the application. Our implementation makes the system digest based on the persistent list that represents the application state. The persistent list contains the operation messages from each of the fully processed requests by the \ac{pbft} protocol. Therefore assuming no errors occur, then the checkpoint for sequence number five has the digest of the list containing the operation from requests one to five. 

The checkpoint workflow starts by first initializing the checkpoint certificate. The certificate includes the information just described, such as the stable sequence number and the digest of the application state. Once the initialization of the checkpoint certificate is done, the checkpoint workflow starts an instance of the checkpoint reactive workflow for the newly created checkpoint certificate. 
We refer to an instance of checkpoint reactive workflow process as an \emph{Checkpoint Listener}. Additionally, the checkpoint certificate is added to the checkpoint logger using the stable sequence number as the key. The process just described can be started in two separate ways. The first method is when the replica itself actively starts the checkpoint process. This is when the replica has processed enough requests in the \ac{pbft} workflow to reach the checkpoint interval. The other approach is when the replica receives a checkpoint message with a sequence number currently not in the checkpoint logger. The checkpoint logger also needs to verify that the checkpoint message has a higher sequence number than the last stable checkpoint stored on the replica. Both methods perform the initialization of the checkpoint certificate and checkpoint listener. However,  the sequence number used for the initialization process differ. The first method uses the last sequence number the protocol processed that initially triggered the checkpoint process. The other way uses the sequence number from the received checkpoint message.  One thing to note is that the replica only performs this process once for a sequence number. Meaning the protocol logger is checked to determine whether or not the checkpoint certificate has already been initialized or not. If the checkpoint certificate is already stored in the logger, the initialization process is not performed again.

Regardless of the way the checkpoint certificate and listener are initialized, the replica is still required to create and multicast its checkpoint message to the \ac{pbft} network once the sequence number matches a checkpoint interval. The checkpoint message created in the replica is also emitted to the checkpoint listener to allow it to be handled the same way as the other checkpoint messages received from the \ac{pbft} network. The checkpoint certificates may be initially stored in the checkpoint logger; we are still not entirely done with them, as we want a checkpoint certificate to become stable. However, for a checkpoint to be deemed stable, it needs to pass the certificate validation processes in the checkpoint listener, which follow the same guidelines as the protocol certificate. A replica can only store one stable checkpoint, meaning the previous stable checkpoint is overwritten whenever a new stable checkpoint with a higher sequence is available. 
The stable checkpoint certificate is used as definitive proof that the \ac{pbft} network agrees on the state of the application up to the stable sequence, meaning the replicas in the \ac{pbft} network now can garbage collect the protocol data from the logger up to the stable sequence number. The garbage collection includes removing any stored checkpoint certificates in the checkpoint logger with lower or equal sequence numbers to the stable checkpoint certificate.
\fi

\subsection{Checkpoint Implementation}
\label{section:ImpCheckpointing}
The checkpointing process only occurs after a certain number of requests have been processed by the \ac{pbft} implementation. The \emph{checkpoint interval} determines the number of requests.
For our implementation, the \emph{checkpoint interval} is set to five, meaning after processing five requests, a new checkpoint is created for the system. Our implementation of the checkpoint workflow is divided into three sections. The first section revolves around creating a checkpoint certificate and starting an instance of the reactive checkpoint workflow. The reactive checkpoint workflow performs the second part of the checkpoint workflow. In this part, a reactive \code{Source} object listens for incoming checkpoint messages, which are then validated and added to the checkpoint certificate’s proof list. This reactive process ends once a certificate has received sufficient checkpoint messages that are deemed valid. The final part consists of emitting the finished stable checkpoint to the server to replace the last stable checkpoint in memory and start the garbage collection process. We are now going to discuss each of these parts in more detail.

\subsubsection{Initialize Checkpoint Certificate}
The checkpoint certificate is initialized using the last sequence number used by the protocol workflow. The checkpoint certificate also needs to create and store a digest of the current state of the application. Our implementation makes the system digest based on the persistent list that represents the application state. The persistent list contains the operation messages from each of the fully processed requests by the \ac{pbft} protocol. Therefore assuming no errors occur, then the checkpoint for sequence number five has the digest of the list containing the operation from requests one to five. 

The checkpoint workflow starts by first initializing the checkpoint certificate. The certificate includes the information just described, such as the stable sequence number and the digest of the application state. Once the initialization of the checkpoint certificate is done, the checkpoint workflow starts an instance of the checkpoint reactive workflow for the newly created checkpoint certificate. 
We refer to an instance of checkpoint reactive workflow process as an \emph{Checkpoint Listener}. Additionally, the checkpoint certificate is added to the checkpoint logger using the stable sequence number as the key. The process just described can be started in two separate ways. The first method is when the replica itself actively starts the checkpoint process. This is when the replica has processed enough requests in the \ac{pbft} workflow to reach the checkpoint interval. The other approach is when the replica receives a checkpoint message with a sequence number currently not in the checkpoint logger. The checkpoint logger also needs to verify that the checkpoint message has a higher sequence number than the last stable checkpoint stored on the replica. Both methods perform the initialization of the checkpoint certificate and checkpoint listener. However,  the sequence number used for the initialization process differ. The first method uses the last sequence number the protocol processed that initially triggered the checkpoint process. The other way uses the sequence number from the received checkpoint message.  One thing to remember is that the replica only performs this process only once for a sequence number. Meaning the protocol logger is checked to determine whether or not the checkpoint certificate has already been initialized or not. If the checkpoint certificate is already stored in the logger, the initialization process is not performed again.

Regardless of the way the checkpoint certificate and listener are initialized, the replica is still required to create and multicast its checkpoint message to the \ac{pbft} network once the sequence number matches a checkpoint interval. The checkpoint message created in the replica is also emitted to the checkpoint listener to allow it to be handled the same way as the other checkpoint messages received from the \ac{pbft} network. The checkpoint certificates initially stored in the checkpoint logger are not stable checkpoint certificates, and our goal is to make at least one of these certificates stable. However, for a checkpoint to be deemed stable, it needs to pass the certificate validation processes in the checkpoint listener, which follow the same guidelines as the protocol certificate. A replica can only store one stable checkpoint, meaning the previous stable checkpoint is overwritten whenever a new stable checkpoint with a higher sequence is available. 
The stable checkpoint certificate is used as definitive proof that the \ac{pbft} network agrees on the state of the application up to the stable sequence, meaning the replicas in the \ac{pbft} network now can garbage collect the protocol data from the logger up to the stable sequence number. The garbage collection includes removing any stored checkpoint certificates in the checkpoint logger with lower or equal sequence numbers to the stable checkpoint certificate.

\subsubsection{Checkpoint Listener Workflow}
\iffalse
The source code for an instance of a checkpoint listener can be seen in \autoref{code:CreateCheckpoint}. The checkpoint listener works similarly to how reactive \code{Source} objects were used in the protocol workflow. The server once it receives a checkpoint message from the network emits the checkpoint message to the \code{Source<Checkpoint>} shared by the server and the checkpoint listener. The checkpoint listener listens for any item emitted by the server to the \code{Source<Checkpoint>} object. The reactive operations performed on the \code{Source<Checkpoint>} object can be seen on lines eight to 17. The checkpoint message received on the stream is first validated before the checkpoint certificate proof list is transformed to have the checkpoint message in its proof list. Unlike the protocol workflow and view-change workflow, each iteration of the checkpoint workflow is not required to finish its execution. In addition, the checkpoint functionality is performed separately to the protocol workflow, meaning it is possible to still process new requests while the checkpoint is created, assuming the protocol workflow has not exceeded the sequence number interval.
This means if the protocol processes enough requests, a new checkpoint listener is created for a checkpoint with a higher sequence number than the previous one. This means it is possible to have multiple checkpoint listeners active at the same time. However, it then becomes a race for the checkpoint listeners to see which one creates the next stable checkpoint certificate. Although it is important to remember that the system does not process any new requests after it has exceeded the current sequence number interval. The reactive listener is finished when all of the reactive operators have been successfully finished, which requires the checkpoint certificate to be deemed stable. A checkpoint certificate is deemed stable once it has $2f+1$ unique and valid checkpoint messages in its proof list. The checkpoint messages must obviously match the checkpoint certificate sequence number and digest.
\fi
\iffalse
\label{sec:checkpointList}
The source code for an instance of a checkpoint listener is presented in \autoref{code:CreateCheckpoint}. The checkpoint listener uses \code{Source<Checkpoint>} similar to how \code{Source<PhaseMessage>} objects were used in the protocol workflow. The server, once it receives a checkpoint message from the network, emits the checkpoint message to the \code{Source<Checkpoint>} shared by the server and the checkpoint listeners. The checkpoint listener listens for checkpoint messages emitted by the server to the \code{Source<Checkpoint>} object. The reactive operations performed on the \code{Source<Checkpoint>} object can be seen on lines 8-17. The checkpoint message received on the stream is first validated before the checkpoint certificate proof list is transformed to have the checkpoint message in its proof list. The \code{WHERE} clauses on line 9 and 10 performs the validation for incoming checkpoint messages. The \code{SCAN} operator is once again used to add the checkpoint to the certificate proof list. 

Unlike the protocol workflow, the iterations of the checkpoint listeners do not need to finish their execution. In addition, the checkpoint functionality is performed separately from the protocol workflow, meaning the protocol workflow can process new requests while the checkpoint workflow tries to create a stable checkpoint certificate. Assuming the protocol workflow has not exceeded the sequence number interval otherwise, no additional requests are processed by the protocol workflow.
If the protocol workflow processes enough requests, a new checkpoint listener is created for another checkpoint with a higher sequence number than the previous one. This means it is possible to have multiple checkpoint listeners active at the same time. However, it then becomes a race for the checkpoint listeners to see which one creates the next stable checkpoint certificate. Although it is important to remember that the system does not process any new requests after it has exceeded the current sequence number interval. 

The reactive listener is finished when all of the reactive operators for the \code{Source<Checkpoint>} have ended, which requires the checkpoint certificate to be stable. A checkpoint certificate is deemed stable once it has $2f+1$ unique and valid checkpoint messages in its proof list. The checkpoint messages in the proof list must obviously match the checkpoint certificate sequence number and digest, which are checked during the certificate validation in the \code{WHERE} clause on line 16.
\fi

\label{sec:checkpointList}
The source code for an instance of a checkpoint listener is presented in \autoref{code:CreateCheckpoint}. The checkpoint listener uses \code{Source<Checkpoint>} similar to how \code{Source<PhaseMessage>} objects were used in the protocol workflow. The server, once it receives a checkpoint message from the network, emits the checkpoint message to the \code{Source<Checkpoint>} shared by the server and the checkpoint listeners. The checkpoint listener listens for checkpoint messages emitted by the server to the \code{Source<Checkpoint>} object. The reactive operations performed on the \code{Source<Checkpoint>} object can be seen on lines 8-17. The checkpoint message received on the stream is first validated before the checkpoint certificate proof list is transformed to have the checkpoint message in its proof list. The \code{WHERE} clauses on line 9 and 10 performs the validation for incoming checkpoint messages. The \code{SCAN} operator is once again used to add the checkpoint to the certificate proof list. 

Unlike the protocol workflow, the iterations of the checkpoint listeners do not need to finish their execution. In addition, the checkpoint functionality is performed separately from the protocol workflow, meaning the protocol workflow can process new requests while the checkpoint workflow tries to create a stable checkpoint certificate. Assuming the protocol workflow has not exceeded the sequence number interval otherwise, no additional requests are processed by the protocol workflow.
If the protocol workflow processes enough requests, a new checkpoint listener is created for another checkpoint with a higher sequence number than the preceding one. This means it is possible to have multiple checkpoint listeners active at the same time. However, it then becomes a race for the checkpoint listeners to see which one creates the next stable checkpoint certificate. Although it is important to remember that the system does not process any new requests after it has exceeded the current sequence number interval. 

The reactive listener is finished when all of the reactive operators for the \code{Source<Checkpoint>} have ended, which requires the checkpoint certificate to be stable. A checkpoint certificate is deemed stable once it has $2f+1$ unique and valid checkpoint messages in its proof list. The checkpoint messages in the proof list must match the checkpoint certificate sequence number and digest, which are checked during the certificate validation in the \code{WHERE} clause on line 16.

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:CreateCheckpoint, caption=Source code for the Checkpoint Listener, captionpos = b, basicstyle=\scriptsize]
public async CTask Listen(
CheckpointCertificate cpc, 
Dictionary<int, RSAParameters> keys, 
Action<CheckpointCertificate> finCallback
)
{
    Console.WriteLine("Checkpoint Listener: " + StableSeqNr);
    await CheckpointBridge
    .Where(check => check.StableSeqNr == StableSeqNr)
    .Where(check => check.Validate(keys[check.ServID]))
    .Scan(cpc.ProofList, (prooflist, message) =>
    {
        prooflist.Add(message);
        return prooflist;
    })
    .Where(_ => cpc.ValidateCertificate(FailureNr))
    .Next();
    finCallback(cpc);
}
    \end{lstlisting}
\end{figure}

\iffalse
\subsubsection{Initiate Garbage Collection}
The third part of the checkpoint functionality is rather simplistic. During startup, the replica initializes its server functionality, including an asynchronous function that listens on a reactive \code{Source<CheckpointCertificate>}. This reactive listener listens for a new stable checkpoint certificate. Once the \code{Source<CheckpointCertificate>} object receives a stable checkpoint certificate, the current stable checkpoint is overwritten by the one it received. Afterwards, the operations in regards to garbage collection are performed. The source code for listening for stable checkpoint certificate can be seen in \autoref{code:ListenForCheckpoint}.
The \code{Source<CheckpointCertificate>} object connected to this function is persisted on the server. The server has a predefined function that uses the Cleipnir scheduler to schedule an emit to this \code{Source} object. Each checkpoint listener is initialized with the callback reference to this function, which allows the checkpoint listener to immediately call the callback address with the finished stable checkpoint certificate whenever all reactive operations are done. The call on the callback reference can be seen in \autoref{code:CreateCheckpoint} on line 18. The Cleipnir execution engine will then schedule the stable checkpoint to be emitted to the reactive listener for stable checkpoint certificates. Once the \code{Source} object receives, the old stable checkpoint certificate is replaced by the new one, even in the case where the replica does not have any existing stable checkpoint certificates. After the new stable checkpoint certificate is assigned to the replica, the garbage collection process begins. The garbage collection process consists of removing records with a lower or equal sequence number to the new stable checkpoint certificate for the protocol, reply, and checkpoint logger.
After the garbage collection is completed, the sequence number interval is extended, allowing the protocol workflow to process more requests.
\fi

\subsubsection{Initiate Garbage Collection}
The third part of the checkpoint functionality is rather simplistic. During startup, the replica initializes its server functionality, including an asynchronous function that listens on a reactive \code{Source<CheckpointCertificate>}. This reactive listener listens for a new stable checkpoint certificate. Once the \code{Source<CheckpointCertificate>} object receives a stable checkpoint certificate, the current stable checkpoint is overwritten by the one it received. Afterwards, the operations in regards to garbage collection are performed. The source code for listening for stable checkpoint certificate can be seen in \autoref{code:ListenForCheckpoint}
The \code{Source<CheckpointCertificate>} object connected to this function is persisted on the server. The server has a predefined function that uses the Cleipnir scheduler to schedule an emit to this \code{Source} object. Each checkpoint listener is initialized with the callback reference to this function, which allows the checkpoint listener to immediately call the callback address with the finished stable checkpoint certificate whenever all reactive operations are done. The call on the callback reference can be seen in \autoref{code:CreateCheckpoint} on line 18. The Cleipnir execution engine then schedules the stable checkpoint to be emitted to the reactive listener for stable checkpoint certificates. Once the \code{Source} object receives, the old stable checkpoint certificate is replaced by the new one, even in the case where the replica does not have any existing stable checkpoint certificates. After the new stable checkpoint certificate is assigned to the replica, the garbage collection process begins. The garbage collection process consists of removing records with a lower or equal sequence number to the new stable checkpoint certificate for the protocol, reply, and checkpoint logger.
After the garbage collection is completed, the sequence number interval is extended, allowing the protocol workflow to process more requests.

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:ListenForCheckpoint, caption=Reactive handler for new stable checkpoints, captionpos = b, basicstyle=\scriptsize]
public async CTask ListenForStableCheckpoint()
{
    Console.WriteLine("Listen for stable checkpoints");
    while (true)
    {
    	var stablecheck = await Subjects.CheckpointFinSubject.Next();
        Console.WriteLine("Update Checkpoint State");
        Console.WriteLine(stablecheck);
        StableCheckpointsCertificate = stablecheck;
        GarbageCollectLog(StableCheckpointsCertificate.LastSeqNr);
        GarbageCollectReplyLog(StableCheckpointsCertificate.LastSeqNr);
        GarbageCollectCheckpointLog(StableCheckpointsCertificate.LastSeqNr);
        UpdateRange(stablecheck.LastSeqNr);
     }
}
    \end{lstlisting}
\end{figure}

\iffalse
\subsubsection{Checkpoint Workflow Evaluation}
%General Protocol workflow 
\label{sec:checkpointEval}
Unfortunately, unlike the normal protocol workflow, we could not keep the checkpoint workflow centered around a single function or file. 
The main challenge design-wise for the checkpoint workflow was the fact that the checkpoint process could be initialized by any replica in the \ac{pbft} network. The checkpoint workflow needed to handle both initialization methods in addition to having the same workflow regardless of whether or not the process was initialized by a received checkpoint message or by the checkpoint interval. Ultimately since the server network layer received the checkpoint messages, we decided it was best to divide up the workflow and instead initialize only if no record existed for the stable sequence number. Regardless we would argue that design-wise, we managed to divide up the program in such a way that the process itself remains simple. It's not more complicated than summarizing the checkpoint workflow as initialization, listening, and initiate garbage collection. Of course, it is a lot more difficult when looking at individual operations in more detail. 

%asynchronous
Due to the checkpoint processes being performed wholly separate from the rest of the protocol workflow, running its operations asynchronously was important. The checkpoint initialization process is only part of the checkpoint workflow that was performed synchronously. Both the checkpoint listener and the listen for stable checkpoint certificate take advantage of asynchronous programming and reactive programming. Both parts of the workflow are required to wait until the desired criteria are met. The checkpoint listener is ideal for asynchronous workflow because it is performed independently from the protocol workflows. In addition, it is unclear when or if an instance of the checkpoint listener would ever finish. Therefore making sure the checkpoint listener doesn't block the thread or steal unnecessary resources important.
The last part of the checkpoint functionality is needed to be practically active all the time as it is unclear when a new stable checkpoint is ready. However, since the garbage collection process rarely occurs and most of the time, the checkpoint functionality is simply waiting, thereby making it desirable to use asynchronous workflow here.

%reactive
Initially, we did not take much advantage of reactive programming when we designed the process of making a checkpoint certificate stable by adding proofs to it. However, changes were made to the design to accommodate for more reactive programming. The result being the checkpoint listener workflow we shown in \autoref{code:CreateCheckpoint} which we described in \autoref{sec:checkpointList}. The original implementation performed all checkpoint message validations and certificate checks whenever a checkpoint message was being added to the proof list. We managed this functionality by using a designated append function, which performed the same operations that are now performed in the reactive chain in the checkpoint listener. The original implementation was functional; however, it was also relatively unstable, meaning we had many additional conditions to check based on where the append function was performed. The primary benefactor to the issues came with the usage of the callback functionality. Not only did we have to assign a callback reference as part of the checkpoint certificate, but the call process also became somewhat unpredictable. A significant contributor was that it was common to call the append function more times than necessary due to receiving more checkpoint messages than was needed. Combine this with the short time intervals between each checkpoint message, and you will get unpredictable results. Not to mention, the Cleipnir execution engine had to schedule the append function calls to avoid the state of the checkpoint certificate becoming unpredictable. Safe to say, we generally preferred the second implementation, which why it was the workflow presented. Generally, the design for the second implementation was a lot more readable and easier to keep track of the workflow. The second implementation also was a lot more stable due to splitting up the processes in the append function into separate reactive operators with more focus on completing a single task. It had better performance due to only having to use the Cleipnir execution engine to schedule the emit to the checkpoint listener rather than having to schedule all of the operations for each checkpoint message as we were forced to with our original design. Finally, the checkpoint certificate no longer needed to have a record of the callback address to the desired emit function in the server. Instead, it was added as a parameter when the checkpoint listener started listening.
The garbage collector functionality has remained the same for both implementations and uses Cleipnir reactive \code{Source} object similar to how channels are used in Golang programming. Generally, this structure works well for the garbage collector as emits only occurs whenever a  new stable checkpoint certificate is ready.
\fi

\subsubsection{Checkpoint Workflow Evaluation}
%General Protocol workflow 
\label{sec:checkpointEval}
Unfortunately, unlike the normal protocol workflow, we could not keep the checkpoint workflow centred around a single function or class. 
The main challenge design-wise for the checkpoint workflow was the fact that the checkpoint process could be initialized by any replica in the \ac{pbft} network. The checkpoint workflow needed to handle both initialization methods in addition to having the same workflow regardless of whether or not the process was initialized by a received checkpoint message or by the checkpoint interval. Ultimately since the server network layer received the checkpoint messages, we decided it was best to divide up the workflow and instead initialize the certificate and listener process only if no record existed for the stable sequence number. Regardless we would argue that design-wise, we managed to divide up the program in such a way that the process itself remains simple. It is not more complicated than summarizing the checkpoint workflow as initialization, listening, and initiate garbage collection. Of course, it is a lot more difficult when looking at individual operations in more detail. 

%asynchronous
Due to the checkpoint processes being performed wholly separate from the rest of the protocol workflow, running most of the checkpoint workflow asynchronously was important. The checkpoint initialization process is only part of the checkpoint workflow that was performed synchronously. Both the checkpoint listener and the listen for stable checkpoint certificate take advantage of both asynchronous programming and reactive programming. Both parts of the workflow are required to wait until the desired criteria are met. The checkpoint listener is ideal for asynchronous workflow because it is performed independently from the protocol workflows. In addition, it is unclear when or if an instance of the checkpoint listener would ever finish. Therefore, it was crucial to make sure the checkpoint listener does not block the thread or steal unnecessary resources.
Regarding the last part of the checkpoint functionality, it is required to be practically active all the time as it is unclear when a new stable checkpoint is ready. However, since the garbage collection process rarely occurs and most of the time, the checkpoint functionality is simply waiting, thereby making it desirable to use asynchronous workflow here.

%reactive
Initially, we did not take much advantage of reactive programming when we designed the process of making a checkpoint certificate stable by adding proofs to it. However, changes were made to the design to accommodate for more reactive programming. The result being the checkpoint listener workflow we shown in \autoref{code:CreateCheckpoint}, which we described in \autoref{sec:checkpointList}. The original implementation performed all checkpoint message validations and certificate checks whenever a checkpoint message was being added to the proof list. We managed this functionality by using a designated append function, which performed the same operations that are now performed in the reactive chain in the checkpoint listener. The original implementation was functional; however, it was also relatively unstable, meaning we had many additional conditions to check based on where the append function was called. The primary benefactor to the issues came with the usage of the callback functionality. Not only did we have to assign a callback reference as part of the checkpoint certificate, but the call process also became somewhat unpredictable. A significant contributor was that it was common to call the append function more times than necessary due to receiving more checkpoint messages than was needed. Combine this with the short time intervals between each checkpoint message, and you will get unpredictable results. Not to mention, the Cleipnir execution engine had to schedule the append function calls to avoid the state of the checkpoint certificate becoming unpredictable. Safe to say, we generally preferred the second implementation, which is why it is the presented workflow. Generally, the design for the second implementation was a lot more readable and easier to keep track of the workflow. The second implementation also was a lot more stable due to splitting up the processes in the append function into separate reactive operators with more focus on completing a single task. It had better performance due to only having to use the Cleipnir execution engine to schedule the emit to the checkpoint listener rather than having to schedule all of the operations for each checkpoint message as we were forced to with our original design. Finally, the checkpoint certificate no longer needed to have a record of the callback address to the desired emit function in the server. Instead, it was added as a parameter when the checkpoint listener started listening.
The garbage collector functionality has remained the same for both implementations and uses Cleipnir reactive \code{Source} object similar to how channels are used in Golang programming. Generally, this structure works well for the garbage collector because emits only occurs whenever a  new stable checkpoint certificate is ready.

\input{sections/ImpViewchange}

\iffalse
\subsection{View-change Implementation}
\subsection{Starting a View-Change}
%insert how to start a view-change. Including timeout, view-change listener, started by timeout/protocol messages.
\subsection{View-Change functionality}
\subsubsection{Initialize View-Change}
\subsubsection{View-Change Listener Workflow}
\subsubsection{New-View Workflow}

As previously mention in \autoref{sec:view-change} the goal of a view-change is to successfully replace a faulty primary replica with another non-faulty replica. In order for a primary change to be successful, the replicas in the \ac{pbft} network needs to agree upon the state the program continues on after the primary change has occurred. Furthermore the view-change must ensure that the new replica selected for primary responsibility is not faulty. 

The operations to ensure these criteria were briefly mentioned in \autoref{sec:view-change}. Although in total there are a quite lot of operations needed for a successful view-change to take place. However, it is possible to divide up the operations into two segments based on which goal the operations attempt to fulfill. Excluding the processes of shutting down the protocol execution, the first part of the view-change process is for the replicas in the network to agree on that a view-change is necessary. This goal is achieved by having the replicas multicast and listen for view-change messages. Since the next primary is determined by the formula $p = v ~mod~ R$, \ac{pbft} doesn't require any election process. The view-change messages instead contains information of the replicas current checkpoint information as well as current state of the logged certificates. This is so that the new primary can have all the relevant information to create the new state for the \ac{pbft} system. The goal of the second segment is to initialize the \ac{pbft} system state after the view-change is finished. This goal is fulfilled by looking at the current stable checkpoint and the current protocol certificates stored in memory. In order to make sure that requests were not fully processed before the view-change occured. The \ac{pbft} needs to redo each of the requests stored in the logger up to the highest sequence number seen in the \ac{pbft} system. Thankfully, due to stable checkpoints, the process does not need to take into account every single request ever processed. The new primary is responsible for starting this process by creating and multicast a new-view message. This message acts as an introduction letter, telling the other replicas in the network that it is the new primary and additionally provide a view-change certificate proving this fact. The new-view message also contain a list of pre-preprepares which are created from the information stored within the protocol certificates in the view-messages received. This new-view message is validated by the other replicas. If the replicas deem the information in new-view message as valid, the replicas will use these pre-prepares message to create prepare and commit messages and redo the \ac{pbft} algorithm for these pre-prepares. The system is finally finished with the view-change once all the pre-prepares have stored their respective two protocol certificates. 

As for implementing this functionality, our implementation can be divided into four segments. The first consist of the timeout functionality that when triggers puts the application into non-active mode. The second part consist of updating the view data, creating view-change messages, multicast these view-changes over the \ac{pbft} network and finally store the collection of view-changes until quorum has been reached. The third consist of creating and validating functionality for new-view message. Finally the last segment consist of the redo protocol functionality. 

In non-active mode all protocol related messages and requests are denied by the main protocol execution. This mode is active until all four segments of the view-change functionality as been completed successfully. The view-change functionality differs from the other functionality due to the handling of timeout. It has already been mention that the view-change functionality start once a replica exceeds its timeout before receiving a pre-preprepare message. However, there are two additional timeout present in the view-change functionality. These timeout exists in order for the system to be absolute sure that the new primary chosen by the $p = v ~mod~ R$ formula does not result in a faulty replica. If the formula does result in a faulty formula, then either view-change process or redo protocol process will most likely fail. Setting a timeout for these two functionalities, the protocol can recover from a potential frozen state and restart the view-change process by now selecting the next replica on the list. Essentially, the view number is incremented every time the view-change protocol changes, meaning a new primary is selected until a non-faulty primary is chosen. Unfortunately the current implementation only handles timeout at the start of the normal protocol workflow, which also gets stopped once the replica receives a pre-prepare message. This means the protocol gets effectively stuck in the case where the protocol fails at handling prepare and commit messages. There were to main reason for why this issue was not resolved in our implementation. The first reason was that timeout functionality relies on the \code{WhenAny} asynchronous function~\cite{WEB:whenany}. This function creates a \code{Task} that is set to finish once either of the attached \code{Task}'s completes. In our implementation this effectively is set to either the timeout is exceeded or the process that is waited for completes. This was unfortunately not very well integrated with reactive listeners, as it is forced to finish all the operators before it is deemed completed. It required the \code{MERGE} operator in order to enforce reactive listeners to stop and dispose of the active reactive stream when a timeout occured. The \code{MERGE} operator required that both the reactive streams that were to merge had the same format. This means that the stop signal to the \code{MERGE} operator needed to also be a phase message for the pre-prepare reactive stream. The current workflow for handling the timeout functionality can be seen in \autoref{code:timeout}. The time is first initialized with a cancellationtoken which is brought into the main protocol workflow so that the protocol can cancel the timeout when it receives a pre-prepare message. The timeout used in the current implementation is set to ten seconds. The timeout gets a reference to an active \code{Source} object which is the same \code{Source} which is listened to at the function \code{ListenForShutdown}. When the timeout exceeds, the timeout function will emit a message to the shutdown \code{Source} which in turn makes the \code{CTask} in \code{ListenForShutdown} to return before the AppOperation, which lets the program flow to continue. The AppOperation is still active as an asynchronous function, meaning we want to forcefully shut it down so as to avoid creating conflicts with the future emits to the protocol \code{Source} object. To solve this we emit an obviously faulty phase message with an unique phase message type called \emph{End}. Thanks to the \code{MERGE} operator, the pre-prepare reactive listener will finish and returns the faulty pre-prepare message. As seen in \autoref{code:Pre-PrepareNonPrimary}, the protocol calls a timeout exception if the pre-prepare reactive listeners returns the faulty phase message, meaning the protocol effectively shuts down as intended.   

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:timeout, caption=Handling timeout for the normal protocol workflow and initiate View-Change, captionpos = b, basicstyle=\scriptsize]
CancellationTokenSource cancel = new CancellationTokenSource();
_ = TimeoutOps.AbortableProtocolTimeoutOperation( //starts timeout
   serv.Subjects.ShutdownSubject,
   10000,
   cancel.Token,
   scheduler
);
execute.Serv.ChangeClientStatus(req.ClientID);
bool res = await WhenAny<bool>.Of(
                AppOperation(req, execute, seq, cancel),
                ListenForShutdown(serv.Subjects.ShutdownSubject)
);
Console.WriteLine("Result: " + res);
if (res)
{
   Console.WriteLine($"APP OPERATION {seq} FINISHED");
   ...
}
else
{
   if (execute.Active)
   {
      Console.WriteLine("View-Change starting");
      execute.Active = false;
      serv.ProtocolActive = false;
      await scheduler.Schedule(() =>
         shutdownPhaseSource.Emit(new PhaseMessage(-1, -1, -1, null, PMessageType.End)
      ));
      await execute.HandlePrimaryChange2(); 
      Console.WriteLine("View-Change completed");
      serv.UpdateSeqNr();
      ...
    \end{lstlisting}
\end{figure} 
 
The view-change exchange segment of the code starts by first setting the replica into the next view by incrementing its view number. The next operation sets the replicas view-change certificate. This step is dependent on whether or not the replica has received previously received view-change messages from another replica. If the replica has not received any view-messages from other replicas than it initializes the view-change certificate and initializes the view-change reactive listener. Info about the rest of the view-change process in the main workflow...

The view-change listener deviates a bit from the other reactive listeners. The main difference is that it also requires the ability to call upon a shutdown emit in the case where the system already has gotten $2f$ view-change messages. The reason for this functionality is mostly due to making the system more efficient. The replica does not need to wait for a timeout to occur if it already has received $2f$ view-change messages since the \ac{pbft} network only requires that replica's view-change in order for the new view to be initialized. Therefore the process is speed up by calling for a shutdown emit if already has $2f$. Ofcourse this functionality is only useful if the replica is still in active mode. This is the reason as to why the option to not trigger the shutdown emit is an option. Other than that the reactive listener performs relatively the same operators for the reactive stream. Firstly, we want to only accept view-change message that belongs to the same next view nr as the replica. Secondly the view-change messages received are validated to make sure that it is a valid view-change message. Assuming the validation process is successful, the view-change message is added to the view-change certificate proof list. The final reactive operator validates that the view-change certificate to see if it has received the sufficient number of valid view-change messages in its proof list. After the view-change reactive listener is finished and a valid view-change certificate is ready, the callback function \emph{finCallback} calls the servers to emit a signal to the view-change workflow that the view-change certificate is finished and can move on with the next step of the view-change process. 

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:viewListener, caption=Source code for View-Change Listener, captionpos = b, basicstyle=\scriptsize]
if (Shutdown && shutdownCallback != null)
{
   Console.WriteLine("With shutdown");
   await ViewBridge
      .Where(vc => vc.NextViewNr == NewViewNr)
      .Where(vc => vc.Validate(keys[vc.ServID], ServerViewInfo.ViewNr))
      .Scan(vcc.ProofList, (prooflist, message) =>
      {
        prooflist.Add(message);
        return prooflist;
      })
      .Where(_ => vcc.ShutdownReached(FailureNr))
      .Next();
   Console.WriteLine("Calling shutdown");
   shutdownCallback();
}
await ViewBridge
   .Where(vc => vc.NextViewNr == NewViewNr)
   .Where(vc => vc.Validate(keys[vc.ServID], ServerViewInfo.ViewNr))
   .Scan(vcc.ProofList, (prooflist, message) =>
   {
     prooflist.Add(message);
     return prooflist;
   })
   .Where(_ => vcc.ValidateCertificate(FailureNr))
   .Next();
Console.WriteLine("Finished Listen view changes");
finCallback();
    \end{lstlisting}
\end{figure} 
segment from motivation, rewrite and get it in the main text somehow!
This could in theory also apply to the view-change description as it is divided into several detailed steps. However, there are several factors which lead to the view-change functionality being split into three separate, but nested, functions. The first reason being that view-changes require the ability to restart the processes in the case where the request processing remains stationary for too long. To handle this functionality we currently use a mix of timeout operations and goto statements in order to reroute the program flow back to the beginning of the view-change process~\cite{WEB:goto}. The second reason, which also applies to checkpoints, is that the view-change process can be initialized early by receiving view-change messages from other replicas. This may seem similar to protocol operations since it is initialized by client requests, however the difference lies in the amount of messages required to initialize the process. Currently, the server needs to support the functionality of starting reactive listeners for view-changes if it ever receives a view-change, however the view-change process itself doesn't start until either the timeout occurs or the replica has received $2f$ messages. Because of this functionality, keeping the code completely synchronous and centered around a single function is not possible. As for checkpoints, because the checkpoint processes can be initialized whenever, and the majority of the time spent for checkpoints is used waiting for a replica to receive $2f+1$ unique checkpoints with identical sequence numbers, the source code cannot be centered around a single function.
\fi


\iffalse 
The checkpointing process follows the \emph{checkpoint interval}. This means it only gets used once the system has processed a certain number of requests equal to the checkpoint interval. In our implementation the current checkpoint interval is set to five, meaning after processing five requests a new checkpoint is created. In our implementation we divided the workflow of the checkpointing into two parts.
The first part is the creation part, which is essentially initializing the checkpoint certificate to the last sequence number using the current application state as digest. In our implementation, we create the system digest from a persistent list which represent the current state of the system. The list contains the operation messages from each of the requests that has been fully processed by the \ac{pbft} protocol. So assuming no errors occurs, than the checkpoint for sequence number five will be the digest of a list containing the operation from requests one to five. After creating the checkpoint certificate and the checkpoint message, a checkpoint reactive listener is initialized. This reactive listener works similar to how reactive listeners worked in the protocol workflow. The server once it receives a checkpoint message from network will emit the checkpoint message to the \code{Source<Checkpoint>} registered in the server. The checkpoint listener will listen for any item emitted by the server and the given checkpoint message will be first validated before transforming the proof list of the checkpoint  certificate to be a proof list which has the checkpoint message. Unlike the protocol workflow, checkpoints can theoretically not be completed during execution and runs separate to the normal protocol workflow. This means if the protocol processes enough requests, a checkpoint checkpoint will be created with higher sequence number the previous one. This means it can be possible to have multiple checkpoint listeners active at the time. However, it becomes a race for the checkpoint certificates to see which one becomes next stable one. After the checkpoint listener has been created, than the replica will also emit its own local checkpoint to the checkpoint listener, meaning it will have to pass all of the same checks as the networked checkpoint messages has to. Since we're never sure which of the replica in the network is the fastest when it comes to setting up the checkpoint certificate, it means the server is also prepared to initialize the checkpoint processes if it receives a checkpoint with higher sequence number than the current stable checkpoint. Unlike the protocol certificates, the checkpoint certificates are added to the checkpoint logger once its been created, no validation is required. However, in order for a checkpoint to be deemed stable it will need to pass the certificate validation processes which follows the same guidelines as the protocol certificate. A replica can only have one stable checkpoint. The goal of the checkpoint process is to attempt to try replace this stable checkpoint, so we can garbage collect the protocol data from the logger. The garbage collection also includes active checkpoints in the checkpoint logger with lower or equal sequence number to the stable checkpoint certificate.
The second part of the checkpoint functionality is rather simplistic. Originally when the server side of a replica was initialized it also initialized another reactive listener, which is set to await for new stable checkpoint certificate. Once it receives a stable checkpoint certificate in the reactive \code{Source} object, it will overwrite the stable checkpoint registered on the system and then perform the garbage collection process. The \code{Source} is linked to the server and it schedules the new checkpoint certificate similar to schedules any message emit to the persistent layer. This is important because each checkpoint listener will have a reference to the callback function which schedules the emit to the \code{Source} object. This means that once the checkpoint certificate passes all of the reactive operators and the checkpoint is deemed valid, the callback function will be called with the resulting checkpoint certificate, which in turn will overwrite the stable checkpoint. Checkpoint process is then deemed successful and the garbage collection processes is started. The source code for the an instance of a checkpoint listener can be seen in \autoref{code:CreateCheckpoint}. The source code for listening for stable checkpoint certificate can be seen in \autoref{code:ListenForCheckpoint}
\fi

\section{Client}
\iffalse
The client implementation created for the \ac{pbft} implementation is a primitive console application that is interactable by the user. The client uses interactivity to create unique operations that are to be handled by the \ac{pbft} algorithm. In our current \ac{pbft} implementation, we treat operations as simple string objects, meaning mostly any assigned string value can be used as an operation value. However, an exception to this rule is that the operation cannot contain a pipeline symbol. This is because the pipeline symbol is used as an end delimiter for serialized messages in order to resolve a \ac{tcp} issue that can occur, which links two messages together. An operation is created by prompting the user for a value representing the value of the operation in the request message. 
Just like the replicas in the system, the client takes the network addresses stored in a \ac{json} file and then establishes a socket connection to each of the network addresses. Unfortunately, this means the client can not be initialized before the replicas, since it expects all replicas to be up and running.

In principle, the workflow for the client implementation is straightforward. The client starts by first initializing its connection to each of the replicas in the system based on the addresses found in the chosen \ac{json} file. Then the user is prompted for the value to be used in the operation. Once the operation is deemed valid, the client creates a new request message using the operation provided by the user. The request is signed by the client’s private key and then multicast to the replicas in the \ac{pbft} network. After the request is sent, the client waits for replicas to reply to the request the same way the normal protocol workflow does for a phase shift. A reply certificate is created, and the client uses a \code{Source<Reply>} to listen for reply messages reactively. When the \code{Source<Reply>} receives a new valid reply message, it is added to the reply certificate until the certificate has received at least $f+1$ valid replies from different replicas. The $f+1$ criteria is referred to as a weak certificate, which is a certificate that can guarantee that at least \emph{f} non-faulty replica stored the request in its protocol log~\cites[p.~9]{PAPER:PBFTRecovery}[p.~2]{PAPER:DPBFT}. Because the client is not part of the \ac{pbft} system, it only requires \emph{f} number of replies to guarantee that the \ac{pbft} system properly processed the original request~\cites[p.~3]{PAPER:OGPBFT}[p.~9]{PAPER:PBFTRecovery}.

If the reply certificate receives $f+1$ replies from different replicas, the certificate is stored in the client’s log. The client application restarts its workflow by again prompting the user for the next operation for the subsequent request. However, if the reply certificate does not become valid within a specific time duration, a timeout will occur, and the request is once again multicasted to the \ac{pbft} network. This process is repeated until the $f+1$ criteria is met. Unfortunately, if the \ac{pbft} application gets stuck on one of the client operations, the server does not accept the resent request as it believes it is already working on another request from the same client.  Unfortunately, this usually leads to an endless loop. A way to get out of this loop would be for a view-change to occur on the \ac{pbft} since the client status information on the replica is reset after the view-change is finished. The resent request is now treated as a new one, and the entire request processing starts anew.

The client shares a lot of the network-related code with the \ac{pbft} replicas. The main difference between the two lies in the client always being responsible for initiating the socket connection. The client also tries to reconnect to replicas it has previously been connected to but now is lost. The reconnection attempt is made whenever the client is about to multicast a request to the \ac{pbft} network. In the case where the reconnection fails, the client moves onto the other replicas. The client does, however, retries to reconnect to the lost replica whenever a new
request is sent to the \ac{pbft} network. 

We decided not to include persistency for the client implementation. Despite this, the network portion of the client still uses the Cleipnir execution engine when it sends incoming replies from the network layer to its reactive listener. The reason for this is because scheduling the emit using the Cleipnir execution engine enforces synchrony. Enforcing synchrony helps the client avoid a potential race condition that can potentially occur in this section of the code. We are currently not sure what is causing this issue. We are running the reactive listener completely outside of Cleipnir’s influence, which means additional threads are not supposed to be created. Despite this, we still have encountered race conditions in this section without using the Cleipnir execution engine.
\fi

The client implementation created for the \ac{pbft} implementation is a primitive console application that is interactable by the user. The client uses interactivity to create unique operations that are to be handled by the \ac{pbft} algorithm. In our current \ac{pbft} implementation, we treat operations as simple string objects, meaning mostly any assigned string value can be used as an operation value. However, an exception to this rule is that the operation cannot contain a pipeline symbol. This is because the pipeline symbol is used as an end delimiter for serialized messages to resolve a \ac{tcp} issue that can occur, which links two messages together. An operation is created by prompting the user for a value representing the value of the operation in the request message. 
Just like the replicas in the system, the client takes the network addresses stored in a \ac{json} file and then establishes a socket connection to each of the network addresses. Unfortunately, this means the client can not be initialized before the replicas since it expects all replicas to be up and running when it attempts to establish socket connections.

In principle, the workflow for the client implementation is straightforward. The client starts by first initializing its connection to each of the replicas in the system based on the addresses found in the chosen \ac{json} file. Then the user is prompted for the value to be used in the operation. Once the operation is deemed valid, the client creates a new request message using the operation provided by the user. The request is signed by the client’s private key and then multicast to the replicas in the \ac{pbft} network. After the client sends the request, it waits for the replicas to reply to the request the same way the normal protocol workflow does for a phase shift. A reply certificate is created, and the client uses a \code{Source<Reply>} to listen for reply messages reactively. When the \code{Source<Reply>} receives a new valid reply message, it is added to the reply certificate until the certificate has received at least $f+1$ valid replies from different replicas. The $f+1$ criteria is referred to as a weak certificate, which is a certificate that can guarantee that at least \emph{f} non-faulty replica stored the request in its protocol log~\cites[p.~9]{PAPER:PBFTRecovery}[p.~2]{PAPER:DPBFT}. Because the client is not part of the \ac{pbft} system, it only requires \emph{f} number of replies to guarantee that the \ac{pbft} system properly processed the original request~\cites[p.~3]{PAPER:OGPBFT}[p.~9]{PAPER:PBFTRecovery}.

If the reply certificate receives $f+1$ replies from different replicas, the certificate is stored in the client’s log. The client application restarts its workflow by again prompting the user for the next operation for the subsequent request. However, if the reply certificate does not become valid within a specific time duration, a timeout will occur, and the request is once again multicasted to the \ac{pbft} network. This process is repeated until the $f+1$ criteria is met. Unfortunately, if the \ac{pbft} application gets stuck on one of the client operations, the server does not accept the resent request as it believes it is already working on another request from the same client.  Unfortunately, this usually leads to an endless loop. A way to get out of this loop would be for a view-change to occur on the \ac{pbft}. This is because the client status information on the replica is reset after the view-change is finished. The resent request is then treated as a new one, and the entire request processing starts anew.

The client shares a lot of the network-related code with the \ac{pbft} replicas. The main difference between the two lies in the client always being responsible for initiating the socket connection. The client also tries to reconnect to replicas it has previously been connected to but now is lost. The reconnection attempt is made whenever the client is about to multicast a request to the \ac{pbft} network. In the case where the reconnection fails, the client moves onto the other replicas. The client does, however, retry to reconnect to the lost replica whenever a new
request is sent to the \ac{pbft} network. 

We decided not to include persistency for the client implementation. Despite this, the network portion of the client still uses the Cleipnir execution engine when it emits incoming replies from the network layer to its reactive listener. The reason for this is because scheduling the emit using the Cleipnir execution engine enforces synchrony. Enforcing synchrony helps the client avoid a potential race condition that can potentially occur in this section of the code. We are currently uncertain in regards to what is causing this issue. We are running the reactive listener completely outside of Cleipnir’s influence, which means additional threads are not supposed to be created. Despite this, we still have encountered race conditions in this section without using the Cleipnir execution engine.
