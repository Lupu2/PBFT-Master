\chapter{Implementation}

%Implementation 
%describe code, but not to detailed
%motivation for using this design
%good parts/bad parts
\label{chapter:Imp}
In this chapter we introduce our PBFT implementation. We will introduce the implementation for the request handler, normal protocol workflow, view-changes and finally checkpointing. We will also discuss how the Cleipnir framework has been used to create the working PBFT implementation as well as discuss some benefits and limitations within the current implementation design.

%Implementation talks about the actual algorithm implementation that is run in protocol execution, includes process of checkpoints and view-changes. Should have figures to simplify explanation. Go over briefly the different phases, show some pseudocode. keeps to take note of, maybe include a model to show the persistency layers present. The importance of using the Cleipnir scheduler and CTask, where you have used them etc.

%outline aka what needs to be talked about, note not in any particular order
%detailed explanation of how the general workflow for the PBFT algorithm is performed, with code snippets.
%the simplicity of some of the necessary tools needed in PBFT is handled. Object oriented programming for Messages, Certificates. Static function for workflow, including Listeners, and handlers
%detailed explanation for how the checkpointing are handled
%detailed explanation for how the view-changes are handled
%Describe how you though of persistency during implementation(not much since its not general focus, and ofcourse doesn't work fully)
%small description on how clients are working, how they use the same reactive operators to count number of replies received.
%Add comment on how well they work, don't work do to design, this is an evaluation afterall.

\section{Motivation}
With the goal of the thesis in mind, the PBFT protocol workflows were designed to be as close to the protocol description as possible. In order to accomplish this we believed the best approach would be to design protocol related workflow as synchronous as possible. In addition, in order to make it easier to understand the protocol workflow, we believe the best approach is to keep the protocol related code centred around single function or class when possible. This is deemed especially important when constructing the normal protocol workflow. This is because the normal protocol description is orderly constructed, going through different phases until consensus as been reached over the PBFT network. This could in theory also apply to the view-change description as it is divided into several detailed steps. However, there are several factors which leads to the view-change functionality being split into three separate, but nested, functions. The first reason being that view-changes requires the ability to restart the processes in the case where the process is stationary for too long. To handle this functionality we currently use a mix of timeout operations and goto statements in order to reroute the program flow back to the beginning of the view-change process~\cite{WEB:goto}. The second reason, which also applies to checkpoints, is that the view-change process can be initialized early by receiving view-change messages from other replicas. This may seem similar to protocol operations since its initialized by client requests, however the difference lie in the amount of messages required to initialize the process. Currently, the server needs to support the functionality of starting reactive listeners for view-changes if it ever receives a view-change, however the view-change process itself doesn't start until either the timeout occurs or the replica as received $2f$ messages. Because of this functionality, keeping the code completely synchronous and centered around a single is not possible. As for checkpoints, because the checkpoint processes can be initialized whenever, and majority of the timespent for checkpoints is simply waiting for a replica to receive $2f+1$ unique checkpoints with identical sequence numbers, the source code cannot be centered around a single function.

As we previously mention in \autoref{sec:persvsephe} and \autoref{section:PersistentProgramming} using normal asynchronous operations inside Cleipnir is not a good idea. Therefore, the only form of asynchronous operations performed inside any of the protocol workflows are restricted to other \code{CTask} operations. The \code{await} operator still works well with creating statemachines for asynchronous \code{CTask} operations and waiting for \code{Source} objects to finish all of its operators. Thereby giving the protocol an abstraction as an synchronous process, when in reality it is not. Other than that, traditional \code{Task} based asynchronous operations are well used in the network layer for our implementation. 

Another important topic discussed in \autoref{sec:persvsephe} was the need to use the Cleipnir execution engine to schedule operations when operations outside of Cleipnir required to affect the system within Cleipnir. To simply this design, practically almost all of the scheduled operations using the Cleipnir execution engine are performed within the server. This design is chosen to make it easier to keep track of the origin of the message emit. The server has several emit functions ready to schedule the given message type to its desired \code{Source} object. In order for the protocols workflow to take advantage of this design, they are either required to have a reference to the server object to call the function, or the workflow gets a callback referring to the emit function in the server. The second functionality is quite useful when the operations are initialized by the server. Checkpoints and view-change listeners tend to be initialized by the server, making it simple to assign the correct callback function.

There are currently exists two different implementations for checkpointing and view-changes. Both versions are still documented in the source code, where the second implementations have the letter 2 at the end of its name. The workflow for the both of them remain practically the same for both implementations. The main difference lies with how the implementations handle creating and handling certificates. Essentially the certificate is not deemed valid until it has received $2f+1$ unique and valid message for said message type. This processes is handled differently between the implementation. The second implementation performs the message validation, adding message to proof list and proof list validation over a \code{Source} object by using reactive operators. Meanwhile the first implementation performs these same operations for certificates inside the checkpoint certificate itself inside an append function. Once the certificates are deemed valid, another emit to \code{Source} object has to be made, so that the view-change and checkpoint operations are given the signal to continue with the next operations. The first implementation is required to have the callback function to the server emit message inside the certificate, while the second implementation simply has this function callback right after the reactive listener. The design was changed in order to accommodate the need for more reactive operations in the application. From our experience the second implementation generally performed better than the first implementation and was also for the most part more consistent. The first implementation sometimes encountered issues with the callback function, especially the view-change implementation.

As persistency is a core part of the Cleipnir it was decided to design the application to handle some form of persistency. This meant the protocol related operations had to be run in Cleipnir, which in turn meant the protocol had to be either synchronous or asynchronous using \code{CTask}'s. In addition objects run in the protocols are required to be persistable, meaning the objects needed to get a serializer function and deserializer function connected to Cleipnir. This also includes using Cleipnir inbuilt data structures when the persisted objects needed to also persist data structures. This is especially present in certificates since they need to persist their list of proofs, meaning the proof list uses the inbuilt Cleipnir \code{CList} to substitute normal list. Unfortunately there is an unfortunate oversight in our part when we designed the application. The application currently uses JSON~\cite{WEB:NewJSON} to serialize and deserialize messages when sent over the PBFT network. JSON formatting does not support inbuilt Cleipnir data structures, which is not surprising yet was not directly discovered until very late into the PBFT implementation. Currently this issue is solved by converting between traditional data structures and inbuilt Cleipnir data structures whenever a message with said data structure needs to be serialized by JSON. The conversation itself is rather simple, a copy of the data storage in the traditional data structure format is created, then the content is copied from the source data storage to the newly created data storage. The process is then reversed on the receiver end after the message as been properly deserialized. Although in retrospect, it might have been more beneficial if the serialization and deserialization for the networking were the same used by Cleipnir as to avoid this issue in the future. Finally, we have done our best to avoid creating circular dependencies. Circular dependencies would essentially cause the serialization process to fail, as both references depend on each other. We do believe there currently are no circular dependencies in our implementation, because the Cleipnir serialization process does not crash during Cleipnir's synchronization process. The server and protocol workflow relationship can potentially be too close to being one. The server emits messages to the protocol workflow while the protocol workflow has a reference to the server. The only reason this does not create a circular dependency is because the server interacts with the protocol workflow through the Cleipnir execution engine and does not have a direct reference to the protocol workflow. Currently the PBFT implementation does not currently support functional persistency. The reason for as to why the persistency  does not work completely is uncertain. There are two main issues as of now. The first issue is that protocol logger for some reason does not persist all the entries in the logger. As of now, no distinguishable pattern as been found with the data which is lost. Either way, losing certificates in the logger does create big problems. The other reason is that some of the \code{Source} objects linked to the server gets duplicated, meaning that in the system there exist two \code{Source} objects with the exact same reference. This becomes a problem, because each time the server emits a message to the \code{Source} object, it will emit the message to both the original and duplicate \code{Source}. This in turn can essentially cause two identical iterations of the protocol to occur. This even includes even storing the resulting protocol certificates to the logger with the same sequence. As it is not ever intended to store four certificates to a single sequence number, it is very clearly not working properly. 
%As our main goal for this thesis was to evaluate the tools given and not focus on making the implementation persistable, we decided on prioritising other factors of our implementation. We do believe however, that if it were possible to solve the previously mention issues, that the current implementation has a decent (word for solid background for getting it to work). 

\section{Workflow Details}

\subsection{Protocol Workflow Implementation}

\subsubsection{Starting protocol instance}
A normal sequence for the PBFT implementation begins once the request handler receives a request message from the server. The source code for the request handler can be seen in \autoref{code:StartProtocol}. The request handler listens for new requests messages emitted to the \code{Source} object \emph{requestMessage}. As mention in \autoref{sec:persvsephe}, the server is tasked with emitting messages its received in the network layer to the appropriate \code{Source} object in order for the protocol to access the message. The request handler will verify that the request is valid and that the client who sent the request does not already have an active request currently being processed. As mention in \autoref{sec:checkpoint}, PBFT will only allow a certain amount of requests be processed before a new checkpoint is required to be performed. Therefore, the request handler also needs to confirm that the system is not about the exceed the sequence number upper bound. The received request will not be processed if the system is about to exceed the upper bound. The last check verifies whether or not the protocol execution is active. If it is not active, then the replica is currently performing a view-change and will not accept any new requests until this operation is finished. Once all checks are passed the request handler will call the asynchronous \code{CTask} function \emph{PerformProtocol} which will initialize and start the PBFT protocol for the given request. It is important that the request handler does not wait for \emph{PerformProtocol} finishes as it is important to not block the \emph{requestMessage} as we desire an application which can process multiple requests from clients at the same time. 

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:StartProtocol, caption=Code section from the request handler, captionpos = b, basicstyle=\scriptsize]
while (true)
{
    var req = await requestMessage.Next();
    if (Crypto.VerifySignature(
        	req.Signature, 
            req.CreateCopyTemplate().SerializeToBuffer(), 
            serv.ClientPubKeyRegister[req.ClientID]
            ) 
            && serv.CurSeqNr < serv.CurSeqRange.End.Value
    )
    {
        if (execute.Active)
        {
            int seq = ++serv.CurSeqNr;
            Console.WriteLine("Curseq: " + seq + " for request: " + req);
            _ = PerformProtocol(execute, serv, scheduler, shutdownPhaseSource, req, seq);
        }
    }
}
	\end{lstlisting}
\end{figure}

\subsubsection{Pre-Prepare phase}
The pre-prepare phase is the only part of the normal operation workflow which has different structure depending on whether or not the replica is the primary or not. If the replica is the primary, than it will take the sequence number initialized by its server and create a pre-prepare message using this sequence number together with its server id, current view and digest of the request. This pre-prepare message will dictate the other replicas sequence number for this given request. The primary will than initialize the protocol certificate used for storing the proof of the prepare phase. This protocol certificate for the prepare phase will always have the pre-prepare message as its first entry in its proof list. The protocol will then use its server reference to multicast this pre-prepare message to the other replicas in the network. The primary's source code for the pre-prepare phase can be seen in \autoref{code:Pre-PreparePrimary}.

The source code of the other non-primary replicas can be seen in \autoref{code:Pre-PrepareNonPrimary}. The other non-primary replicas will subscribe to the \code{Source<PhaseMessage>} \emph{MesBridge} and attempt to listen for incoming phase messages. Since the replica only wants pre-prepare message in this reactive listener, it will use a \code{WHERE} clause to ignore any other phase message other than ones which uses the pre-prepare messages type. In addition, another \code{WHERE} clause is assigned to avoid any pre-prepare messages designated for other requests by comparing the request digests. The final \code{WHERE} clause validates the phase message where the validation  rules are the same as the once mention in \autoref{sec:detailedProtocol} for pre-prepare messages. Once the replica receives a pre-prepare phase message which passes all the \code{WHERE} clauses it will create its own protocol certificate which uses the sequence number given by the primary, which in turn means all protocol certificates for the prepare phase should match for each replica. The non-primary replica will finally end the pre-prepare phase and start the prepare phase by creating a prepare message and multicast this message using the same method the primary used for multicasting its pre-prepare phase. 

The \code{MERGE} operator is used to ensure that the protocol execution is terminated if a view-change occurs. If the timeout occurs, a unique phase message will be emitted to the \code{Source<PhaseMessage>} \emph{ShutdownBridgePhase}. The \code{MERGE} operator will somewhat bind these reactive streams together. This essentially means the the \emph{MesBridge} will be unsubscribe for new phase messages if a phase message is detected in the \emph{ShutdownBridgePhase} and will return this phase message as the resulting phase message. The opposite situation will also apply for the \emph{ShutdownBridgePhase}. As this phase message is intentionally faulty it will not be allowed to be used in the prepare phase of the protocol. Therefore a timeout exception is called instead, which will properly close this instance of the protocol execution.

The design for the source code for the pre-prepare phase is simple and follows a synchronous workflow as we desired, which in turn makes it easier for developers to write. Unfortunately there are two severe issues with our current implementation of the pre-prepare phase. These issues are caused by a combination of having to split the code based on primary vs non-primary and the importance of initializing instances of the reactive listeners early. Both issues are theoretically very similar as they both are caused by improper initialization of the reactive listeners used in the PBFT implementation. The first issue occurs when the primary sends out its pre-prepare phase message before the non-primary replicas as initialized the pre-prepare reactive listener. Which results in the phase message not being received by the non-replica, which means it fails the pre-prepare phase and the timeout will put the replica into view-change mode. The second issue occurs when a non-primary receives a prepare message before it has received the pre-prepare message from the primary. When this situation occurs the prepare message gets filtered out and will not be available once this non-primary reaces the prepare phase. In worst case scenario the replica loses all of the other replicas prepare message, meaning it gets stuck in the prepare phase once it finally receives its pre-prepare message.

These issues mostly comes down to the application struggling on handling phase messages being received out of order. There are several workarounds to handling messages out of order, however most of the workarounds available would require adding a lot more complexity to the implementation. As our goal for this thesis being to create the simplest and most true implementation of PBFT when compared to the protocol description, it was decided not to redesign the protocol workflow to handle issues with pre-prepare messages out of order. As the pre-prepare message is meant to be responsible for getting the other non-primary to start processing the request and assigning the sequence number, we feel it would not be true to the original algorithm to change this design. One workaround to this issue would for instance be to simply initialize a prepare reactive listeners at the start and once the pre-prepare message was received, the prepare messages not related to the given request with different sequence numbers would be filtered out. Currently, in order to somewhat mitigate this issue, the primary is forced to wait for atleast a second before starting to multicast its pre-prepare message. This to allow the other replicas to catch up, meaning its less likely that a replica is far enough behind to lose out on prepare messages before getting handling their pre-prepare message. With this workaround, the issue is for the most part stable, with an average of 15 operations being processed before encountering this issue.

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:Pre-PreparePrimary, caption= Source code for pre-prepare phase for primary replica, captionpos = b, basicstyle=\scriptsize]
ProtocolCertificate qcertpre;
byte[] digest = Crypto.CreateDigest(clireq);
int curSeq; 
if (Serv.IsPrimary()) //Primary
{
    curSeq = leaderseq;
    Console.WriteLine("CurSeq:" + curSeq);
    Serv.InitializeLog(curSeq);
    PhaseMessage preprepare = new PhaseMessage(
        Serv.ServID, 
    	curSeq, 
        Serv.CurView, 
        digest, 
        PMessageType.PrePrepare
    );
    Serv.SignMessage(preprepare, MessageType.PhaseMessage);
    qcertpre = new ProtocolCertificate(
        preprepare.SeqNr, 
        preprepare.ViewNr, 
        digest, 
        CertType.Prepared, 
        preprepare
    );
    await Sleep.Until(1000);
    Serv.Multicast(preprepare.SerializeToBuffer(), MessageType.PhaseMessage);
}
	\end{lstlisting}
\end{figure}
\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:Pre-PrepareNonPrimary, caption= Pre-prepare phase for non-primary replica, captionpos = b, basicstyle=\scriptsize]
else	//Not Primary
{ 
    var preprepared = await MesBridge
    	              .Where(pm => pm.PhaseType == PMessageType.PrePrepare)
                      .Where(pm => pm.Digest != null && pm.Digest.SequenceEqual(digest))
                      .Where(pm => pm.Validate(
                        Serv.ServPubKeyRegister[pm.ServID],
                        Serv.CurView, 
                        Serv.CurSeqRange)
                       )
                       .Merge(ShutdownBridgePhase)
                       .Next();
                
    if (preprepared.ServID == -1 && preprepared.PhaseType == PMessageType.End) 
        throw new TimeoutException("Timeout Occurred! System is no longer active!");
    qcertpre = new ProtocolCertificate(
        preprepared.SeqNr, 
        Serv.CurView, 
        digest, 
        CertType.Prepared, 
        preprepared
    );
    curSeq = qcertpre.SeqNr; 
    Serv.InitializeLog(curSeq);
    PhaseMessage prepare = new PhaseMessage(
        Serv.ServID, 
        curSeq, 
        Serv.CurView, 
        digest, 
        PMessageType.Prepare
    );
    Serv.SignMessage(prepare, MessageType.PhaseMessage);
    qcertpre.ProofList.Add(prepare);
    Serv.Multicast(prepare.SerializeToBuffer(), MessageType.PhaseMessage);
}
	\end{lstlisting}
\end{figure}		

\subsubsection{Prepare phase and Commit phase}
In comparison to the Pre-prepare phase and starting the prepare phase, the rest of the workflow in the implementation is relatively stable and straightforward. The prepare and commit phase source code can be seen in \autoref{code:PrepareAndCommit}. The first step is to initialize the reactive listeners for prepare and commit phase messages. This is done early for two reasons! The first is to mitigate the time between waiting for pre-prepare messages and prepare messages as two avoid potentially losing prepare messages. The other reason is so that the protocol can listen for both prepare messages and commit messages, which means there aren't any ordering issues between messages during the prepare and commit phase. If the pre-prepare message did not dictate the sequence number for non-primary replicas, this would have also been the ideal design for handling pre-prepare message. 

The reactive listener used for the prepare phase are pretty much the same for both phases. The only major difference between the two is that they only accept phase messages for their respective protocol phase. Additionally a commit certificate is initialized early to be used together with the commit reactive listener. Since the prepare messages and prepare certificates are already been initialized in the pre-prepare phase, there is only one more thing to do in the prepare phase. The prepare phase will wait until the prepare certificate as received $2f+1$ unique prepare phase messages which passes all of the \code{WHERE} clauses assigned. In actuality it is to wait for $2f$ prepares and one pre-prepare message. To add the valid phase messages to the designated certificates, we use the \code{SCAN} operator to transform the original proof list for the certificate to include the messages received in the reactive listener. The final \code{WHERE} clause determines whether or not the certificate has received the desired number of unique valid phase messages. Essentially calculating the number of phase messages inside the proof list excluding duplicates, and making sure the phase messages in the list are valid. The asynchronous \code{await} operator is used to wait for the \emph{CAwaitable} to finish this all of the operators linked to the prepare reactive listener before moving on with the protocol. Once the prepare certificate has succeeded its validation process, the prepare certificate is added to protocol log in the server and the commit phase officially starts. 

Like the other phases, the first step will be for each replica to create their commit phase messages and use the server to multicast their commit phase over the PBFT network. Afterwards the protocol will wait for the proof list for the commit certificate to reach $2f+1$. This rule applies to each of the replica as there are no difference in operations between primaries and non-primaries in the commit phase. The reactive listener for the commit phase will additionally check that prepare phase as already finished validating as to avoid finishing the commit certificate before the protocol certificate. However, this extra check does not effect the protocol workflow in either way, since the protocol certificate is awaited earlier in the process. After the commit certificate is successfully validated, the protocol workflow is essentially completed. The remaining operations performed in the protocol execution is to add the commit certificate to the logger similar to the prepare certificate. The server will now have two valid certificates for the given sequence number to request, meaning the replica now has proof that the protocol was successful for the given request. Finally a reply message will be created, signed and sent to the client that sent the processed request. Additionally the operation within the request will be performed by the application. In our PBFT implementation the 'operation' will simply be to write the operation to the console window and add the operation to a persistent list. The persistent list representing the application state will be more discussed in \autoref{section:ImpCheckpointing}.

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:PrepareAndCommit, caption= Prepare and Commit phase, captionpos = b, basicstyle=\scriptsize]
	
var prepared = MesBridge
               .Where(pm => pm.PhaseType == PMessageType.Prepare)
               .Where(pm => pm.SeqNr == qcertpre.SeqNr)
               .Where(pm => pm.Validate(
                    Serv.ServPubKeyRegister[pm.ServID], 
                    Serv.CurView, 
                    Serv.CurSeqRange, 
                    qcertpre)
                )
                .Where(pm => pm.Digest.SequenceEqual(qcertpre.CurReqDigest))
                .Scan(qcertpre.ProofList, (prooflist, message) =>
                {
                    prooflist.Add(message);
                    return prooflist;
                })
                .Where(_ => qcertpre.ValidateCertificate(FailureNr))
                .Next();
ProtocolCertificate qcertcom = new ProtocolCertificate(
    qcertpre.SeqNr, 
    Serv.CurView, 
    digest, 
    CertType.Committed
);   
var committed = MesBridge
                .Where(pm => pm.PhaseType == PMessageType.Commit)
                .Where(pm => pm.SeqNr == qcertcom.SeqNr)
                .Where(pm => pm.Validate(
                    Serv.ServPubKeyRegister[pm.ServID], 
                    Serv.CurView, 
                    Serv.CurSeqRange, 
                    qcertcom)
                )
                .Where(pm => pm.Digest.SequenceEqual(qcertcom.CurReqDigest))
                .Scan(qcertcom.ProofList, (prooflist, message) =>
                {
                    prooflist.Add(message);
                    return prooflist;
                })
                .Where(_ => qcertcom.ValidateCertificate(FailureNr))
                .Where(_ => qcertpre.ValidateCertificate(FailureNr))
                .Next();
                
Console.WriteLine("Waiting for prepares");
if (Active) await prepared;
else throw new ConstraintException("System is no longer active!");
                
//Commit phase
Serv.AddProtocolCertificate(qcertpre.SeqNr, qcertpre); //add first certificate to Log
PhaseMessage commitmes = new PhaseMessage(
    Serv.ServID, 
    curSeq, 
    Serv.CurView, 
   	digest, 
    PMessageType.Commit
);
Serv.SignMessage(commitmes, MessageType.PhaseMessage);
Serv.Multicast(commitmes.SerializeToBuffer(), MessageType.PhaseMessage);
Serv.EmitPhaseMessageLocally(commitmes);
Console.WriteLine("Waiting for commits");
if (Active) await committed;
else throw new ConstraintException("System is no longer active!");
Serv.AddProtocolCertificate(qcertcom.SeqNr, qcertcom); //add second certificate to Log
	\end{lstlisting}
\end{figure}

\subsection{View-change Implementation}

\subsection{Checkpoint Implementation}
\label{section:ImpCheckpointing}
The checkpointing process follows the \emph{checkpointinterval}. This means it only gets used once the system has processed certain number of requests equal to the checkpoint interval. In our implementation the current checkpoint interval is set to five, meaning after processing five requests a new checkpoint is created. In our implementation we divided the workflow of the checkpointing into two parts. 

The first part is the creation part, which is essentially initializing the checkpoint certificate to the last sequence number using the current application state as digest. In our implementation, we create the system digest from a persistent list which represent the current state of the system. The list contains the operation messages from each of the requests that has been fully processed by the PBFT protocol. So assuming no errors occurs, than the checkpoint for sequence number five will be the digest of a list containing the operation from requests one to five. After creating the checkpoint certificate and the checkpoint message, a checkpoint reactive listener is initialized. This reactive listener works similar to how reactive listeners worked in the protocol workflow. The server once it receives a checkpoint message from network will emit the checkpoint message to the \code{Source<Checkpoint>} registered in the server. The checkpoint listener will listen for any item emitted by the server and the given checkpoint message will be first validated before transforming the proof list of the checkpoint  certificate to be a proof list which has the checkpoint message. Unlike the protocol workflow, checkpoints can theoretically not be completed during execution and runs separate to the normal protocol workflow. This means if the protocol processes enough requests, a checkpoint checkpoint will be created with higher sequence number the previous one. This means it can be possible to have multiple checkpoint listeners active at the time. However, it becomes a race for the checkpoint certificates to see which one becomes next stable one. After the checkpoint listener has been created, than the replica will also emit its own local checkpoint to the checkpoint listener, meaning it will have to pass all of the same checks as the networked checkpoint messages has to. Since we're never sure which of the replica in the network is the fastest when it comes to setting up the checkpoint certificate, it means the server is also prepared to initialize the checkpoint processes if it receives a checkpoint with higher sequence number than the current stable checkpoint. Unlike the protocol certificates, the checkpoint certificates are added to the checkpoint logger once its been created, no validation is required. However, in order for a checkpoint to be deemed stable it will need to pass the certificate validation processes which follows the same guidelines as the protocol certificate. A replica can only have one stable checkpoint. The goal of the checkpoint process is to attempt to try replace this stable checkpoint, so we can garbage collect the protocol data from the logger. The garbage collection also includes active checkpoints in the checkpoint logger with lower or equal sequence number to the stable checkpoint certificate. 

The second part of the checkpoint functionality is rather simple. When the server side of a replica was initialized it also initialized another reactive listener, which is set to await for new stable checkpoint certificate. Once it receives a stable checkpoint certificate in the reactive \code{Source} object, it will overwrite the stable checkpoint registered on the system and then perform the garbage collection process. The \code{Source} is linked to the server and it schedules the new checkpoint certificate similar to schedules any message emit to the persistent layer. This is important because each checkpoint listener will have a reference to the callback function which schedules the emit to the \code{Source} object. This means that once the checkpoint certificate passes all of the reactive operators and the checkpoint is deemed valid, the callback function will be called with the resulting checkpoint certificate, which in turn will overwrite the stable checkpoint. Checkpoint process is then deemed successful and the garbage collection processes is started. The source code for the an instance of a checkpoint listener can be seen in \autoref{code:CreateCheckpoint}. The source code for listening for stable checkpoint certificate can be seen in \autoref{code:ListenForCheckpoint}

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:CreateCheckpoint, caption=Listener for checkpoint messages, captionpos = b, basicstyle=\scriptsize]
public async CTask Listen(
CheckpointCertificate cpc, 
Dictionary<int, RSAParameters> keys, 
Action<CheckpointCertificate> finCallback
)
{
    Console.WriteLine("Checkpoint Listener: " + StableSeqNr);
    await CheckpointBridge
    .Where(check => check.StableSeqNr == StableSeqNr)
    .Where(check => check.Validate(keys[check.ServID]))
    .Scan(cpc.ProofList, (prooflist, message) =>
    {
        prooflist.Add(message);
        return prooflist;
    })
    .Where(_ => cpc.ValidateCertificate(FailureNr))
    .Next();
    finCallback(cpc);
}
    \end{lstlisting}
\end{figure}

\begin{figure}[H]
	\centering
	%\lstset{style=sharpc}
	\begin{lstlisting}[label = code:ListenForCheckpoint, caption=Listener for valid checkpoint, captionpos = b, basicstyle=\scriptsize]

public async CTask ListenForStableCheckpoint()
{
    Console.WriteLine("Listen for stable checkpoints");
    while (true)
    {
    	var stablecheck = await Subjects.CheckpointFinSubject.Next();
        Console.WriteLine("Update Checkpoint State");
        Console.WriteLine(stablecheck);
        StableCheckpointsCertificate = stablecheck;
        GarbageCollectLog(StableCheckpointsCertificate.LastSeqNr);
        GarbageCollectReplyLog(StableCheckpointsCertificate.LastSeqNr);
        GarbageCollectCheckpointLog(StableCheckpointsCertificate.LastSeqNr);
        UpdateRange(stablecheck.LastSeqNr);
     }
}
    \end{lstlisting}
\end{figure}

\section{Client}
The client implementation created for the PBFT implementation is a primitive console application. Just like the replicas in the system, the client will take the network addresses stored in a JSON file and than create a socket connection to each of network addresses. Unfortunately, this means the client should be initialized only once the other replicas have already been initialized. The client requires user interaction, by prompting the user for the operation which the client sends to the PBFT network. Currently the PBFT implementation treats operations as simple strings, meaning mostly any value can be given to the operation. Although an exception to this rule is that the operation cannot contain a pipeline symbol in the operation. This is because the pipeline symbol is used as an end delimiter for serialized message in order to resolve a TCP issue with messages being linked together.

The workflow of the client is in principle very simple. The client will start by first initializing its connection to each of the replicas in the system. Then the user will be prompted for the value to be used in the operation. Once the operation is deemed valid, the client will create a new request message using the message provided by the user. This request will be signed by the client and be multicast to the replicas in the PBFT network. After a request is sent, the client will wait for a reply messages from the replicas essentially the same way as  the normal workflow will wait for a phase shift. A reply certificate is created and the client uses a \code{Source<Reply>} to wait reactively add new reply messages to the reply certificate until the certificate has received atleast $f+1$ valid replies from different replicas. The $f+1$ criteria is known as a weak certificate, which is a certificate that can provide that atleast \emph{f} non-faulty replica stored the request in its log\cites[p.~9]{PAPER:PBFTRecovery}[p.~2]{PAPER:DPBFT}. Because the client is not part of the PBFT system it only requires \emph{f} number replies in order to guarantee the success of the request\cites[p.~3]{PAPER:OGPBFT}[p.~9]{PAPER:PBFTRecovery}. 

If the reply certificate receives $f+1$ replies from different replicas the certificate will be stored in the clients log and application moves back to prompting user for the next operation to be sent to the server. However, if the reply certificate does not become valid within a specific time duration, a timeout will occur and the request will be once again multicasted to the PBFT network. This process will be repeated until the $f+1$ criteria is met. Unfortunately, if the PBFT application gets stuck on one of the client operations, the server will not accept the resent request if another request is currently processed. Which could potentially lead to an endless loop. A way to get out of this loop would be for a view-change to occur as the status of the clients are reset after a view-change, meaning the new request will be treated as a new one and the entire processing starts anew. 

The client shares a lot of the network related code with the pbft replicas. The main difference lies in the client being responsible for initiating the socket connection. This also means the client needs to attempt to reconnect to a lost replica in the situation where a replica has been previously went down and the client is about to multicast a request to the PBFT network. In the case where the reconnection fails, the client will simply continue to ignore this replica until a new request is made.

We decided to not include persistency for the client implementation. Despite this, the network portion of the client still uses the Cleipnir execution engine when sending replies from the network layer to the reactive listener. The reason for this is because scheduling the emit using Cleipnir execution engine will enforce synchrony and will avoid a race condition in this section of the code that frequently occurs. We are currently not sure what is causing this issue. We are running the reactive listener completely outside of Cleipnir's influence, which means additional threads should not be created despite very clearly appearing.