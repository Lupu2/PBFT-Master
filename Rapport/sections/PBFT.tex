\chapter{Practical Byzantine Fault Tolerance}
\label{chapter:PBFT}
%version 1

Practical Byzantine Fault Tolerance(PBFT) is a consensus algorithm specifically designed to handle Byzantine fault's in an asynchronous distributed network. The algorithm was originally published in 1999 by Miguel Castro and Barbara Liskov~\cite{PAPER:OGPBFT}.
Notably the Linux foundation's open source blockchain by the name of Hyperledger~\cite{WEB:PBFTGeeks, SLIDES:PBFT} uses PBFT.

The problems derived from byzantine faults originally came to light through a well-known problem known as the Byzantine Generals Problem~\cites{WEB:BFTInfo}{ART:lamportByzGenProb}[p.~240-253]{BOOK:BuildDepDistSyst}.
The byzantine generals problem can be shortly summarized as a couple of army generals leading different armies and have to reach a common decision. The most common scenario is for the armies to coordinate an attack on a surrounded city. The armies can only survive if all of the generals agree to either attack the city together or all agree to retreat. There are also traitor generals that actively attempts to sabotage the order. The decision is also irreversible regardless of the action performed by the other armies. A byzantine fault tolerant system is a system can handle the issue introduced by the byzantine generals problem and is the main goal for consensus algorithm to achieve this state. This includes the PBFT algorithm~\cite{WEB:BFTInfo, ART:lamportByzGenProb}.
%mulig det mangler en setning der, hva er egentlig hoved problemet for Byzantine Generals Problem, kinda cuts off

The PBFT algorithm focuses on creating a state machine network that can withstand byzantine failures~\cite[p.~456]{BOOK:MVstandver3}. The protocol achieves this by providing two main properties for the network. These properties are referred to as safety and the liveness.
To summarize these properties:\\
\textbf{Safety} is the property that ensures that the total ordering of requests are equal for all the non-faulty participating servers. In other words the system state should be similar to a synchronous system, operating one operation at the time, despite the fact that the system is operated over multiple remote machines.\\ 
\textbf{Liveness} is the property that ensures that the correct result is eventually agreed upon and returned by the system~\cites[p.~456]{BOOK:MVstandver3}{WEB:ConsesAlgo}[p.~2]{PAPER:OGPBFT}{SLIDES:PBFT}[p.~403]{PAPER:PBFTRecovery}[p.~257]{BOOK:BuildDepDistSyst}.

\input{sections/systemModel}

\section{Detailed Protocol Operations}
\label{sec:detailedProtocol}
The PBFT consensus protocol is divided into three phases. The Pre-Prepare, Prepare and the Commit phase. If the PBFT protocol operations are executed properly, consensus has been achieved for an operation once all three phases have transpired on $3f+1$ replicas~\cite[p.~257-259]{BOOK:BuildDepDistSyst}. Role of the pre-prepare phase and prepare phase is to propose an ordering for requests delivered to the system, while the combination of prepare phase and the commit phase establishes the execution order for the replicas in the system~\cite[p.~4]{PAPER:OGPBFT}. 

The PBFT protocol starts once a client send a request containing their desired operation to the primary~\cite[p.~4]{PAPER:OGPBFT}. Sometimes the client will also multicast their request to the other replicas in the system as well, which is the model that we followed in our implementation~\cites[p.~2]{PAPER:DPBFT}[p.~406]{PAPER:PBFTRecovery}[p.~258]{BOOK:BuildDepDistSyst}. Regardless of which of these models is used for the request message, the primary is the one responsible for starting the PBFT algorithm which processes the client's request. The primary will create a Pre-Prepare message and assign the request with a sequence number which is then multicasted to the other replicas in the network with the same view number as the primary. Once a replica receives the Pre-Prepare message it will validate the Pre-Prepare message. The validation process consist of the following~\cites[p.~4]{PAPER:OGPBFT} {SLIDES:PBFT}[p.~259]{BOOK:BuildDepDistSyst}
\begin{itemize}
	\item[-]Validating the Signature in the message.
	\item[-]Checking that the view number in the message match the current view number.
	\item[-]The message sequence number is not out of bounds with the current sequence number interval~\cites{SLIDES:PBFT}[p.~4]{PAPER:OGPBFT}.
	\item[-]Make sure the replica has not already received another Pre-Prepare message with the same sequence number, but with a different request.
\end{itemize} 
Once the validation process is finished the replica will officially start the prepare phase by creating a prepare message and multicast it over the network. The prepare phase ends for a replica once its stored up to $2f+1$ validated pre-prepare/prepare messages from different replicas. After this condition is met, the replica enters the state known as \emph{prepared}, in this state it will log the message data so far in what is called a prepare certificate. A prepare certificate is essentially a proof that the prepared phase is finished and is properly executed for that given request. The proof of a prepare certificate is a list of the valid prepare messages, basically confirming that quorum has been reached for the certificate if the number of messages stored in the list is higher than the desired limit of $2f + 1$~\cites[p.~408]{PAPER:PBFTRecovery}[p.~457]{BOOK:MVstandver3}. 
The last phase is the commit phase which functions very similar to the prepare phase. Each replica that is finished with the prepare phase will start the commit phase by multicasting commit messages to the other replicas in the system~\cite[p.~4]{PAPER:OGPBFT}. In this phase, the primary functions exactly the same as every other replica. The validation process is also the same as it was for the prepare messages. The goal is also the same as in the prepare phase, which is for a replica to receive $2f+1$ commit messages, which includes the replicas own commit message~\cite[p.~5]{PAPER:OGPBFT}. Once a replica has received enough commit messages, then the protocol reaches the \emph{committed} phase for the replica. This essentially means that a commit certificate is created and is logged similar to a prepare certificate~\cites[p.~409]{PAPER:PBFTRecovery}[p.~457]{BOOK:MVstandver3}. When a replica has finished both a prepare certificate and a commit certificate, then consensus has been achieved and each replica will perform the operation requested by the client~\cites[p.~409]{PAPER:PBFTRecovery}[p.~5]{PAPER:OGPBFT}. After the operation is executed each replica sends back a reply message containing the appropriate identification values as well as the result of processing the given request. The last requests sent by the clients are also stored in memory, to account for the situation where the client does not receive the reply messages. In this case the client will re-transmit the same request to the system and the replicas will re-transmit their reply for that following request~\cite[p.~409]{PAPER:PBFTRecovery}. A client will accept the result if it gets $f+1$ replies back from the replicas. 

The replicas can only handle a certain amount of requests before the system is required to save the its state. As mentioned in the validation process, a replica can only processes a protocol message that is within a given sequence number interval. This sequence interval length is always constant and will adjust based on the systems checkpoint period which will be discussed in the next section \autoref{sec:checkpoint}\cites[p.~262]{BOOK:BuildDepDistSyst}[p.~4-5]{PAPER:OGPBFT}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{figures/PBFTWorkflow}
	\caption{Practical Byzantine Fault Tolerance Normal Workflow}
	\label{fig:pbftnormalworkflow}
\end{figure}

\section{Checkpointing}
\label{sec:checkpoint}
PBFT also incorporates checkpointing, which is a mechanism used for garbage collecting the logs. Checkpointing is required so that the replica does not use up all of its memory for logging messages~\cite[p.~261]{BOOK:BuildDepDistSyst}. Therefore, the replicas must agree upon a point in which the system is stable for all the replicas. Afterwards the replicas can delete any records in the logs prior to the consented state~\cites[p.~5]{PAPER:OGPBFT}[p.~410]{PAPER:PBFTRecovery}. 

Checkpoints are essentially the state records of the system after progressing a specific interval of requests. The checkpoint has information regarding the last sequence number that was performed for the system. This sequence number is used on the garbage collector to put an upper bound on the records that are to be removed. For instance, if the stable sequence number was set to 50, then the garbage collector would remove set of logged data up to 50. The checkpoint will also have a digest of the system for that stable sequence number. This digest is used to confirm that the replicas have the same system state for the given sequence number~\cites[p.~5]{PAPER:OGPBFT}[p.~410]{PAPER:PBFTRecovery}. 

In order for replicas to be able to validate checkpoints, they each must multicast a checkpoint message over the network containing the information mentioned above with its own replica id. Like the rest of the PBFT protocol messages a checkpoint is considered to be valid for a replica if it has stored $2f+1$ checkpoint messages with different replica id`s with the same stable sequence number and system digest~\cites[p.~261-262]{BOOK:BuildDepDistSyst}[p.~5]{PAPER:OGPBFT}[p.~410]{PAPER:PBFTRecovery}. Once a checkpoint has been validated successfully it is referred to as a stable checkpoint~\cites[p.~3]{PAPER:DPBFT}[p.~261]{BOOK:BuildDepDistSyst}. The replica will usually store checkpoint messages for different sequence numbers in memory and has only a single record for a stable checkpoint. Once a new stable checkpoint is determined, any checkpoint records with lower sequence numbers are removed from memory and if there is exist a previous stable checkpoint in memory with a lower sequence number, then it will be replaced by the new one.%potentially add a cite here

In PBFT checkpointing is usually performed periodically after a constant number of requests have been processed. This interval length is constant and is referred to as a checkpoint period~\cites[p.~261]{BOOK:BuildDepDistSyst}[p.~410]{PAPER:PBFTRecovery}. As mention earlier in~\autoref{sec:detailedProtocol} PBFT will normally only process a sequence number if it is in the set of currently available sequence numbers. The length of the sequence number interval is usually designed to follow the format $[checkpointinterval+1-2*checkpointinterval]$. Which means the system attempts to calculate two checkpoints during a single sequence number interval. Once a stable checkpoint is obtained, the system will extend the sequence number interval where the new interval starts at the last stable sequence for the current stable checkpoint~\cites[p.~5]{PAPER:OGPBFT}[p.~410]{PAPER:PBFTRecovery}. Unless a replica has exceeded the upper bound of the sequence number interval, the replica will usually perform the checkpoint functionality concurrently to the protocol workflow.

\section{View changes}
In the scenario in which the primary is the faulty replica, a view-change will eventually occur. The purpose of the view-change is to reassign the  responsibility for a primary away from the primary replica that is deemed to be faulty. Which then given to another replica that is not faulty~\cites[p.262]{BOOK:BuildDepDistSyst}. As mentioned in \autoref{sec:systemModel}, the replica that is chosen as next primary is based on the replica id and the next view number. Therefore, the view-change will update the view number for the system in order to change the primary replica of the system. There are some operations that need to be performed for a view-change to be deemed successful. The first operation is to update the view number to set another replica as the primary~\cites[p.~6]{PAPER:OGPBFT}[p.~411]{PAPER:PBFTRecovery}{WEB:SawtoothPBFT}. This includes multicasting view-change messages between replicas to start the new view session. The other more demanding operation is that the primary needs to make sure that the system is stable and that replicas starts the new view with the same operation state. Therefore, all requests that have been performed after the last stable sequence number, needs to be reprocessed between the replicas. This is done so that the system can guarantee that the replicas are not missing any of the previous operations performed in the system.~\cites[p.~458]{BOOK:MVstandver3}[p.~263-265]{BOOK:BuildDepDistSyst}. 

There are several ways for a replica to deem its primary to be faulty, the most common way is to have a timeout functionality for the protocol execution. The timeout which is the most common is to start a timeout once a replica has received a request from the client. If the replica does not receive any pre-prepare messages for that request before the timeout expires, than the replica will go into view-change mode and will no longer participate in any of the protocol operations~\cites{SLIDES:PBFT}[p.~5-6]{PAPER:OGPBFT}[p.~263]{BOOK:BuildDepDistSyst}.   

The view-change process starts by having the replica increment its view number. Then the replica will create, sign and multicast a view-change message over the network. The replica will then wait for $2f+1$ view-change messages~\cites{SLIDES:PBFT}[p.~6]{PAPER:OGPBFT}[p.~411]{PAPER:PBFTRecovery}{WEB:SawtoothPBFT}. A timeout is also used here, if the replica does not receive enough view-change messages in time, then the process repeats with the next incremented view number. In some cases a replica can also be designed to go into view-change mode if a replica has already received two view-change messages from other replicas, as it now only requires its own view-change message for the system to agree that a view-change is necessary\cite{BOOK:BuildDepDistSyst}. Once the appropriate number of view-change messages are received, then the new primary will be responsible for creating, signing and multicasting a new-view message to the other replicas~\cite[p.~264]{BOOK:BuildDepDistSyst}. Before the new-view message can be multicast to the other replicas, a new primary must go through its log and create new pre-prepare messages for all sequence number that has occurred after the last stable sequence number. If the new primary lacks a record in the log for any of the sequence numbers, the new pre-prepare message will have its request digest be set to null. This information is included in the new-view message, which is then sent to the other backup replicas. The backup replicas than validates and re-process each of the sequence numbers that has a valid pre-prepare message. This essentially means that the other replicas have to multicast a new prepare message and then participate in a commit phase together with the new primary for each of the pre-prepare messages in the new-view message~\cites[p.~6]{PAPER:OGPBFT}[p.~458]{BOOK:MVstandver3}[p.~265]{BOOK:BuildDepDistSyst}. A timeout is once again being used in the case where the reprocessing takes too long. This process can also fail if the pre-prepares in the new-view message fails the validation process. If this happens, then it is back to the start for the view-change process. Once all pre-prepares have been reprocessed, the view-change procedure is over, and the replica will return to normal protocol operations with the new chosen primary. Keep in mind that any new requests received during the view-change process will be ignored by the system~\cite[p.~263]{BOOK:BuildDepDistSyst}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{figures/PBFTViewChange}
	\caption{Practical Byzantine Fault Tolerance View-Change}
	\label{fig:pbftviewchange}
\end{figure}

%In the scenario in which the primary is the fault replica, a view-change occurs. The purpose of the view-change is to reassign the primary responsibility away from a replica that is deemed to be faulty. As mentioned in \autoref{sec:systemModel}, the replica which is chosen as the primary is based on the replica id and the current view number. Therefore, the view-change updates the view number to update the primary replica. There are 3 main operations that need to be performed for a view-change to be deemed successful. The first is to update the view number to set another replica as the primary. The second is to validate this new leader on whether it is a suitable replacement. Finally, all operations that are performed after the last stable checkpoint needs to be reprocessed between the replicas. There are several ways for a replica to deem its primary to be faulty, the most common way is to have a timeout functionality for the protocol execution. The one that is most common is to start a timeout once a replica has received a request from the client. If the replica does not receive any pre-prepare messages for that request before the timeout expires, the replica will go into the view-change mode and will no longer participate in any of the protocol operations.  

%The view-change process starts by having the replica increment its view number. Then the replica will create, sign and multicast a view-change message. The replica will then wait for $2f+1$ view-change messages. A timeout is also used here, if the replica does not receive enough view-change messages in time, then the process repeats with the next incremented view number. In some cases a replica can also be designed to go into view-change mode if a replica has already received two view-change messages from other replicas, as it now only requires its own view-change message for the system to agree upon the view-change. Once the appropriate number of view-change messages are received, then the new primary will be responsible for creating, signing and multicasting a new-view message to the other replicas. Before the new-view message can be multicast to the other replicas, a new primary must go through its log and create new pre-prepares for all sequence number that has occurred after the last stable sequence number. If the new primary lacks a record in the log for any of the sequence numbers, the new pre-prepare message will have its request digest be set to null. This information is included in the new-view message, which is then used by the other replicas to validate and re-process each of the sequence numbers that has a valid pre-prepare. This essentially means that the other replicas have to multicast a new prepare message and then participate in a commit phase together with the new primary for of the pre-prepare messages in the new-view message. A timeout is once again being used in the case where the reprocessing fails or takes too long. This process can also fail if the pre-prepares in the new-view message fails the validation process. If this happens, then its back to start and in the view-change process. Once all pre-prepares have been reprocessed, the view-change procedure is over, and the replica will return to normal protocol operations using the chosen new primary. Any requests received during the view-change process will be ignored by the system. 

%TODO rewrite after implementing the basics of view change for app
%In the scenario where the primary is the faulty replica a view change occurs. A view change increments the view number for all participating replicas, which in turn chooses as a new primary replica for the network following the formula mention in \autoref{section:systemModel}. 
%Normally each replica starts a timeout operation whenever it receives a request from a client~\cite[p.~415]{PAPER:PBFTRecovery}. 
%If the replica does not receive a pre-prepare message before the timeout expires, then the replica deems the primary to be faulty and desires a view change. The replica will multicast a view-change message to the other replicas in the view, including the potentially faulty primary, and will ignore any %new incoming messages with the exception of view-change, new view and checkpoint messages~\cites[p.~5-6]{PAPER:OGPBFT}[p.~263]{BOOK:BuildDepDistSyst}. 
%A replica receiving a view change will reply with a view-change-ack.  The new primary chosen will collect view-change messages and view-change-ack messages to create a view-change certificate. Once the certificate is valid and has reached $2f+1$ unique messages a new-view message is multicasted to %every replica in the network, officially starting the new view~\cites[p.~412-414]{PAPER:PBFTRecovery}[p.~263-p.265]{BOOK:BuildDepDistSyst}[p.~458]{BOOK:MVstandver3}.
%%PBFT also incorporates checkpointing, which is a mechanism used for garbage collecting the saved logs. Without checkpoints, the replica would eventually use up all of its memory for logging protocol messages~\cite{BOOK:BuildDepDistSyst}. 
%%Checkpoints are essentially a proof where the replicas have agreed on a stable state for the system after a specified interval of requests has been processed~\cite[p.~5]{PAPER:OGPBFT}. For instance, if the checkpoint interval was set to 50, then the system would only allow sequence numbers 0 to 50 to %be performed in the first interval. Once the checkpoint interval is reached the log data is hashed and saved as a checkpoint message. The message is then multicasted over the network. Consensus is reached when a replica receives $2f+1$ checkpoint messages with different identifiers, but they all have same checkpoint hash value. The checkpoint is then saved on the replica and all the protocol messages with lower or equal sequence number to the checkpoint interval will be deleted from the log. As an example, if the checkpoint interval was 50, then certificates saved up for requests 0 to 50 are deleted. Then the interval is updated to be $[checkpointinterval+1-2*checkpointinterval]$, so for the last example the new interval would be between 51 to 100~\cites[p.~3]{PAPER:DPBFT}[p.~5]{PAPER:OGPBFT}[p.~409-410]{PAPER:PBFTRecovery}[p.~p.261-262]{BOOK:BuildDepDistSyst}.

\iffalse
\cite[p.~415]{PAPER:PBFTRecovery}
\cite[p.~5-6]{PAPER:OGPBFT}
\cite[p.~263]{BOOK:BuildDepDistSyst}
\cites[p.~3]{PAPER:DPBFT}
\cite[p.~458]{BOOK:MVstandver3}
\fi

%\cite{WEB:PBFTGeeks}
%\cite{WEB:ImpPBFTBlock} %not used
%\cite{WEB:UnderpBFT} %not used
%\cite{WEB:PBFTConSeries} %not used
%\cite{SLIDES:PBFT}
%\cite{VIDEO:YPBFT}
%\cite{BOOK:MVstandver3}
%\cite{BOOK:BuildDepDistSyst}
%\cite{PAPER:OGPBFT}
%\cite{PAPER:DPBFT}
%\cite{PAPER:PBFTRecovery}
%\lstset{style=sharpc}
%\begin{lstlisting}
%Your c# code here
%class Request
%{
%	private m string;
%} 
%\end{lstlisting}